[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.18.0","content-config-digest","a6eab140b5b99a47","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://dblooman.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[],\"actionBodySizeLimit\":1048576},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,28,29,62,63,80,81,105,106,130,131,148,149,190,191,229,230,256,257,277,278,313,314,355,356,400,401,433,434,479,480,517,518,565,566,601,602,628,629,668,669,710,711],"2012-11-15-mobile-network-traffic-inspection",{"id":11,"data":13,"body":17,"filePath":18,"digest":19,"rendered":20},{"title":14,"description":15,"pubDate":16},"Mobile Network Traffic Inspection","In this post, i will talk about how to view network traffic and the HAR HTTP Archive file. This is useful for doing several things, my initial intention was...",["Date","2012-11-15T00:00:00.000Z"],"\u003Cp>In this post, i will talk about how to view network traffic and the HAR (HTTP Archive) file. This is useful for doing several things, my initial intention was to discover network traffic from mobile devices.  This was something i wanted to view in order to find out whether our vary headers and stats were correctly being logged, but there is also an interesting way to measure performance.\u003C/p>\n\u003Cp>Requirements\u003C/p>\n\u003Cul>\u003Cli>\u003Cspan>OSX\u003C/span>\u003C/li>\n\u003Cli>\u003Cspan>\u003Ca href=\"http://www.tcpdump.org/\">TCP Dump\u003C/a>\u003C/span>\u003C/li>\n\u003Cli>\u003Cspan>\u003Ca href=\"http://pcapperf.appspot.com/\">PCAP\u003C/a>\u003C/span>\u003C/li>\n\u003Cli>\u003Cspan>\u003Ca href=\"https://github.com/andrewf/pcap2har\">PCAP2har\u003C/a>\u003C/span>\u003C/li>\n\u003C/ul>\u003Cp>The basic usage is to capture the HAR file, you will use tcpdump to capture TCP traffic and save it to PCAP file. The website above, you guessed it, shows you the PCAP file in it's HAR form. This will allow you to view the HAR timeline, request headers, UA string etc.\u003C/p>\n\u003Cp>Run this command in terminal. This takes the tcpdump from my bridged adaptor, the best way to do this is to share your wired conniption through your wireless and connect any mobile device to the new wireless access point.\u003C/p>\n\u003Cpre class=\"prettyprint\">sudo tcpdump -i bridge0 -n -s 0 -w nameoffile.pcap tcp or port 53\u003C/pre>\n\u003Cp>Once you have done all of you browsing, in some cases this will simply be loading a web page, hit control + C and the file is created in the directory you are in. The next step either requires you to have a local instance of the \u003Ca href=\"https://github.com/andrewf/pcap2har\">HAR viewer \u003C/a>on your machine, or use the \u003Ca href=\"http://pcapperf.appspot.com/\">online version\u003C/a>. Which ever you decide to use, the output will seem very familiar to most.\u003C/p>\n\u003Cp>By using this tool, we can see how each device loads the page, where you could improve upon for certain devices and what, if any, are the issues with your website.\u003C/p>\n\u003Cp>While this isn't as good as Chrome for Android that shows you all of this in real time, for very old devices, this is the best way to inspect your traffic.\u003C/p>","src/content/blog/2012-11-15-mobile-network-traffic-inspection.md","9acfff3f4c867b3f",{"html":17,"metadata":21},{"headings":22,"localImagePaths":23,"remoteImagePaths":24,"frontmatter":25,"imagePaths":27},[],[],[],{"title":14,"description":15,"pubDate":26},"2012-11-15",[],"2013-11-10-game-client-web-browsers",{"id":28,"data":30,"body":34,"filePath":35,"digest":36,"rendered":37},{"title":31,"description":32,"pubDate":33},"Game Client Web Browsers","I recently got Battlefield 4, an intensive first person shooter with high detail graphics and enough shooting to satisfy any teenage angst. I purchased the g...",["Date","2013-11-10T00:00:00.000Z"],"I recently got Battlefield 4, an intensive first person shooter with high detail graphics and enough shooting to satisfy any teenage angst.  I purchased the game on the PC, meaning I am locked into EA's(Electronic Arts) distribution software called [Origin](https://www.origin.com/en-gb/store/-ANW.html).  This software enables buying, trading, chatting, it also has a social aspect in the form of profile pages, but it also has an inbuilt web browser.  This browser isn't actually accessible from the Windows desktop, it is designed for use within the game itself.  \n\n\u003Cimg src=\"http://media.tumblr.com/ba9dc7aecb527ff43985b7660a8ed386/tumblr_inline_mw2d5vGrm21r7w4ky.png\" class=\"img-responsive\" alt=\"Screenshot\">\n\nThis is not a new concept, the main rival in this space is [Valve](http://store.steampowered.com/), with a similar distribution software called Steam.  Steam also has a browser, similar to Origin, and is also only accessible inside a running game.  The browser is accessible via a shortcut key which displays an overlay, the overlay exposes a slue of features for the now dead/bored gamer.  These browsers are meant to eliminate the need to leave the game and return to the desktop, which in theory, should lead to extra sales of in-game content.  In any case, these browsers exist, but I have never actually thought about what they are capable of, they always seemed ok, but I took some time to find out what's under the hood of these browsers.\n\n## The Browsers\n\n## Steam\nA quick look at the UA string says a lot about where these browsers started, for Steam, it is Chromium, or more importantly, [(CEF)Chromium embedded framework](https://code.google.com/p/chromiumembedded/).  This version of Chromium is quite old, version 18 with Web kit build 535.19, though the CEF doesn't get the same treatment as regular Chromium in update terms.  There is also a reliance on Valve to update Steams CEF to the latest, another barrier to the latest standards.  The general experience of Valve's browser is good, flash can be installed if the user decides to, with generally OK performance.  There is a tab interface, but that is about all it does have.  It wont win any speed contests, sometimes it can be slow to load big pages.  Responsive sites work as expected, though there is another dynamic to web browsing in Steam, the viewport.  \n\nAlthough you may have a 24 inch screen with HD resolution, some gamers like to run low resolutions to get the best performance, some will even run in a different aspect ration.  What this can mean is a 1920x1080 display is reduced to a 800x600 viewport, leaving responsive sits below what some would call \"desktop\" resolutions.  Steam respects the resolution of the game and limit the browser accordingly, so even on a large desktop monitor, responsive websites have an important home.\n\nThe scores from this browser follow from [Anna Debenham's](https://twitter.com/anna_debenham) [console browser scores](http://console.maban.co.uk/), using Acid3, CSS3test and HTML5test.\n\nCSS3 51%  \nAcid3 100%  \nHTML5test [354](http://beta.html5test.com/s/e322c81c5bf15a0e.html)\n\nUA String : Mozilla/5.0 (Windows; U; Windows NT 6.2; en-US; Valve Steam GameOverlay/1383158641; ) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19\n\n## Origin\n\nOrigin is little less forthcoming about it's origins, yes, I went there.  It points to a custom build of web kit, 534.34, while actually indicating it is Safari.  The interface is slightly different from Steam, but overall is a basic browser with tabs.  Flash can be installed in Origin too.  As both clients allow for chatting in the overlay, resizable browsers are key, which is very convenient for responsive sites.  Origin seems to have the same limiting in-game performance as Steam, scrolling and general use can have a element of lag to it.  If you stick to Facebook, Twitter and BBC News, you should be fine, but some parallax sites were noticeably janky.\n\nScores :\n\nCSS3 45%  \nAcid3 100%\nHTML5test [270](http://beta.html5test.com/s/66bc031c5bf22f76.html)\n\nUA String : Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/534.34 (KHTML, like Gecko) Origin/9.3.10.4710 Safari/534.34\n\n## Testing\n\nSome of the more interesting results came from simple tasks such as loading images, both browsers had strange results.  On websites like the Guardians new responsive site, Steam simply gave up trying to load javascript, so only a core experience was usable.  In Origin, images loaded, but the browser had serious issues with layouts of some responsive sites, they just didn't look right.  \n\n\u003Cimg src=\"http://media.tumblr.com/b7047c3c66e927a10174c571cf2fd634/tumblr_inline_mw2d55SURE1r7w4ky.png\" class=\"img-responsive\" alt=\"Guardian\">\n\nVideo was interesting, Origin behaved itself and didn't attempt to load HTML5 video, but Steam did, leading to a disappointed user experience.  On BBC News for example, clicking the video would remove the poster image and leave a disappointing non functional video player in place.  \n\u003Cimg src=\"http://media.tumblr.com/3ad25ef48d199df18a7dcf54900a9e74/tumblr_inline_mw2d3vI3001r7w4ky.png\" class=\"img-responsive\" alt=\"Steam\">\n\nScroll bars were also an issue for Steam, lets hope an important button isn't hidden away.\n\n## Conclusion\n\nIn-game browsers have been around since 2010, but Steam recently hit 60 million users and Origin is growing rapidly, though it is likely that Origin and Steam share a lot of users.  This growth could lead to yet another issue for more mainstream websites, testing browser compatibility for an audience that is typically quite good with a computer and it's capabilities.\n\nI suppose the best part about in-game browsers for Steam and Origin is that they are limited to PC's on a desk with a keyboard and mouse...........[sigh](http://store.steampowered.com/livingroom/SteamOS/)","src/content/blog/2013-11-10-game-client-web-browsers.md","c1ed6323b7318128",{"html":38,"metadata":39},"\u003Cp>I recently got Battlefield 4, an intensive first person shooter with high detail graphics and enough shooting to satisfy any teenage angst.  I purchased the game on the PC, meaning I am locked into EA’s(Electronic Arts) distribution software called \u003Ca href=\"https://www.origin.com/en-gb/store/-ANW.html\">Origin\u003C/a>.  This software enables buying, trading, chatting, it also has a social aspect in the form of profile pages, but it also has an inbuilt web browser.  This browser isn’t actually accessible from the Windows desktop, it is designed for use within the game itself.\u003C/p>\n\u003Cimg src=\"http://media.tumblr.com/ba9dc7aecb527ff43985b7660a8ed386/tumblr_inline_mw2d5vGrm21r7w4ky.png\" class=\"img-responsive\" alt=\"Screenshot\">\n\u003Cp>This is not a new concept, the main rival in this space is \u003Ca href=\"http://store.steampowered.com/\">Valve\u003C/a>, with a similar distribution software called Steam.  Steam also has a browser, similar to Origin, and is also only accessible inside a running game.  The browser is accessible via a shortcut key which displays an overlay, the overlay exposes a slue of features for the now dead/bored gamer.  These browsers are meant to eliminate the need to leave the game and return to the desktop, which in theory, should lead to extra sales of in-game content.  In any case, these browsers exist, but I have never actually thought about what they are capable of, they always seemed ok, but I took some time to find out what’s under the hood of these browsers.\u003C/p>\n\u003Ch2 id=\"the-browsers\">The Browsers\u003C/h2>\n\u003Ch2 id=\"steam\">Steam\u003C/h2>\n\u003Cp>A quick look at the UA string says a lot about where these browsers started, for Steam, it is Chromium, or more importantly, \u003Ca href=\"https://code.google.com/p/chromiumembedded/\">(CEF)Chromium embedded framework\u003C/a>.  This version of Chromium is quite old, version 18 with Web kit build 535.19, though the CEF doesn’t get the same treatment as regular Chromium in update terms.  There is also a reliance on Valve to update Steams CEF to the latest, another barrier to the latest standards.  The general experience of Valve’s browser is good, flash can be installed if the user decides to, with generally OK performance.  There is a tab interface, but that is about all it does have.  It wont win any speed contests, sometimes it can be slow to load big pages.  Responsive sites work as expected, though there is another dynamic to web browsing in Steam, the viewport.\u003C/p>\n\u003Cp>Although you may have a 24 inch screen with HD resolution, some gamers like to run low resolutions to get the best performance, some will even run in a different aspect ration.  What this can mean is a 1920x1080 display is reduced to a 800x600 viewport, leaving responsive sits below what some would call “desktop” resolutions.  Steam respects the resolution of the game and limit the browser accordingly, so even on a large desktop monitor, responsive websites have an important home.\u003C/p>\n\u003Cp>The scores from this browser follow from \u003Ca href=\"https://twitter.com/anna_debenham\">Anna Debenham’s\u003C/a> \u003Ca href=\"http://console.maban.co.uk/\">console browser scores\u003C/a>, using Acid3, CSS3test and HTML5test.\u003C/p>\n\u003Cp>CSS3 51%\u003Cbr>\nAcid3 100%\u003Cbr>\nHTML5test \u003Ca href=\"http://beta.html5test.com/s/e322c81c5bf15a0e.html\">354\u003C/a>\u003C/p>\n\u003Cp>UA String : Mozilla/5.0 (Windows; U; Windows NT 6.2; en-US; Valve Steam GameOverlay/1383158641; ) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19\u003C/p>\n\u003Ch2 id=\"origin\">Origin\u003C/h2>\n\u003Cp>Origin is little less forthcoming about it’s origins, yes, I went there.  It points to a custom build of web kit, 534.34, while actually indicating it is Safari.  The interface is slightly different from Steam, but overall is a basic browser with tabs.  Flash can be installed in Origin too.  As both clients allow for chatting in the overlay, resizable browsers are key, which is very convenient for responsive sites.  Origin seems to have the same limiting in-game performance as Steam, scrolling and general use can have a element of lag to it.  If you stick to Facebook, Twitter and BBC News, you should be fine, but some parallax sites were noticeably janky.\u003C/p>\n\u003Cp>Scores :\u003C/p>\n\u003Cp>CSS3 45%\u003Cbr>\nAcid3 100%\nHTML5test \u003Ca href=\"http://beta.html5test.com/s/66bc031c5bf22f76.html\">270\u003C/a>\u003C/p>\n\u003Cp>UA String : Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/534.34 (KHTML, like Gecko) Origin/9.3.10.4710 Safari/534.34\u003C/p>\n\u003Ch2 id=\"testing\">Testing\u003C/h2>\n\u003Cp>Some of the more interesting results came from simple tasks such as loading images, both browsers had strange results.  On websites like the Guardians new responsive site, Steam simply gave up trying to load javascript, so only a core experience was usable.  In Origin, images loaded, but the browser had serious issues with layouts of some responsive sites, they just didn’t look right.\u003C/p>\n\u003Cimg src=\"http://media.tumblr.com/b7047c3c66e927a10174c571cf2fd634/tumblr_inline_mw2d55SURE1r7w4ky.png\" class=\"img-responsive\" alt=\"Guardian\">\n\u003Cp>Video was interesting, Origin behaved itself and didn’t attempt to load HTML5 video, but Steam did, leading to a disappointed user experience.  On BBC News for example, clicking the video would remove the poster image and leave a disappointing non functional video player in place.\u003Cbr>\n\u003Cimg src=\"http://media.tumblr.com/3ad25ef48d199df18a7dcf54900a9e74/tumblr_inline_mw2d3vI3001r7w4ky.png\" class=\"img-responsive\" alt=\"Steam\">\u003C/p>\n\u003Cp>Scroll bars were also an issue for Steam, lets hope an important button isn’t hidden away.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>In-game browsers have been around since 2010, but Steam recently hit 60 million users and Origin is growing rapidly, though it is likely that Origin and Steam share a lot of users.  This growth could lead to yet another issue for more mainstream websites, testing browser compatibility for an audience that is typically quite good with a computer and it’s capabilities.\u003C/p>\n\u003Cp>I suppose the best part about in-game browsers for Steam and Origin is that they are limited to PC’s on a desk with a keyboard and mouse…\u003Ca href=\"http://store.steampowered.com/livingroom/SteamOS/\">sigh\u003C/a>\u003C/p>",{"headings":40,"localImagePaths":57,"remoteImagePaths":58,"frontmatter":59,"imagePaths":61},[41,45,48,51,54],{"depth":42,"slug":43,"text":44},2,"the-browsers","The Browsers",{"depth":42,"slug":46,"text":47},"steam","Steam",{"depth":42,"slug":49,"text":50},"origin","Origin",{"depth":42,"slug":52,"text":53},"testing","Testing",{"depth":42,"slug":55,"text":56},"conclusion","Conclusion",[],[],{"title":31,"description":32,"pubDate":60},"2013-11-10",[],"2013-07-30-wraith-opensource",{"id":62,"data":64,"body":68,"filePath":69,"digest":70,"rendered":71},{"title":65,"description":66,"pubDate":67},"Wraith Opensource","Today we are announcing the release of a front end regression testing tool called Wraith . &nbsp;This tool is something we have been using for the last 6 mon...",["Date","2013-07-30T00:00:00.000Z"],"\u003Cp>Today we are announcing the release of a front end regression testing tool called \u003Ca href=\"https://github.com/BBC-News/wraith\">Wraith\u003C/a>. &nbsp;This tool is something we have been using for the last 6 months, it has proven invaluable in testing CSS changes, both deliberate and unintentional.\u003C/p>\n\u003Cp>Download Wraith here :&nbsp;\u003Ca href=\"https://github.com/BBC-News/wraith\">https://github.com/BBC-News/wraith\u003C/a>\u003C/p>\n\u003Cp>This tool came about as we continued to see small changes from release to release, as more teams joined the project, small front end bugs cropped up more frequently. &nbsp;Our solution was to compare screenshots of pages, at the pull request level and when merged into master. &nbsp;This process produces fewer bugs and unintended changes, while also being able to ensure intentional changes appear correctly.\u003C/p>\n\u003Ch2>How it works\u003C/h2>\n\u003Cp>The tool is 2 parts, the capturing, using a headless browser and the comparing, using \u003Ca href=\"http://phantomjs.org/\">Imagemagick\u003C/a>. &nbsp;\u003Ca href=\"http://phantomjs.org/\">PhantomJS\u003C/a> is a headless browser designed exactly for these types of tasks. &nbsp;It doesn't have a UI, all your settings are applied on the command line, or in our case, in snap.js. &nbsp;We can use PhantomJS to capture multiple resolutions, so RWD testing is made much easier. &nbsp;An alternative is&nbsp;\u003Ca href=\"http://slimerjs.org/\">SlimerJS\u003C/a>, this is another headless browser. &nbsp;It is not out of the box headless on some operating systems, so is not as simple to setup as PhantomJS. &nbsp;The good thing is that it is essentially a clone of PhantomJS, so all settings will work with both browsers. &nbsp;\u003C/p>\n\u003Cp>The main difference between the two browsers is webkit and \u003Ca href=\"https://developer.mozilla.org/en-US/docs/Mozilla/Gecko\">Gecko\u003C/a>, PhantomJS being Webkit and SlimerJS being Gecko. &nbsp;This means that you can test across two rendering engines. &nbsp;By default, we have set up Wraith to use PhantomJS, this is due to the extra configuration to make Slimer headless. &nbsp;\u003C/p>\n\u003Cp>Using Wraith, you can see where your website really breaks by starting to put in resolutions you don't normally consider. &nbsp;Each screenshot has it's file name comprised of the resolution and the environment you're grabbing the screenshot from, the individual web pages will label the folder so all your screenshots will be grouped. &nbsp;\u003C/p>\n\n\u003Cimg src=\"http://media.tumblr.com/6cf7be1ea8c922595df53c0a58f2f52f/tumblr_inline_moafeq77KP1qz4rgp.png\" class=\"img-responsive\" alt=\"folders\">\n\n\u003Cp>\u003Cspan>-All the screenshots in their folders\u003C/span>\u003C/p>\n\u003Cp>Wraith captures images from 2 domains, in most cases a live website and then a local or staging environment. &nbsp;This is where most of the other tools I looked didn't fit our needs, they used historical base line images instead of a live site. &nbsp;Not only does this not take into account other teams or external dependencies, it also assumes that you are using static data. &nbsp;\u003C/p>\n\u003Cp>\u003Cspan>For News, that is not really possible, we use our sandboxes, test, stage and live environments for comparison. By using 2 domains, even if a dependency changes, e.g a twitter module, when you run the comparison, you will be using the latest version of the dependency. &nbsp;This wont flag up a change compared to a baseline with an old version of a twitter module that would. &nbsp;This may not be for those of you who are going to build a page and leave it for 6 months, but for a ever changing codebase like news, baseline images are out of date almost every day. &nbsp;\u003C/span>\u003C/p>\n\u003Ch2>I've got the Magick...\u003C/h2>\n\u003Cp>Once the 2 images have been captured, they are then compared and a third diff image is output. &nbsp;The third image shows changes in blue, even 1px changes are shown, so the accuracy is excellent. &nbsp;For this process, we use Imagemagick.\u003C/p>\n\u003Cp>Imagemagick is a very powerful tool for image work, so comparing images is a snap. &nbsp;We looked at other tools for comparing the images, but we found that there was an inherent problem with the way we were capturing images, anti-aliasing. &nbsp;Anti-aliasing caused changes to be shown where there were none, so our diff image wasn't that valuable. &nbsp;Imagemagick was our choice of tool because it takes care of the AA issue for you, but setting a fuzz of 20%, we were able to eliminate the AA differences. &nbsp;This setting was found by trial and error, so I suggest you fine tune this to your own preference. &nbsp;The fuzz parameter is set in the Rakefile.\u003C/p>\n\n\n\u003Cimg src=\"http://media.tumblr.com/c5bf7caefde043ee7532b641c7f0e157/tumblr_inline_moaf774b5i1qz4rgp.jpg\" class=\"img-responsive\" alt=\"diff\">\n\u003Cblockquote>\n  \u003Cp>-Two pages with the diff result\u003C/p>\n\u003C/blockquote>\n\u003Cp>\u003Cspan>Once we have the 3 images, we can start to review, this is simply a process of going through the diff images looking for lots of blue. &nbsp;If you don't like the blue colour, you can change it, but blue seems distinctive and should be easier to see on a white canvas. &nbsp;This is the only manual part of the process, but it is not something that takes that long. &nbsp;The total amount of time to capture the images and compare will be based on your Internet connection and the speed of your computer, but we usually capture 200 images and by the time we have our diffs, it has taken around 10 minutes. &nbsp;\u003C/span>\u003C/p>\n\u003Ch2>Conclusion\u003C/h2>\n\u003Cp>\u003Cspan>The amount of time spent testing for CSS regressions can be lengthy if you don't automate, looking at devices and different browser resolutions can be a drag on testing. &nbsp;By using this tool, we have cut our testing time down dramatically, with fewer bugs making there way into master and the live site.\u003C/span>\u003C/p>","src/content/blog/2013-07-30-wraith-opensource.md","28238b10e241241d",{"html":72,"metadata":73},"\u003Cp>Today we are announcing the release of a front end regression testing tool called \u003Ca href=\"https://github.com/BBC-News/wraith\">Wraith\u003C/a>.  This tool is something we have been using for the last 6 months, it has proven invaluable in testing CSS changes, both deliberate and unintentional.\u003C/p>\n\u003Cp>Download Wraith here : \u003Ca href=\"https://github.com/BBC-News/wraith\">https://github.com/BBC-News/wraith\u003C/a>\u003C/p>\n\u003Cp>This tool came about as we continued to see small changes from release to release, as more teams joined the project, small front end bugs cropped up more frequently.  Our solution was to compare screenshots of pages, at the pull request level and when merged into master.  This process produces fewer bugs and unintended changes, while also being able to ensure intentional changes appear correctly.\u003C/p>\n\u003Ch2>How it works\u003C/h2>\n\u003Cp>The tool is 2 parts, the capturing, using a headless browser and the comparing, using \u003Ca href=\"http://phantomjs.org/\">Imagemagick\u003C/a>.  \u003Ca href=\"http://phantomjs.org/\">PhantomJS\u003C/a> is a headless browser designed exactly for these types of tasks.  It doesn't have a UI, all your settings are applied on the command line, or in our case, in snap.js.  We can use PhantomJS to capture multiple resolutions, so RWD testing is made much easier.  An alternative is \u003Ca href=\"http://slimerjs.org/\">SlimerJS\u003C/a>, this is another headless browser.  It is not out of the box headless on some operating systems, so is not as simple to setup as PhantomJS.  The good thing is that it is essentially a clone of PhantomJS, so all settings will work with both browsers.  \u003C/p>\n\u003Cp>The main difference between the two browsers is webkit and \u003Ca href=\"https://developer.mozilla.org/en-US/docs/Mozilla/Gecko\">Gecko\u003C/a>, PhantomJS being Webkit and SlimerJS being Gecko.  This means that you can test across two rendering engines.  By default, we have set up Wraith to use PhantomJS, this is due to the extra configuration to make Slimer headless.  \u003C/p>\n\u003Cp>Using Wraith, you can see where your website really breaks by starting to put in resolutions you don't normally consider.  Each screenshot has it's file name comprised of the resolution and the environment you're grabbing the screenshot from, the individual web pages will label the folder so all your screenshots will be grouped.  \u003C/p>\n\u003Cimg src=\"http://media.tumblr.com/6cf7be1ea8c922595df53c0a58f2f52f/tumblr_inline_moafeq77KP1qz4rgp.png\" class=\"img-responsive\" alt=\"folders\">\n\u003Cp>\u003Cspan>-All the screenshots in their folders\u003C/span>\u003C/p>\n\u003Cp>Wraith captures images from 2 domains, in most cases a live website and then a local or staging environment.  This is where most of the other tools I looked didn't fit our needs, they used historical base line images instead of a live site.  Not only does this not take into account other teams or external dependencies, it also assumes that you are using static data.  \u003C/p>\n\u003Cp>\u003Cspan>For News, that is not really possible, we use our sandboxes, test, stage and live environments for comparison. By using 2 domains, even if a dependency changes, e.g a twitter module, when you run the comparison, you will be using the latest version of the dependency.  This wont flag up a change compared to a baseline with an old version of a twitter module that would.  This may not be for those of you who are going to build a page and leave it for 6 months, but for a ever changing codebase like news, baseline images are out of date almost every day.  \u003C/span>\u003C/p>\n\u003Ch2>I've got the Magick...\u003C/h2>\n\u003Cp>Once the 2 images have been captured, they are then compared and a third diff image is output.  The third image shows changes in blue, even 1px changes are shown, so the accuracy is excellent.  For this process, we use Imagemagick.\u003C/p>\n\u003Cp>Imagemagick is a very powerful tool for image work, so comparing images is a snap.  We looked at other tools for comparing the images, but we found that there was an inherent problem with the way we were capturing images, anti-aliasing.  Anti-aliasing caused changes to be shown where there were none, so our diff image wasn't that valuable.  Imagemagick was our choice of tool because it takes care of the AA issue for you, but setting a fuzz of 20%, we were able to eliminate the AA differences.  This setting was found by trial and error, so I suggest you fine tune this to your own preference.  The fuzz parameter is set in the Rakefile.\u003C/p>\n\u003Cimg src=\"http://media.tumblr.com/c5bf7caefde043ee7532b641c7f0e157/tumblr_inline_moaf774b5i1qz4rgp.jpg\" class=\"img-responsive\" alt=\"diff\">\n\u003Cblockquote>\n  \u003Cp>-Two pages with the diff result\u003C/p>\n\u003C/blockquote>\n\u003Cp>\u003Cspan>Once we have the 3 images, we can start to review, this is simply a process of going through the diff images looking for lots of blue.  If you don't like the blue colour, you can change it, but blue seems distinctive and should be easier to see on a white canvas.  This is the only manual part of the process, but it is not something that takes that long.  The total amount of time to capture the images and compare will be based on your Internet connection and the speed of your computer, but we usually capture 200 images and by the time we have our diffs, it has taken around 10 minutes.  \u003C/span>\u003C/p>\n\u003Ch2>Conclusion\u003C/h2>\n\u003Cp>\u003Cspan>The amount of time spent testing for CSS regressions can be lengthy if you don't automate, looking at devices and different browser resolutions can be a drag on testing.  By using this tool, we have cut our testing time down dramatically, with fewer bugs making there way into master and the live site.\u003C/span>\u003C/p>",{"headings":74,"localImagePaths":75,"remoteImagePaths":76,"frontmatter":77,"imagePaths":79},[],[],[],{"title":65,"description":66,"pubDate":78},"2013-07-30",[],"2013-11-26-screen-recording-in-android",{"id":80,"data":82,"body":86,"filePath":87,"digest":88,"rendered":89},{"title":83,"description":84,"pubDate":85},"Screen Recording In Android","Sometimes you need to show someone something on a phone, words are just not good enough. With testing moving more to mobile, having the ability to show a col...",["Date","2013-11-26T00:00:00.000Z"],"Sometimes you need to show someone something on a phone, words are just not good enough.  With testing moving more to mobile, having the ability to show a colleague the issue has become a little harder.  Google introduced screencasting in Android 4.4, a way to record everything you do on screen.  Great, but that makes a big video that is difficult to look at on Github, so lets convert that video into a GIF.\n\nFor this you will need the Android SDK, FFMPEG and ImageMagick.  Install on Mac by using Homebrew.  You will need and Android 4.4 phone to make this work, these instructions are also tailored for OSX system.\n\nInstall binaries\n\u003Cpre>\nbrew install android-sdk imagemagick ffmpeg --devel\u003C/pre>\n\nWith terminal open and your android 4.4 phone plugged in, use the following commands, swapping out the file name as you see fit.  \n\nTo start the recording, use the adb shell screenrecord command \u003C/pre>\n\n\u003Cpre>adb shell screenrecord  /sdcard/pull_request.mp4\u003C/pre>\n\nYou can also change the bit rate from 4mbps up to 8mbps by using\n\n\u003Cpre>adb shell screenrecord --bit-rate 8000000 /sdcard/pull_request.mp4\u003C/pre>\n\nHit control + c to stop recording.\n\nWhen you are done, pull the video off by using\n\n\u003Cpre>adb pull /sdcard/pull_request.mp4\u003C/pre>\n\nThis is now the video file we will work with.  If you want to delete lots of files you didn't use, you can `adb shell` in terminal to access the phones storage.\n\n\n# Converting the video to GIF\n\n\u003Cpre>ffmpeg -i pull_request.mp4 -vf scale=360:-1 -r 5 output.gif \u003C/pre>\n\nLets break it down,\n\nffmpeg is the application we are using\n\npull_request.mp4 is the file name of the video\n\n-vf is the video filter graph for setting FPS and scale\n\nscale=360:-1 is the WxH, so 360 as that is 1/4 the width of the Nexus 5, this is to reduce file size.  Using -1 sets the aspect ratio automatically for us based on the input video size, 16:9 in our case, meaning less guess work.\n\n-r is the frame rate of the GIF, I have used 5, but you could go to 6 or 7.\n\noutput.gif is our finished file name.\n\n\n# Compressing the GIF\n\nThis is the hard part, finding a balance for the quality Vs file size.  We ended up with a 3.5MB GIF from a 20 second video clip, using Imagemagick, we got that down to 1.3MB, but the results were not that useful.  Instead, I have decided on a 10% fuzz to give a nice result, but still reducing file size to 2.1MB, meaning it is roughly 100k per second.  \n\n\u003Cpre>convert output.gif -fuzz 10% -layers Optimize final.gif\u003C/pre>\n\nI decided not to embed the GIF, save you the download, if you want to see it, click the [link](http://imageshack.com/a/img29/3360/0zqt.gif)","src/content/blog/2013-11-26-screen-recording-in-android.md","4da81cf16862a680",{"html":90,"metadata":91},"\u003Cp>Sometimes you need to show someone something on a phone, words are just not good enough.  With testing moving more to mobile, having the ability to show a colleague the issue has become a little harder.  Google introduced screencasting in Android 4.4, a way to record everything you do on screen.  Great, but that makes a big video that is difficult to look at on Github, so lets convert that video into a GIF.\u003C/p>\n\u003Cp>For this you will need the Android SDK, FFMPEG and ImageMagick.  Install on Mac by using Homebrew.  You will need and Android 4.4 phone to make this work, these instructions are also tailored for OSX system.\u003C/p>\n\u003Cp>Install binaries\u003C/p>\n\u003Cpre>brew install android-sdk imagemagick ffmpeg --devel\u003C/pre>\n\u003Cp>With terminal open and your android 4.4 phone plugged in, use the following commands, swapping out the file name as you see fit.\u003C/p>\n\u003Cp>To start the recording, use the adb shell screenrecord command \u003C/p>\n\u003Cpre>adb shell screenrecord  /sdcard/pull_request.mp4\u003C/pre>\n\u003Cp>You can also change the bit rate from 4mbps up to 8mbps by using\u003C/p>\n\u003Cpre>adb shell screenrecord --bit-rate 8000000 /sdcard/pull_request.mp4\u003C/pre>\n\u003Cp>Hit control + c to stop recording.\u003C/p>\n\u003Cp>When you are done, pull the video off by using\u003C/p>\n\u003Cpre>adb pull /sdcard/pull_request.mp4\u003C/pre>\n\u003Cp>This is now the video file we will work with.  If you want to delete lots of files you didn’t use, you can \u003Ccode>adb shell\u003C/code> in terminal to access the phones storage.\u003C/p>\n\u003Ch1 id=\"converting-the-video-to-gif\">Converting the video to GIF\u003C/h1>\n\u003Cpre>ffmpeg -i pull_request.mp4 -vf scale=360:-1 -r 5 output.gif \u003C/pre>\n\u003Cp>Lets break it down,\u003C/p>\n\u003Cp>ffmpeg is the application we are using\u003C/p>\n\u003Cp>pull_request.mp4 is the file name of the video\u003C/p>\n\u003Cp>-vf is the video filter graph for setting FPS and scale\u003C/p>\n\u003Cp>scale=360:-1 is the WxH, so 360 as that is 1/4 the width of the Nexus 5, this is to reduce file size.  Using -1 sets the aspect ratio automatically for us based on the input video size, 16:9 in our case, meaning less guess work.\u003C/p>\n\u003Cp>-r is the frame rate of the GIF, I have used 5, but you could go to 6 or 7.\u003C/p>\n\u003Cp>output.gif is our finished file name.\u003C/p>\n\u003Ch1 id=\"compressing-the-gif\">Compressing the GIF\u003C/h1>\n\u003Cp>This is the hard part, finding a balance for the quality Vs file size.  We ended up with a 3.5MB GIF from a 20 second video clip, using Imagemagick, we got that down to 1.3MB, but the results were not that useful.  Instead, I have decided on a 10% fuzz to give a nice result, but still reducing file size to 2.1MB, meaning it is roughly 100k per second.\u003C/p>\n\u003Cpre>convert output.gif -fuzz 10% -layers Optimize final.gif\u003C/pre>\n\u003Cp>I decided not to embed the GIF, save you the download, if you want to see it, click the \u003Ca href=\"http://imageshack.com/a/img29/3360/0zqt.gif\">link\u003C/a>\u003C/p>",{"headings":92,"localImagePaths":100,"remoteImagePaths":101,"frontmatter":102,"imagePaths":104},[93,97],{"depth":94,"slug":95,"text":96},1,"converting-the-video-to-gif","Converting the video to GIF",{"depth":94,"slug":98,"text":99},"compressing-the-gif","Compressing the GIF",[],[],{"title":83,"description":84,"pubDate":103},"2013-11-26",[],"2016-02-02-a-month-with-no-job",{"id":105,"data":107,"body":111,"filePath":112,"digest":113,"rendered":114},{"title":108,"description":109,"pubDate":110},"A Month With No Job","Sixteen days ago, I quit my job.",["Date","2016-02-02T00:00:00.000Z"],"Sixteen days ago, I quit my job.\n\nI had been planning on leaving for a while, since October, so between then and the time I quit, I limited the amount of time I took off.  This allowed me to take 17 days holiday, getting paid for work up until the 15th of March.  My new job starts at the end of March, so between the 29th of February and 29th March, I had nothing to do.  \n\n### I have a whole month off.\n\nThis is quite an interesting opportunity.  Do I spend a month on a beach; do some freelance, travel around Europe??  Another plan for me this year is to buy a flat, with most of money going towards saving for a deposit, going on a 4 week trip is not something I can afford right now.  While I have a short trip to somewhere warm planned, that still leaves a few weeks of well, nothing.\n\n### Switching Off\n\nThere are people who I worked with who when the laptop closes at 5, they stop caring about all things computer.  While part of me would love to be able to do this, my brain is wired to keep reading and learning.  One of the things I felt the BBC did well is training, one of the things they did badly was learning.  \nFor example, if you are learning Erlang for your own benefit, great.  If you are learning Java because if you don't, you know will be put in a situation where you are being asked to write Java with no time to learn it, then this is a problem.  You are raising the expectation of yourself, while the company gets your time for free in the evenings and weekends.  As the BBC is quite big, this is my opinion and other teams have it different.  Some teams have 10% time, many hack days a year, work libraries with books, both paper and digital.\n\n### This month is about learning\n\nI have decided to spend a lot of this month learning new things that have no relevance to any work past or future.  I am pretty weak in the area of front end for example, something I hope to improve upon.  There are many other areas, from infrastructure, different languages, working with databases that I plan to work on.  I will also create projects and open source them on Github, updating my blog with info about what kind of things I learned.","src/content/blog/2016-02-02-a-month-with-no-job.md","5899f9f51ac8673d",{"html":115,"metadata":116},"\u003Cp>Sixteen days ago, I quit my job.\u003C/p>\n\u003Cp>I had been planning on leaving for a while, since October, so between then and the time I quit, I limited the amount of time I took off.  This allowed me to take 17 days holiday, getting paid for work up until the 15th of March.  My new job starts at the end of March, so between the 29th of February and 29th March, I had nothing to do.\u003C/p>\n\u003Ch3 id=\"i-have-a-whole-month-off\">I have a whole month off.\u003C/h3>\n\u003Cp>This is quite an interesting opportunity.  Do I spend a month on a beach; do some freelance, travel around Europe??  Another plan for me this year is to buy a flat, with most of money going towards saving for a deposit, going on a 4 week trip is not something I can afford right now.  While I have a short trip to somewhere warm planned, that still leaves a few weeks of well, nothing.\u003C/p>\n\u003Ch3 id=\"switching-off\">Switching Off\u003C/h3>\n\u003Cp>There are people who I worked with who when the laptop closes at 5, they stop caring about all things computer.  While part of me would love to be able to do this, my brain is wired to keep reading and learning.  One of the things I felt the BBC did well is training, one of the things they did badly was learning.\u003Cbr>\nFor example, if you are learning Erlang for your own benefit, great.  If you are learning Java because if you don’t, you know will be put in a situation where you are being asked to write Java with no time to learn it, then this is a problem.  You are raising the expectation of yourself, while the company gets your time for free in the evenings and weekends.  As the BBC is quite big, this is my opinion and other teams have it different.  Some teams have 10% time, many hack days a year, work libraries with books, both paper and digital.\u003C/p>\n\u003Cp>### This month is about learning\u003C/p>\n\u003Cp>I have decided to spend a lot of this month learning new things that have no relevance to any work past or future.  I am pretty weak in the area of front end for example, something I hope to improve upon.  There are many other areas, from infrastructure, different languages, working with databases that I plan to work on.  I will also create projects and open source them on Github, updating my blog with info about what kind of things I learned.\u003C/p>",{"headings":117,"localImagePaths":125,"remoteImagePaths":126,"frontmatter":127,"imagePaths":129},[118,122],{"depth":119,"slug":120,"text":121},3,"i-have-a-whole-month-off","I have a whole month off.",{"depth":119,"slug":123,"text":124},"switching-off","Switching Off",[],[],{"title":108,"description":109,"pubDate":128},"2016-02-02",[],"2012-03-28-responsive-testing",{"id":130,"data":132,"body":136,"filePath":137,"digest":138,"rendered":139},{"title":133,"description":134,"pubDate":135},"Responsive Testing","We thought we'd share some information on our testing process. & 13;",["Date","2012-03-28T00:00:00.000Z"],"\u003Cp>We thought we'd share some information on our testing process.\u003C/p>\n\u003Cp>My name is David Blooman, I am the principal tester for responsive news, all questions and queries can be sent to me at \u003Ca href=\"https://twitter.com/dblooman\">@dblooman\u003C/a>\u003C/p>\n\u003Cp>When testing the devices, there are 4 main areas to cover, feature phones, smart phones with low level support, smart phones and tablets. The primary tablet market will be 7-inch devices up to 720x1280 pixels with the lowest smart phone being 240x360 pixels.\u003Cbr /> Currently, we are only approaching devices that are not conforming to desktops, so smart TV's, games consoles and other exotic devices are test, desktops are not.\u003C/p>\n\n\u003Cimg src=\"/images/devices.jpg\" class=\"img-responsive\" alt=\"Devices\">\n\n\u003Cp>We do test desktop browsers, Chrome, Firefox, IE, however we typical test the version which our stats point to, or when there was a feature update.  As some browsers use auto updating services, it can often be a case of waiting for the next release to switch testing to a new version. \u003C/p>\n\u003Cp>When testing on a desktop, the hardware does play an important part.  The pixel density of large desktop screens, which is typically low, means testing on a desktop screen may prove fruitless, given that any CSS issues may be masked.  When testing for mobile, images may also appear over blown or scaled incorrectly, given the larger screen.  \u003C/p>\n\u003Cp>The final part of our per-requisite list is cutting the mustard -\u003Ca class=\"external-link\" href=\"http://responsivenews.co.uk/post/18948466399/cutting-the-mustard\" rel=\"nofollow\"> blog post\u003C/a>\u003C/p>\n\u003Cp>Minimum Device Testing\u003Cbr /> ----------------------\u003C/p>\n\u003Cul class=\"alternate\">\u003Cli>Symbian S60 - N96 (Symbian 9.3)\u003Cbr /> This phone represents the lowest of our requirements, mobile barlesque, our header, is 240px wide, meaning that our lowest support width is the same. The Nokia N96 is a great test case to identify the impacts to other dependencies within a webpage. This phone is also widely used, given the low cost nature of the OS(open source Symbian), as well as having a strong user base in our stats.  The OS also supports opera mini/mobile for comparing a non JS/JS experience\u003C/li>\n\u003Cli>Android 2.2 Native Browser- Galaxy S\u003Cbr /> This phone is our minimum Android OS support for smart phones, this is based upon the fact that it cuts the mustard in our tests. While the user base for this OS is dropping due to more recent versions of Android, 4.0 etc.  This devices is a good hardware point to test against as well. It features a 1ghz CPU, which was less optimised when 2.2 was launched, meaning that 2.2 on a slower CPU may produce a smoother experience. With manual testing, we have discovered a consistency across Android in that, if it works on 2.2, it has a strong chance of working on later versions. Android 2.2 is the best browser to test against for maximum stability across the Android user base, from 2.1 to 2.3.3\u003C/li>\n\u003Cli>iOS 6 Native Browser - iPhone4S\u003Cbr /> This browser has taken most of the iOS user base, being stable and having a lot of the more advanced features of HTML5.  The browser produces a huge amount of web traffic globally, so testing this is a must for any large commercial website.\u003C/li>\n\u003Cli>Blackberry OS 6 - Bold 9700\u003Cbr /> BBOS6 is not the latest version of the OS, but does feature a near identical browser as 7, with minor improvements seen in 7. Even so, the uptake of 7 is light, so the focus should be on 6. This is for 2 reasons, 1, BBOS6 supports a Webkit browser, 2, the browser cuts the mustard. What is important to note about BBO6 is the Curve range.  The curve range is a budget range of phones with lower report rates or resolutions.  The 9300 for example, has a strong user base, but it also is a good indicator of how the smartphone experience is impacted by a screen with a small vertical height.  For this reason, testing on a 9300 could give you a skewed result, the same for only testing on the 9700.  If you are only testing features that have no layouts and CSS changes, the 9700 is the better of the two, but testing on the 9300 will give you an idea of the budget uses experience.\u003C/li>\n\u003Cli>Nexus 7\u003Cbr />Android 4.1 was launched on the most popular Android tablet, the Nexus 7.  With 4.1 came Chrome for Android by default, this represents a shift away from the native browser, which many Android OEMs customise.  By having a consistent browser across all devices, Google makes testing Chrome easier too.  More than that, the Nexus 7 size, resolution and pixel density make it a solid device to test tablet and phone layouts.\u003C/li>\n\u003C/ul>\u003Cp>Review\u003Cbr /> ----------------------\u003C/p>\n\u003Cp>The list above may be different to the list many others have come up with, this is because many testers will focus on existing markets and user base, which is Android and iOS oriented at the moment.  This type of focus is inadvisable given the ease of switching services from apps to web, other competitors to yours.  This under estimation of the market and how quickly it can change may lead you to not fully support a browser or entire range of devices.  In many test devices list, you will see Android 2.1,2.2,2.3 and 4 devices listed as more priority over Symbian devices.  The project will then focus on using emulators with opera installed for Symbian testing, which may provide such a different result, that it could lose market share in Africa, for example.  The devices above will cover different break points for design, but also play a secondary part, third party browser testing.  Opera is the most popular mobile browser, which is supported on iOS, Android and Symbian, it brings HTML5 support to even the N95.  For this reason, Opera should be looked at closely on these devices if you looking into lots of different markets and countries.  Opera Mini also plays a big part, the news website is reduced to feature phone look as Opera Mini uses proxy servers, disabling most of the Javascript on the page.  A Galaxy Nexus with a 720p screen can make some websites look terrible when using Opera Mini, ensuring you have Opera on you test list is key to success. \u003C/p>\n\u003Cp>Hardware Tests\u003Cbr /> ----------------------\u003C/p>\n\u003Cul class=\"alternate\">\u003Cli>Device of 240 pixels wide screen - N95\u003C/li>\n\u003Cli>Device of more than 1000 wide screen - Any device\u003C/li>\n\u003Cli>Touch Screen - Any device will be relevant, this is to test hit states, button placements and general usage while using a device with on screen keyboard.\u003C/li>\n\u003Cli>Non-touch - In most cases, Symbian and blackberry phones will fall into this category, how the user can navigate the site is of importance, so testing the functionality of the site with such devices is a high priority.\u003C/li>\n\u003C/ul>\u003Cp>These 4 additional requirements may require additional devices, but in investing in this equipment, the devices will cover most of the core stats and you will be able to test against the majority of browsers and input methods.  This should allow you to see where your problem areas are going to be, as you can quickly identify which browsers have the key levels of JavaScript and technologies support required.\u003C/p>\n\u003Cp>It is recommended that you clear all cookies, cache and reset browser to default before beginning any testing. Not doing this can lead to older versions of the site interfering with testing as you always must look at the latest version of the site. With builds occurring all the time, minor changes will often show little impact, such as back end work. Front end work can mean changes to the site that will fundamentally affect testing, as the look can be radically different on certain devices. You should always consider this when working, there is no such thing as a minor change when testing responsive web pages.\u003C/p>\n\u003Cp>Further Testing\u003Cbr /> ---------------\u003C/p>\n\u003Cp>For further testing, the following list identifies the most common Mobile OS's. Below is explanation of the differences between the OS's and browsers, this could help you determine whether you testing need to extend to all the devices.\u003Cbr /> The Android 2.3 OS has the the largest user base of Android, therefor the most users on that versions browser. We use the Galaxy S as 1, it is one of the most popular devices in our stats, 2, it has a typical size screen and pixel density for that OS, meaning a more balanced approach when doing UI testing.\u003Cbr /> The Bold 9700 is again, one of the more popular devices, but it also represents a crossover device from OS5 to OS6. It's screen is of a good pixel size for Blackberrys and its internals are one of the more high specifications for a feature phone.\u003Cbr /> Windows phone is a good way of testing IE10, as a large amount of the code base was used to create the browser on WP8, though mobile has less features. \u003C/p>\n\u003Cul class=\"alternate\">\u003Cli>iOS 6 - iphone 4\u003C/li>\n\u003Cli>Android 2.2 - HTC desire\u003C/li>\n\u003Cli>Android 2.3 - Samsung Galaxy S\u003C/li>\n\u003Cli>Android 4 - Both stock and Chrome browsers\u003C/li>\n\u003Cli>Blackberry OS 5 - Bold 9700\u003C/li>\n\u003Cli>Blackberry OS 6 - Bold 9700\u003C/li>\n\u003Cli>Blackberry OS 7 - 9380\u003C/li>\n\u003Cli>Windows Phone 8 - Lumia 920\u003C/li>\n\u003Cli>Symbian S60 - Symbian Browser\u003C/li>\n\u003Cli>Symbian S40 - Nokia Browser\u003C/li>\n\u003Cli>Nexus 7\u003C/li>\n\u003Cli>iOS 6 - iPad\u003C/li>\n\u003C/ul>\u003Cp>\u003Cstrong>Browser and OS Analysis\u003C/strong>\u003Cbr /> -----------------------\u003C/p>\n\u003Cp>While emulators are very useful in a lab style environment, they often miss things that are only found when using handsets. There is also a lot of nuances that you will discover when you use this many devices. Below is a combination of personal experience, information from other sources and my opinion on the direction when testing responsive design.\u003C/p>\n\u003Cp>\u003Cstrong>iOS.\u003C/strong> The iphone and ipod range from the 3Gs upwards all support iOS5, which has support for most standards, the browser on iOS5 is the same on every device, therefore testing on 1 device is suitable. The second most supported version of the OS which supports a slightly lower performing browser is iOS4, however this has not performed differently except in hardware (larger screen) Here is a comparison of the 2 versions \u003Ca class=\"external-link\" href=\"http://www.blaze.io/mobile/ios5-top10-performance-changes/\" rel=\"nofollow\">http://www.blaze.io/mobile/ios5-top10-performance-changes/\u003C/a> - The range of iOS devices represents nearly 20 million page views per month, hence it's high priority status.\u003C/p>\n\u003Cp>\u003Cstrong>Android\u003C/strong> 2.1 is the lowest version of Android to cut the mustard. Android has varying levels of support, Android 2.2 browser and onwards has been the most consistent, with Android 2.2 and 2.3 being the highest user base. Android 2.1 has the ability to support many smart phone features, but lacks some font embedding and other technologies, such as Flash. This has proven to be one of the more problematic devices, but is considered a baseline phone for smart phone support. This particular version of Android is in decline as the jump from 2.1 to 2.2 is being made, or the user upgrades to a different phone. Android 2.1 represents about 25% of the Android user base with a million page views per month.\u003C/p>\n\u003Cp>\u003Cstrong>Android 2.2\u003C/strong> was the last version to be open source, so a lot of devices used this version for Google TVs, phones, music players etc. The browser for 2.3 is also based on this version. Differences in the operating system can have some impact on the browser, but the difference between 2.2 and 2.3 is mostly down to how the particular vendor changes the phone. A change log is listed in the link below \u003Ca class=\"external-link\" href=\"http://www.knowitsdifference.com/difference-between-android-2-2-and-android-2-3/\" rel=\"nofollow\">http://www.knowitsdifference.com/difference-between-android-2-2-and-android-2-3/\u003C/a> Android 2.2 is a measurable jump in performance over 2.1, the user base is almost 3 times as big with nearly 8 million page views per month. Android 2.3 offers an overall system update that is relevant to user, not as much for browsers though. In our test, there have been less that 1% of difference between the 2, with only 1 or 2 bugs discovered not to replicate on both versions. The 7 million combined users with 20 million page views shows us how important it is to correctly test on Android 2.x versions.\u003Cbr /> What is important to note about 2.2 is the browser features a shrunk down version of desktop Chrome's V8 JavaScript engine, Google have integrated it into the Android 2.3 OS to drastically improve JS performance. When testing Android 2.1 and 2.2, the should be treated as different entities, rather than just a point release, such as 2.2 and 2.3.\u003C/p>\n\u003Cp>\u003Cstrong>Android 3\u003C/strong> is the tablet skew of the OS. 3.2 and above supports lower screen sizes that mean you can have a large range of devices, so testing on this version is best. The update of Android tablets is still very small compared to iOS devices, like the iPad, however the browser is very high quality. The Android 3 OS is essentially a fork of Android 2.2, as such is inherits much of the browser from there. However there is added support for multi touch within Javascript and improved zooming options.\u003C/p>\n\u003Cp>\u003Cstrong>Android 4\u003C/strong> is the newest and most advanced version of the OS, it currently supports 2 browsers, Chrome and the Android browser. It seems that Google intend to replace the stock browser with Chrome on upcoming devices, so testing on both until this is that case is prudent. The current version of Chrome uses a new version of Webkit than the stock android browser. Info on browser \u003Ca class=\"external-link\" href=\"http://www.android.com/about/ice-cream-sandwich/\" rel=\"nofollow\">http://www.android.com/about/ice-cream-sandwich/\u003C/a>. What it is important is the that Chrome is a Google product, therefore it may not be present on the open source version of the OS, so for future testing, stats will determine which webkit version and by extension, which browser is the most popular on the platform.  Android 4.1 brought a few improvements over 4.0, \u003Cspan>better HTML5 video support, supports the updated HTML5 Media Capture specification on input elements.WebView now supports vertical text, including Ruby Text and other vertical text glyphs.\u003C/span>\u003C/p>\n\u003Cp>\u003Cspan>\u003Cstrong>Android 4.1\u003C/strong> had a few tweaks to the browser, but in reality, the focus is now on Chrome as the stock browser faces heavier competition.  Chrome Should be seen as an equal in testing to the stock browser.\u003C/span>\u003C/p>\n\u003Cp>\u003Cstrong>Blackberry OS 5\u003C/strong> is a pure feature phone experience, it's screen size support is one of the biggest factors, given the high landscape resolution, but low vertical. The browser has been the most consistent for feature phones, supporting all the requirements.\u003C/p>\n\u003Cp>\u003Cstrong>Blackberry OS 6\u003C/strong> has patchy support, it uses a webkit browser, but the support for font embedding has been hit and miss. The browser is currently one of the highest usage of the website, meaning that this is important OS for testing. Newer versions of the Blackberry browser seem to have fixed the issues we are having with 6. Browser Info \u003Ca class=\"external-link\" href=\"http://crackberry.com/blackberry-6-review\" rel=\"nofollow\">http://crackberry.com/blackberry-6-review\u003C/a> - new-browserhttp://crackberry.com/blackberry-6-review#new-browser One of the core issues with Blackberries is the fact that they have unusual screens. The majority of phones sold between 2008 and 2011 have a 480x360 screen; this is intended for reading emails instead of web pages. What should also be tested is what the screen reports itself as, on some devices the resolution is dropped to 360 wide due to hardware limitations. It is important to identify issues with the look and feel on a webpage based upon these types of devices, especially for us with more than 4 million users and 16 million page views.\u003C/p>\n\u003Cp>\u003Cstrong>Blackberry OS 7\u003C/strong> has improvements over the BBOS6 browser.  Most of the bugs have been fixed, while adding new features such as a web inspector and better video/audio support. The full list can be found \u003Ca href=\"http://docs.blackberry.com/en/developers/deliverables/29272/BlackBerry_Browser_for_BlackBerry_7-New_In_This_Release--1677266-0531020015-001-7.0-US.pdf\">here\u003C/a>, but the highlights include support for native touch and cursor events, web socket API support and new media support in the form of  &lt;audio&gt; and &lt;video&gt; elements, as well as the &lt;source&gt; element which lets you specify a media resource.  There is also a new \u003Cspan>JIT (Just-In-Time) JavaScript® compiler that offers better performance.\u003C/span>\u003C/p>\n\u003Cp>\u003Cstrong>Windows Phone 7\u003C/strong> uses the same code base as Internet Explorer 9, in a lot of cases, phone issues replicate onto the desktop. If there are issues on the mobile OS, testing on IE9 could be the fastest way to identify the issue, the mobile version is essentially a stripped down version of the desktop browser, as such, don't expect high levels of support for technologies.  The lack of font embedding is of a particular annoyance, for this reason, IE9 and WP7 fall into smartphone with low level support category.  \u003Ca class=\"external-link\" href=\"http://en.wikipedia.org/wiki/Windows_Phone_7.5\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Windows_Phone_7.5\u003C/a>  Windows Phone 8 will launch with a new, IE10 based browser with much better HTML5 support, at time of writing, it still is being developed.  Windows phone 7 is now a deprecated platform, with not more updates coming, as usage goes down, so will the need to support this browser.\u003C/p>\n\u003Cp>\u003Cstrong>Windows Phone 8\u003C/strong> has an IE10 based browser with much better HTML5 support, at time of writing, it still has some issues.  IE 10 has a large bug with viewport tags, this leads to hardware pixels being detected rather than software.  There is a fix in the works from Microsoft, no timeline on that though.  IE 10 offers a much better support listed \u003Ca href=\"http://blogs.windows.com/windows_phone/b/wpdev/archive/2012/11/08/internet-explorer-10-brings-html5-to-windows-phone-8-in-a-big-way.aspx\">here\u003C/a>, some of the things IE10 mobile can't support are Inline video, Some new manipulation views APIs for touch panning and zooming, with the exception of –ms-touch-action, Multi-track HTML5 audio (simultaneous), ActiveX and VBScript, Drag-and-drop APIs, File access APIs with the exception of blobs which are supported on Windows Phone 8.\u003C/p>\n\u003Cp>\u003Cstrong>Symbian\u003C/strong> phones are a large combination of S60, S40 and Symbian Belle, there is massive variations in the devices as there are hardware and software limitations of the range. In most cases, the S40 phones are feature phones. Some phones are being logged with a new webkit browser; this browser can do some JavaScript, however the usage figures are low as it requires a manual update. The S60 phones are also mostly features phones, though Nokia is updating many phones with a newer browser that supports up to HTML 5.0 and JavaScript 1.8, most of these phones will receive the feature phone version of the site, however these are low usage devices. In stats, these phones can appear as either S60 or S9.x, the latest version of the S9.x is 9.5. Finally there is a new version of Symbian, sometimes seen as Nokia OS or Symbian Belle, this version is also a hit and miss support OS. While this version is low usage, it could become more common in emerging countries. Given that Nokia doesn't use Over The Air (OTA) updates for its phones, users of the Nokia N8, the latest Symbian phone, could be running any one of 3 different versions of Symbian, each with its own version of the webkit browser, only the latest version supports HTML 5. Some phones will also come with some version coming with Opera, netfront and webkit depending on location, so Symbian testing is very limiting when trying to ensure maximum support. In our testing, we should focus on the Nokia N95 and Nokia N8, these feature different versions of S60 and have different browser versions, so provide a solid range of old browsers in Africa, with modern the UK. The most recent UK launch of a Nokia Symbian phone is the N8, which currently is classed as a feature phone, even though it support is relatively good, it does not cut the mustard. The best browser for this type of Symbian phone is Opera mobile, fast and fully feature, we do a lot more testing of Opera on Symbian S60 with Opera than any other platform. Link to different Symbian versions: \u003Ca class=\"external-link\" href=\"http://en.wikipedia.org/wiki/Web_Browser_for_S60\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Web_Browser_for_S60\u003C/a>\u003C/p>\n\u003Cp>\u003Cstrong>Symbian 10\u003C/strong> is also known as Symbian Belle, a new brand from Nokia.  This comes with a brand new browser which is Webkit based and supports lots of good HTML5 features.  There a few notable issues, such as font embed support, but it brings a few Nokia/Symbian phones into the realm of HTML5, which is a good thing.  Check out my post here for more i\u003Ca href=\"http://mobiletestingfordummies.tumblr.com/post/31395546379/keep-up-to-date\">nfo\u003C/a>\u003C/p>\n\u003Cp>\u003Cstrong>Silk Browser\u003C/strong>.  Amazon has forked Android 2.2 to create the Kindle OS with Silk browser. This browser routes all traffic through Amazon servers and can affect some elements of the website. Form submission is something that should be thoroughly tested to ensure the server is able to receive the form data. You may also find that because of the form factor, you will have to make a decision, only use smartphone layouts or use both tablet and smartphone layouts, this is mostly due to the screen width being able to support tablet web pages. But what is most troubling for the tester is that you may have to test both of these layouts on the say device, this is going to really annoy you.\u003C/p>\n\u003Cp>\u003Cstrong>Silk Browser version 2\u003C/strong> is a fork of 4.0 with the AOSP browser.  This browser has had some changes for the cloud sync, but has the same support as Android 4 stock browser.\u003C/p>\n\u003Cp>The \u003Cstrong>iPad\u003C/strong> is currently the market leader, with the single device having 60% of the market, the market grows every month though, so the user base is ever growing.  While the differences to the iPad browser are subtle to the iPhone browser, it is important to look at the iPad as the biggest device, in terms of screen size, that users will visit with.  While this device can support most, if not all, desktop sites with ease, it is important to remember that speed and simplicity are often desired more than functionality.  For this reason, users of iPads and other tablets, will use the mobile version of the site, especially if they are using a data service.  If it is outside of the scope of your project, you should consider potential markets with the iPad, responsive news' largest device support is 7 inches, however testing is conducted on the iPad.\u003C/p>\n\u003Cp>\u003Cstrong>Browsers\u003C/strong>\u003Cbr />-----------------------\u003C/p>\n\u003Cp>The next section will include information about the browsers that you will need to test in addition to the native browsers we have already talked about.  Just to reiterate, all the operating systems we have talked about so far have their own native browser.\u003C/p>\n\u003Cp>Third party browsers that have big impact in the world include:\u003C/p>\n\u003Cp>UC Browser - \u003Ca href=\"http://www.ucweb.com/English/UCbrowser/download.html\">Information\u003C/a>\u003C/p>\n\u003Cp>Chrome Mobile - \u003Ca href=\"https://www.google.com/intl/en/chrome/browser/mobile/\">Information\u003C/a>\u003C/p>\n\u003Cp>Opera Mini/Mobile - \u003Ca href=\"http://www.opera.com/mobile/\">Information\u003C/a>\u003C/p>\n\u003Cp>Firefox - \u003Ca href=\"http://www.mozilla.org/en-US/mobile/\">Information\u003C/a>\u003C/p>\n\u003Cp>As we are test for users all over the world, UC browser and Opera Mini may be less important to you.  These 2 browsers combined represent a sizeable chunk of Africa and Asia browsing traffic, so if you are in that region, you are probably already testing on it.  Firefox is growing, but they are updating rapidly, so testing on the latest stable build is advisable, especially as it is more than likely the user will upgrade.  The complete custom build is only available on Android.  Chrome is not yet the default browser, it will also sit alongside the native Android browser, at least for now.  Testing on both is probably the best approach, similar to Firefox, testing the latest build is best.  There is a version of Chrome for iOS, but this uses the same version of Webkit as the version of IOS you are on.  The Android version of Chrome is update frequently and is the only place to get the complete Chrome build.\u003C/p>\n\u003Cp>\u003Cstrong>Summary\u003C/strong>\u003Cbr />-----------------------\u003C/p>\n\u003Cp>In total, you final device list could have other smaller markets included, our current device list looks something like this :\u003C/p>\n\u003Cul class=\"alternate\">\u003Cli>iOS 6 - iPhone\u003C/li>\n\u003Cli>iOS 6 - iPad mini\u003C/li>\n\u003Cli>iOS 6 - iPad\u003C/li>\n\u003Cli>iOS 5 - iPad\u003C/li>\n\u003Cli>iOS 5 - iPod\u003C/li>\n\u003Cli>Android 2.1 - HTC desire\u003C/li>\n\u003Cli>Android 2.2 - Samsung Galaxy S\u003C/li>\n\u003Cli>Android 2.3 - Samsung Galaxy S\u003C/li>\n\u003Cli>Android 3.2 - Motorola Xoom\u003C/li>\n\u003Cli>Android 4.0 - Galaxy Nexus (Stock and Chrome)\u003C/li>\n\u003Cli>Android 4.1 - Galaxy Nexus (Stock and Chrome)\u003C/li>\n\u003Cli>Blackberry OS 4 - Curve 8520 (We have recently dropped testing this browser)\u003C/li>\n\u003Cli>Blackberry OS 5 - Bold 9700\u003C/li>\n\u003Cli>Blackberry OS 6 - Bold 9700\u003C/li>\n\u003Cli>Blackberry OS 7 - Bold 9860\u003C/li>\n\u003Cli>Windows Phone 7.5 - Lumia 800\u003C/li>\n\u003Cli>Windows Phone 8 - Lumia 920\u003C/li>\n\u003Cli>Symbian S60 - N95\u003C/li>\n\u003Cli>Symbain S60 - N8\u003C/li>\n\u003Cli>Symbian S40 - C3\u003C/li>\n\u003Cli>Symbian S40 - Asha 303\u003C/li>\n\u003Cli>Kindle E-Reader\u003C/li>\n\u003Cli>Kindle Fire - Android 2.2 Fork\u003C/li>\n\u003Cli>Kindle Fire HD - Android 4 Fork\u003C/li>\n\u003Cli>Android 3.0 - Samsung 7.7\u003C/li>\n\u003Cli>Android 4 - Samsung 7.0 v2\u003C/li>\n\u003Cli>Android 4.2 - Nexus 7 \u003C/li>\n\u003Cli>IE 10 - Microsoft Surface\u003C/li>\n\u003C/ul>\u003Cp>Edit 20/6/2012 - We have recently determined our 3 in market tablets to look at, Kindle fire, BB Playbook and Galaxy Tab 7.7.  They are all webkit based browsers, but have some differences in support, as well as volume of sales.  The 7.7 uses a pixel ratio of 1, whereas the other 2 dont, this means that the break points will have to be correctly established for tablets in order for users not to receive broken features.\u003C/p>\n\u003Cp>Edit 29/8/2012 - We are no longer testing on BBOS4 anymore, we have found the share to drop below our support level and global usage has dropped to an all time low.  We have added other versions of Android and other hardware sets, such as 7 inch tablets.\u003C/p>\n\u003Cp>Edit 13/9/2012 - I have made some big changes to the testing list this month, we have changed the minimum level of support to start at Android 2.2 and BBOS6, this is due to total browser market shifts on the site as well as globally and within the UK.\u003C/p>\n\u003Cp>Edit 13/11/2012 - I have now updated list to include changes to BBOS7, IE 10 on Windows and some Android and iOS support changes.  \u003C/p>\n\u003Cp>Edit 16/01/2013 - I have updated Windows phone to indicate our new support list, as well as changing our testing devices\u003C/p>","src/content/blog/2012-03-28-responsive-testing.md","5f1f107d96a05e0c",{"html":140,"metadata":141},"\u003Cp>We thought we'd share some information on our testing process.\u003C/p>\n\u003Cp>My name is David Blooman, I am the principal tester for responsive news, all questions and queries can be sent to me at \u003Ca href=\"https://twitter.com/dblooman\">@dblooman\u003C/a>\u003C/p>\n\u003Cp>When testing the devices, there are 4 main areas to cover, feature phones, smart phones with low level support, smart phones and tablets. The primary tablet market will be 7-inch devices up to 720x1280 pixels with the lowest smart phone being 240x360 pixels.\u003Cbr> Currently, we are only approaching devices that are not conforming to desktops, so smart TV's, games consoles and other exotic devices are test, desktops are not.\u003C/p>\n\u003Cimg src=\"/images/devices.jpg\" class=\"img-responsive\" alt=\"Devices\">\n\u003Cp>We do test desktop browsers, Chrome, Firefox, IE, however we typical test the version which our stats point to, or when there was a feature update.  As some browsers use auto updating services, it can often be a case of waiting for the next release to switch testing to a new version. \u003C/p>\n\u003Cp>When testing on a desktop, the hardware does play an important part.  The pixel density of large desktop screens, which is typically low, means testing on a desktop screen may prove fruitless, given that any CSS issues may be masked.  When testing for mobile, images may also appear over blown or scaled incorrectly, given the larger screen.  \u003C/p>\n\u003Cp>The final part of our per-requisite list is cutting the mustard -\u003Ca class=\"external-link\" href=\"http://responsivenews.co.uk/post/18948466399/cutting-the-mustard\" rel=\"nofollow\"> blog post\u003C/a>\u003C/p>\n\u003Cp>Minimum Device Testing\u003Cbr> ----------------------\u003C/p>\n\u003Cul class=\"alternate\">\u003Cli>Symbian S60 - N96 (Symbian 9.3)\u003Cbr> This phone represents the lowest of our requirements, mobile barlesque, our header, is 240px wide, meaning that our lowest support width is the same. The Nokia N96 is a great test case to identify the impacts to other dependencies within a webpage. This phone is also widely used, given the low cost nature of the OS(open source Symbian), as well as having a strong user base in our stats.  The OS also supports opera mini/mobile for comparing a non JS/JS experience\u003C/li>\n\u003Cli>Android 2.2 Native Browser- Galaxy S\u003Cbr> This phone is our minimum Android OS support for smart phones, this is based upon the fact that it cuts the mustard in our tests. While the user base for this OS is dropping due to more recent versions of Android, 4.0 etc.  This devices is a good hardware point to test against as well. It features a 1ghz CPU, which was less optimised when 2.2 was launched, meaning that 2.2 on a slower CPU may produce a smoother experience. With manual testing, we have discovered a consistency across Android in that, if it works on 2.2, it has a strong chance of working on later versions. Android 2.2 is the best browser to test against for maximum stability across the Android user base, from 2.1 to 2.3.3\u003C/li>\n\u003Cli>iOS 6 Native Browser - iPhone4S\u003Cbr> This browser has taken most of the iOS user base, being stable and having a lot of the more advanced features of HTML5.  The browser produces a huge amount of web traffic globally, so testing this is a must for any large commercial website.\u003C/li>\n\u003Cli>Blackberry OS 6 - Bold 9700\u003Cbr> BBOS6 is not the latest version of the OS, but does feature a near identical browser as 7, with minor improvements seen in 7. Even so, the uptake of 7 is light, so the focus should be on 6. This is for 2 reasons, 1, BBOS6 supports a Webkit browser, 2, the browser cuts the mustard. What is important to note about BBO6 is the Curve range.  The curve range is a budget range of phones with lower report rates or resolutions.  The 9300 for example, has a strong user base, but it also is a good indicator of how the smartphone experience is impacted by a screen with a small vertical height.  For this reason, testing on a 9300 could give you a skewed result, the same for only testing on the 9700.  If you are only testing features that have no layouts and CSS changes, the 9700 is the better of the two, but testing on the 9300 will give you an idea of the budget uses experience.\u003C/li>\n\u003Cli>Nexus 7\u003Cbr>Android 4.1 was launched on the most popular Android tablet, the Nexus 7.  With 4.1 came Chrome for Android by default, this represents a shift away from the native browser, which many Android OEMs customise.  By having a consistent browser across all devices, Google makes testing Chrome easier too.  More than that, the Nexus 7 size, resolution and pixel density make it a solid device to test tablet and phone layouts.\u003C/li>\n\u003C/ul>\u003Cp>Review\u003Cbr> ----------------------\u003C/p>\n\u003Cp>The list above may be different to the list many others have come up with, this is because many testers will focus on existing markets and user base, which is Android and iOS oriented at the moment.  This type of focus is inadvisable given the ease of switching services from apps to web, other competitors to yours.  This under estimation of the market and how quickly it can change may lead you to not fully support a browser or entire range of devices.  In many test devices list, you will see Android 2.1,2.2,2.3 and 4 devices listed as more priority over Symbian devices.  The project will then focus on using emulators with opera installed for Symbian testing, which may provide such a different result, that it could lose market share in Africa, for example.  The devices above will cover different break points for design, but also play a secondary part, third party browser testing.  Opera is the most popular mobile browser, which is supported on iOS, Android and Symbian, it brings HTML5 support to even the N95.  For this reason, Opera should be looked at closely on these devices if you looking into lots of different markets and countries.  Opera Mini also plays a big part, the news website is reduced to feature phone look as Opera Mini uses proxy servers, disabling most of the Javascript on the page.  A Galaxy Nexus with a 720p screen can make some websites look terrible when using Opera Mini, ensuring you have Opera on you test list is key to success. \u003C/p>\n\u003Cp>Hardware Tests\u003Cbr> ----------------------\u003C/p>\n\u003Cul class=\"alternate\">\u003Cli>Device of 240 pixels wide screen - N95\u003C/li>\n\u003Cli>Device of more than 1000 wide screen - Any device\u003C/li>\n\u003Cli>Touch Screen - Any device will be relevant, this is to test hit states, button placements and general usage while using a device with on screen keyboard.\u003C/li>\n\u003Cli>Non-touch - In most cases, Symbian and blackberry phones will fall into this category, how the user can navigate the site is of importance, so testing the functionality of the site with such devices is a high priority.\u003C/li>\n\u003C/ul>\u003Cp>These 4 additional requirements may require additional devices, but in investing in this equipment, the devices will cover most of the core stats and you will be able to test against the majority of browsers and input methods.  This should allow you to see where your problem areas are going to be, as you can quickly identify which browsers have the key levels of JavaScript and technologies support required.\u003C/p>\n\u003Cp>It is recommended that you clear all cookies, cache and reset browser to default before beginning any testing. Not doing this can lead to older versions of the site interfering with testing as you always must look at the latest version of the site. With builds occurring all the time, minor changes will often show little impact, such as back end work. Front end work can mean changes to the site that will fundamentally affect testing, as the look can be radically different on certain devices. You should always consider this when working, there is no such thing as a minor change when testing responsive web pages.\u003C/p>\n\u003Cp>Further Testing\u003Cbr> ---------------\u003C/p>\n\u003Cp>For further testing, the following list identifies the most common Mobile OS's. Below is explanation of the differences between the OS's and browsers, this could help you determine whether you testing need to extend to all the devices.\u003Cbr> The Android 2.3 OS has the the largest user base of Android, therefor the most users on that versions browser. We use the Galaxy S as 1, it is one of the most popular devices in our stats, 2, it has a typical size screen and pixel density for that OS, meaning a more balanced approach when doing UI testing.\u003Cbr> The Bold 9700 is again, one of the more popular devices, but it also represents a crossover device from OS5 to OS6. It's screen is of a good pixel size for Blackberrys and its internals are one of the more high specifications for a feature phone.\u003Cbr> Windows phone is a good way of testing IE10, as a large amount of the code base was used to create the browser on WP8, though mobile has less features. \u003C/p>\n\u003Cul class=\"alternate\">\u003Cli>iOS 6 - iphone 4\u003C/li>\n\u003Cli>Android 2.2 - HTC desire\u003C/li>\n\u003Cli>Android 2.3 - Samsung Galaxy S\u003C/li>\n\u003Cli>Android 4 - Both stock and Chrome browsers\u003C/li>\n\u003Cli>Blackberry OS 5 - Bold 9700\u003C/li>\n\u003Cli>Blackberry OS 6 - Bold 9700\u003C/li>\n\u003Cli>Blackberry OS 7 - 9380\u003C/li>\n\u003Cli>Windows Phone 8 - Lumia 920\u003C/li>\n\u003Cli>Symbian S60 - Symbian Browser\u003C/li>\n\u003Cli>Symbian S40 - Nokia Browser\u003C/li>\n\u003Cli>Nexus 7\u003C/li>\n\u003Cli>iOS 6 - iPad\u003C/li>\n\u003C/ul>\u003Cp>\u003Cstrong>Browser and OS Analysis\u003C/strong>\u003Cbr> -----------------------\u003C/p>\n\u003Cp>While emulators are very useful in a lab style environment, they often miss things that are only found when using handsets. There is also a lot of nuances that you will discover when you use this many devices. Below is a combination of personal experience, information from other sources and my opinion on the direction when testing responsive design.\u003C/p>\n\u003Cp>\u003Cstrong>iOS.\u003C/strong> The iphone and ipod range from the 3Gs upwards all support iOS5, which has support for most standards, the browser on iOS5 is the same on every device, therefore testing on 1 device is suitable. The second most supported version of the OS which supports a slightly lower performing browser is iOS4, however this has not performed differently except in hardware (larger screen) Here is a comparison of the 2 versions \u003Ca class=\"external-link\" href=\"http://www.blaze.io/mobile/ios5-top10-performance-changes/\" rel=\"nofollow\">http://www.blaze.io/mobile/ios5-top10-performance-changes/\u003C/a> - The range of iOS devices represents nearly 20 million page views per month, hence it's high priority status.\u003C/p>\n\u003Cp>\u003Cstrong>Android\u003C/strong> 2.1 is the lowest version of Android to cut the mustard. Android has varying levels of support, Android 2.2 browser and onwards has been the most consistent, with Android 2.2 and 2.3 being the highest user base. Android 2.1 has the ability to support many smart phone features, but lacks some font embedding and other technologies, such as Flash. This has proven to be one of the more problematic devices, but is considered a baseline phone for smart phone support. This particular version of Android is in decline as the jump from 2.1 to 2.2 is being made, or the user upgrades to a different phone. Android 2.1 represents about 25% of the Android user base with a million page views per month.\u003C/p>\n\u003Cp>\u003Cstrong>Android 2.2\u003C/strong> was the last version to be open source, so a lot of devices used this version for Google TVs, phones, music players etc. The browser for 2.3 is also based on this version. Differences in the operating system can have some impact on the browser, but the difference between 2.2 and 2.3 is mostly down to how the particular vendor changes the phone. A change log is listed in the link below \u003Ca class=\"external-link\" href=\"http://www.knowitsdifference.com/difference-between-android-2-2-and-android-2-3/\" rel=\"nofollow\">http://www.knowitsdifference.com/difference-between-android-2-2-and-android-2-3/\u003C/a> Android 2.2 is a measurable jump in performance over 2.1, the user base is almost 3 times as big with nearly 8 million page views per month. Android 2.3 offers an overall system update that is relevant to user, not as much for browsers though. In our test, there have been less that 1% of difference between the 2, with only 1 or 2 bugs discovered not to replicate on both versions. The 7 million combined users with 20 million page views shows us how important it is to correctly test on Android 2.x versions.\u003Cbr> What is important to note about 2.2 is the browser features a shrunk down version of desktop Chrome's V8 JavaScript engine, Google have integrated it into the Android 2.3 OS to drastically improve JS performance. When testing Android 2.1 and 2.2, the should be treated as different entities, rather than just a point release, such as 2.2 and 2.3.\u003C/p>\n\u003Cp>\u003Cstrong>Android 3\u003C/strong> is the tablet skew of the OS. 3.2 and above supports lower screen sizes that mean you can have a large range of devices, so testing on this version is best. The update of Android tablets is still very small compared to iOS devices, like the iPad, however the browser is very high quality. The Android 3 OS is essentially a fork of Android 2.2, as such is inherits much of the browser from there. However there is added support for multi touch within Javascript and improved zooming options.\u003C/p>\n\u003Cp>\u003Cstrong>Android 4\u003C/strong> is the newest and most advanced version of the OS, it currently supports 2 browsers, Chrome and the Android browser. It seems that Google intend to replace the stock browser with Chrome on upcoming devices, so testing on both until this is that case is prudent. The current version of Chrome uses a new version of Webkit than the stock android browser. Info on browser \u003Ca class=\"external-link\" href=\"http://www.android.com/about/ice-cream-sandwich/\" rel=\"nofollow\">http://www.android.com/about/ice-cream-sandwich/\u003C/a>. What it is important is the that Chrome is a Google product, therefore it may not be present on the open source version of the OS, so for future testing, stats will determine which webkit version and by extension, which browser is the most popular on the platform.  Android 4.1 brought a few improvements over 4.0, \u003Cspan>better HTML5 video support, supports the updated HTML5 Media Capture specification on input elements.WebView now supports vertical text, including Ruby Text and other vertical text glyphs.\u003C/span>\u003C/p>\n\u003Cp>\u003Cspan>\u003Cstrong>Android 4.1\u003C/strong> had a few tweaks to the browser, but in reality, the focus is now on Chrome as the stock browser faces heavier competition.  Chrome Should be seen as an equal in testing to the stock browser.\u003C/span>\u003C/p>\n\u003Cp>\u003Cstrong>Blackberry OS 5\u003C/strong> is a pure feature phone experience, it's screen size support is one of the biggest factors, given the high landscape resolution, but low vertical. The browser has been the most consistent for feature phones, supporting all the requirements.\u003C/p>\n\u003Cp>\u003Cstrong>Blackberry OS 6\u003C/strong> has patchy support, it uses a webkit browser, but the support for font embedding has been hit and miss. The browser is currently one of the highest usage of the website, meaning that this is important OS for testing. Newer versions of the Blackberry browser seem to have fixed the issues we are having with 6. Browser Info \u003Ca class=\"external-link\" href=\"http://crackberry.com/blackberry-6-review\" rel=\"nofollow\">http://crackberry.com/blackberry-6-review\u003C/a> - new-browserhttp://crackberry.com/blackberry-6-review#new-browser One of the core issues with Blackberries is the fact that they have unusual screens. The majority of phones sold between 2008 and 2011 have a 480x360 screen; this is intended for reading emails instead of web pages. What should also be tested is what the screen reports itself as, on some devices the resolution is dropped to 360 wide due to hardware limitations. It is important to identify issues with the look and feel on a webpage based upon these types of devices, especially for us with more than 4 million users and 16 million page views.\u003C/p>\n\u003Cp>\u003Cstrong>Blackberry OS 7\u003C/strong> has improvements over the BBOS6 browser.  Most of the bugs have been fixed, while adding new features such as a web inspector and better video/audio support. The full list can be found \u003Ca href=\"http://docs.blackberry.com/en/developers/deliverables/29272/BlackBerry_Browser_for_BlackBerry_7-New_In_This_Release--1677266-0531020015-001-7.0-US.pdf\">here\u003C/a>, but the highlights include support for native touch and cursor events, web socket API support and new media support in the form of  &#x3C;audio> and &#x3C;video> elements, as well as the &#x3C;source> element which lets you specify a media resource.  There is also a new \u003Cspan>JIT (Just-In-Time) JavaScript® compiler that offers better performance.\u003C/span>\u003C/p>\n\u003Cp>\u003Cstrong>Windows Phone 7\u003C/strong> uses the same code base as Internet Explorer 9, in a lot of cases, phone issues replicate onto the desktop. If there are issues on the mobile OS, testing on IE9 could be the fastest way to identify the issue, the mobile version is essentially a stripped down version of the desktop browser, as such, don't expect high levels of support for technologies.  The lack of font embedding is of a particular annoyance, for this reason, IE9 and WP7 fall into smartphone with low level support category.  \u003Ca class=\"external-link\" href=\"http://en.wikipedia.org/wiki/Windows_Phone_7.5\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Windows_Phone_7.5\u003C/a>  Windows Phone 8 will launch with a new, IE10 based browser with much better HTML5 support, at time of writing, it still is being developed.  Windows phone 7 is now a deprecated platform, with not more updates coming, as usage goes down, so will the need to support this browser.\u003C/p>\n\u003Cp>\u003Cstrong>Windows Phone 8\u003C/strong> has an IE10 based browser with much better HTML5 support, at time of writing, it still has some issues.  IE 10 has a large bug with viewport tags, this leads to hardware pixels being detected rather than software.  There is a fix in the works from Microsoft, no timeline on that though.  IE 10 offers a much better support listed \u003Ca href=\"http://blogs.windows.com/windows_phone/b/wpdev/archive/2012/11/08/internet-explorer-10-brings-html5-to-windows-phone-8-in-a-big-way.aspx\">here\u003C/a>, some of the things IE10 mobile can't support are Inline video, Some new manipulation views APIs for touch panning and zooming, with the exception of –ms-touch-action, Multi-track HTML5 audio (simultaneous), ActiveX and VBScript, Drag-and-drop APIs, File access APIs with the exception of blobs which are supported on Windows Phone 8.\u003C/p>\n\u003Cp>\u003Cstrong>Symbian\u003C/strong> phones are a large combination of S60, S40 and Symbian Belle, there is massive variations in the devices as there are hardware and software limitations of the range. In most cases, the S40 phones are feature phones. Some phones are being logged with a new webkit browser; this browser can do some JavaScript, however the usage figures are low as it requires a manual update. The S60 phones are also mostly features phones, though Nokia is updating many phones with a newer browser that supports up to HTML 5.0 and JavaScript 1.8, most of these phones will receive the feature phone version of the site, however these are low usage devices. In stats, these phones can appear as either S60 or S9.x, the latest version of the S9.x is 9.5. Finally there is a new version of Symbian, sometimes seen as Nokia OS or Symbian Belle, this version is also a hit and miss support OS. While this version is low usage, it could become more common in emerging countries. Given that Nokia doesn't use Over The Air (OTA) updates for its phones, users of the Nokia N8, the latest Symbian phone, could be running any one of 3 different versions of Symbian, each with its own version of the webkit browser, only the latest version supports HTML 5. Some phones will also come with some version coming with Opera, netfront and webkit depending on location, so Symbian testing is very limiting when trying to ensure maximum support. In our testing, we should focus on the Nokia N95 and Nokia N8, these feature different versions of S60 and have different browser versions, so provide a solid range of old browsers in Africa, with modern the UK. The most recent UK launch of a Nokia Symbian phone is the N8, which currently is classed as a feature phone, even though it support is relatively good, it does not cut the mustard. The best browser for this type of Symbian phone is Opera mobile, fast and fully feature, we do a lot more testing of Opera on Symbian S60 with Opera than any other platform. Link to different Symbian versions: \u003Ca class=\"external-link\" href=\"http://en.wikipedia.org/wiki/Web_Browser_for_S60\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Web_Browser_for_S60\u003C/a>\u003C/p>\n\u003Cp>\u003Cstrong>Symbian 10\u003C/strong> is also known as Symbian Belle, a new brand from Nokia.  This comes with a brand new browser which is Webkit based and supports lots of good HTML5 features.  There a few notable issues, such as font embed support, but it brings a few Nokia/Symbian phones into the realm of HTML5, which is a good thing.  Check out my post here for more i\u003Ca href=\"http://mobiletestingfordummies.tumblr.com/post/31395546379/keep-up-to-date\">nfo\u003C/a>\u003C/p>\n\u003Cp>\u003Cstrong>Silk Browser\u003C/strong>.  Amazon has forked Android 2.2 to create the Kindle OS with Silk browser. This browser routes all traffic through Amazon servers and can affect some elements of the website. Form submission is something that should be thoroughly tested to ensure the server is able to receive the form data. You may also find that because of the form factor, you will have to make a decision, only use smartphone layouts or use both tablet and smartphone layouts, this is mostly due to the screen width being able to support tablet web pages. But what is most troubling for the tester is that you may have to test both of these layouts on the say device, this is going to really annoy you.\u003C/p>\n\u003Cp>\u003Cstrong>Silk Browser version 2\u003C/strong> is a fork of 4.0 with the AOSP browser.  This browser has had some changes for the cloud sync, but has the same support as Android 4 stock browser.\u003C/p>\n\u003Cp>The \u003Cstrong>iPad\u003C/strong> is currently the market leader, with the single device having 60% of the market, the market grows every month though, so the user base is ever growing.  While the differences to the iPad browser are subtle to the iPhone browser, it is important to look at the iPad as the biggest device, in terms of screen size, that users will visit with.  While this device can support most, if not all, desktop sites with ease, it is important to remember that speed and simplicity are often desired more than functionality.  For this reason, users of iPads and other tablets, will use the mobile version of the site, especially if they are using a data service.  If it is outside of the scope of your project, you should consider potential markets with the iPad, responsive news' largest device support is 7 inches, however testing is conducted on the iPad.\u003C/p>\n\u003Cp>\u003Cstrong>Browsers\u003C/strong>\u003Cbr>-----------------------\u003C/p>\n\u003Cp>The next section will include information about the browsers that you will need to test in addition to the native browsers we have already talked about.  Just to reiterate, all the operating systems we have talked about so far have their own native browser.\u003C/p>\n\u003Cp>Third party browsers that have big impact in the world include:\u003C/p>\n\u003Cp>UC Browser - \u003Ca href=\"http://www.ucweb.com/English/UCbrowser/download.html\">Information\u003C/a>\u003C/p>\n\u003Cp>Chrome Mobile - \u003Ca href=\"https://www.google.com/intl/en/chrome/browser/mobile/\">Information\u003C/a>\u003C/p>\n\u003Cp>Opera Mini/Mobile - \u003Ca href=\"http://www.opera.com/mobile/\">Information\u003C/a>\u003C/p>\n\u003Cp>Firefox - \u003Ca href=\"http://www.mozilla.org/en-US/mobile/\">Information\u003C/a>\u003C/p>\n\u003Cp>As we are test for users all over the world, UC browser and Opera Mini may be less important to you.  These 2 browsers combined represent a sizeable chunk of Africa and Asia browsing traffic, so if you are in that region, you are probably already testing on it.  Firefox is growing, but they are updating rapidly, so testing on the latest stable build is advisable, especially as it is more than likely the user will upgrade.  The complete custom build is only available on Android.  Chrome is not yet the default browser, it will also sit alongside the native Android browser, at least for now.  Testing on both is probably the best approach, similar to Firefox, testing the latest build is best.  There is a version of Chrome for iOS, but this uses the same version of Webkit as the version of IOS you are on.  The Android version of Chrome is update frequently and is the only place to get the complete Chrome build.\u003C/p>\n\u003Cp>\u003Cstrong>Summary\u003C/strong>\u003Cbr>-----------------------\u003C/p>\n\u003Cp>In total, you final device list could have other smaller markets included, our current device list looks something like this :\u003C/p>\n\u003Cul class=\"alternate\">\u003Cli>iOS 6 - iPhone\u003C/li>\n\u003Cli>iOS 6 - iPad mini\u003C/li>\n\u003Cli>iOS 6 - iPad\u003C/li>\n\u003Cli>iOS 5 - iPad\u003C/li>\n\u003Cli>iOS 5 - iPod\u003C/li>\n\u003Cli>Android 2.1 - HTC desire\u003C/li>\n\u003Cli>Android 2.2 - Samsung Galaxy S\u003C/li>\n\u003Cli>Android 2.3 - Samsung Galaxy S\u003C/li>\n\u003Cli>Android 3.2 - Motorola Xoom\u003C/li>\n\u003Cli>Android 4.0 - Galaxy Nexus (Stock and Chrome)\u003C/li>\n\u003Cli>Android 4.1 - Galaxy Nexus (Stock and Chrome)\u003C/li>\n\u003Cli>Blackberry OS 4 - Curve 8520 (We have recently dropped testing this browser)\u003C/li>\n\u003Cli>Blackberry OS 5 - Bold 9700\u003C/li>\n\u003Cli>Blackberry OS 6 - Bold 9700\u003C/li>\n\u003Cli>Blackberry OS 7 - Bold 9860\u003C/li>\n\u003Cli>Windows Phone 7.5 - Lumia 800\u003C/li>\n\u003Cli>Windows Phone 8 - Lumia 920\u003C/li>\n\u003Cli>Symbian S60 - N95\u003C/li>\n\u003Cli>Symbain S60 - N8\u003C/li>\n\u003Cli>Symbian S40 - C3\u003C/li>\n\u003Cli>Symbian S40 - Asha 303\u003C/li>\n\u003Cli>Kindle E-Reader\u003C/li>\n\u003Cli>Kindle Fire - Android 2.2 Fork\u003C/li>\n\u003Cli>Kindle Fire HD - Android 4 Fork\u003C/li>\n\u003Cli>Android 3.0 - Samsung 7.7\u003C/li>\n\u003Cli>Android 4 - Samsung 7.0 v2\u003C/li>\n\u003Cli>Android 4.2 - Nexus 7 \u003C/li>\n\u003Cli>IE 10 - Microsoft Surface\u003C/li>\n\u003C/ul>\u003Cp>Edit 20/6/2012 - We have recently determined our 3 in market tablets to look at, Kindle fire, BB Playbook and Galaxy Tab 7.7.  They are all webkit based browsers, but have some differences in support, as well as volume of sales.  The 7.7 uses a pixel ratio of 1, whereas the other 2 dont, this means that the break points will have to be correctly established for tablets in order for users not to receive broken features.\u003C/p>\n\u003Cp>Edit 29/8/2012 - We are no longer testing on BBOS4 anymore, we have found the share to drop below our support level and global usage has dropped to an all time low.  We have added other versions of Android and other hardware sets, such as 7 inch tablets.\u003C/p>\n\u003Cp>Edit 13/9/2012 - I have made some big changes to the testing list this month, we have changed the minimum level of support to start at Android 2.2 and BBOS6, this is due to total browser market shifts on the site as well as globally and within the UK.\u003C/p>\n\u003Cp>Edit 13/11/2012 - I have now updated list to include changes to BBOS7, IE 10 on Windows and some Android and iOS support changes.  \u003C/p>\n\u003Cp>Edit 16/01/2013 - I have updated Windows phone to indicate our new support list, as well as changing our testing devices\u003C/p>",{"headings":142,"localImagePaths":143,"remoteImagePaths":144,"frontmatter":145,"imagePaths":147},[],[],[],{"title":133,"description":134,"pubDate":146},"2012-03-28",[],"2015-02-22-real-time-gatling-results-with-docker-and-influxdb",{"id":148,"data":150,"body":154,"filePath":155,"digest":156,"rendered":157},{"title":151,"description":152,"pubDate":153},"Real Time Gatling Results With Docker And Influxdb","Gatling http://gatling.io/ is an open source, Scala based load testing tool. It offers a simple proxy based web recorder for following user journeys, or alte...",["Date","2015-02-22T00:00:00.000Z"],"[Gatling](http://gatling.io/) is an open-source, Scala based load testing tool.  It offers a simple proxy based web recorder for following user journeys, or alternatively you can codify the user journey via Gatling's Scala based DSL.\n\n### Gatling\nWith BBC moving most of its infrastructure to AWS, each product team needs to ensure its stack is stable.  News has started to load test its own components with consultation from our internal load test team, rather than ask the team to do it.  Within my team, BBC News, we have recently moved to Gatling from [Apache Jmeter](http://jmeter.apache.org/); so far we have found Gatling to suite our needs better than Jmeter.  \n\nWe made this move because we wanted to get real time metrics from our EC2 instance that we run Jmeter on.  One of the common ways to store metrics is by using the [Graphite protocol](https://github.com/graphite-project).  Graphite offers a web interface, database and REST API. While a Graphite listener is in the master branch of Jmeter, it are not released, so at some point Jmeter will be offering this feature.  For now though, Gatling offers us this and much more, as well as being consistent with our load test team.\n\nIn order to ensure that Gatling was the right choice for us, I needed to explore the benefits of real time metrics and see what we would need to setup to record these results.  This meant choosing a front end dashboard and backend datastore that were compatible with Gatling, as well as not creating too much maintenance overhead.  The rest of the post will explain my choices and how Docker impacted them.\n\n### Storing the Data\nThe standard way of recording metrics in Gatling is by using the Graphite metrics protocol.  You must start the database and web service separately by running Python files.  On some OS's such as Ubuntu, there are pre-configured packages, these aren't available on CentOS though, which is the distribution we use.  Because of this, we decided to go with [InfluxDB](http://influxdb.com/), which accepts the Graphite protocol.\n\nInfluxDB is a time series database with a admin interface and a REST API all in one Go binary.  Pretty neat compared to the pain of setting up Graphite.\n\n### View Metrics in Grafana\nOnce InfluxDB was our database of choice, there was really only one option for the frontend dashboard, [Grafana](http://grafana.org/).  This dashboard has become something of a standard for people looking to visualise metrics, it is very customisable and works with Graphite, InfluxDB and OpenTSB.  \n\n### Docker?\nNow we have our technology choices, we need to get up and running.  Before I propose a lot of potential tasks to my team, we need to make sure it offers us benefits and fits our needs, so we need to do a few things.  Install python, get source code for both InfluxDB and Grafana, setup the databases, configure the InfluxDB backend and setup the Grafana front end.  Docker offered me a simple way to get this all running with no impact to my dev machine and make it easy to share with others so they can try it out.  \n\nWith the metrics offering in place, we can now get Gatling feeding in data, once I saw that first stream of data, I am once again reminded how simple Docker makes exploring new technology options.\n\n### Running InfluxDB and Grafana\nAll the Dockerfiles and configs for this setup are available on [Github](https://github.com/DaveBlooman/gatling-docker)\n\nTo setup and run the stack, run the start script, `./start`, this will download the images from the registry and run them.  Visit [localhost:8081](localhost:8081) or your boot2dockerIP [http://192.168.59.103:8081/](http://192.168.59.103:8081/) and you're all setup with your metric system.  \n\n### InfluxDB\n\nAll the options for InfluxDB are the influxdb directory, but the web interface is located at [localhost:8083](localhost:8083) or your boot2dockerIP and port e.g [http://192.168.59.103:8083/](http://192.168.59.103:8083/)\n\nLogin using `root:root` and click the database tab to view the gatling data. Once here, you can type list series to view all data in the database. InfluxDB uses a SQLish query language, so queries are straight forward once you know the data format.\n\n### Feeding Data in from Gatling\n\nI have included a sample gatling.conf in the repo which is mostly the vanilla conf except for writers section under the data block, as well as the graphite options. InfluxDB accepts data using the Graphite protocol, so by using these options, data will still go into the database as if it were Whisper database used by Graphite.\n\nOnce you have your load test running, you should data like this :\n\n\n\u003Ca href=\"/images/grafana.jpg\" alt=\"Grafana\">Link to image\u003C/a>\n\n\u003Cimg src=\"/images/grafana.jpg\" class=\"img-responsive\" alt=\"Grafana\">\n\n\nI have created a basic dashboard, but it gives some of the key data from the load test.\n\n### Making the Choice\n\nWhen making a technology choice, it can be easy to lose faith in the choice because the setup is so hard, especially when you may only want to try something for a few short period of time.  Together, Docker, InfluxDB and Grafana made the choice of using Gatling more about the capabilities and features and less about how quickly we can setup things up or get things working.","src/content/blog/2015-02-22-real-time-gatling-results-with-docker-and-influxdb.md","e164c6d91108da8f",{"html":158,"metadata":159},"\u003Cp>\u003Ca href=\"http://gatling.io/\">Gatling\u003C/a> is an open-source, Scala based load testing tool.  It offers a simple proxy based web recorder for following user journeys, or alternatively you can codify the user journey via Gatling’s Scala based DSL.\u003C/p>\n\u003Ch3 id=\"gatling\">Gatling\u003C/h3>\n\u003Cp>With BBC moving most of its infrastructure to AWS, each product team needs to ensure its stack is stable.  News has started to load test its own components with consultation from our internal load test team, rather than ask the team to do it.  Within my team, BBC News, we have recently moved to Gatling from \u003Ca href=\"http://jmeter.apache.org/\">Apache Jmeter\u003C/a>; so far we have found Gatling to suite our needs better than Jmeter.\u003C/p>\n\u003Cp>We made this move because we wanted to get real time metrics from our EC2 instance that we run Jmeter on.  One of the common ways to store metrics is by using the \u003Ca href=\"https://github.com/graphite-project\">Graphite protocol\u003C/a>.  Graphite offers a web interface, database and REST API. While a Graphite listener is in the master branch of Jmeter, it are not released, so at some point Jmeter will be offering this feature.  For now though, Gatling offers us this and much more, as well as being consistent with our load test team.\u003C/p>\n\u003Cp>In order to ensure that Gatling was the right choice for us, I needed to explore the benefits of real time metrics and see what we would need to setup to record these results.  This meant choosing a front end dashboard and backend datastore that were compatible with Gatling, as well as not creating too much maintenance overhead.  The rest of the post will explain my choices and how Docker impacted them.\u003C/p>\n\u003Ch3 id=\"storing-the-data\">Storing the Data\u003C/h3>\n\u003Cp>The standard way of recording metrics in Gatling is by using the Graphite metrics protocol.  You must start the database and web service separately by running Python files.  On some OS’s such as Ubuntu, there are pre-configured packages, these aren’t available on CentOS though, which is the distribution we use.  Because of this, we decided to go with \u003Ca href=\"http://influxdb.com/\">InfluxDB\u003C/a>, which accepts the Graphite protocol.\u003C/p>\n\u003Cp>InfluxDB is a time series database with a admin interface and a REST API all in one Go binary.  Pretty neat compared to the pain of setting up Graphite.\u003C/p>\n\u003Ch3 id=\"view-metrics-in-grafana\">View Metrics in Grafana\u003C/h3>\n\u003Cp>Once InfluxDB was our database of choice, there was really only one option for the frontend dashboard, \u003Ca href=\"http://grafana.org/\">Grafana\u003C/a>.  This dashboard has become something of a standard for people looking to visualise metrics, it is very customisable and works with Graphite, InfluxDB and OpenTSB.\u003C/p>\n\u003Ch3 id=\"docker\">Docker?\u003C/h3>\n\u003Cp>Now we have our technology choices, we need to get up and running.  Before I propose a lot of potential tasks to my team, we need to make sure it offers us benefits and fits our needs, so we need to do a few things.  Install python, get source code for both InfluxDB and Grafana, setup the databases, configure the InfluxDB backend and setup the Grafana front end.  Docker offered me a simple way to get this all running with no impact to my dev machine and make it easy to share with others so they can try it out.\u003C/p>\n\u003Cp>With the metrics offering in place, we can now get Gatling feeding in data, once I saw that first stream of data, I am once again reminded how simple Docker makes exploring new technology options.\u003C/p>\n\u003Ch3 id=\"running-influxdb-and-grafana\">Running InfluxDB and Grafana\u003C/h3>\n\u003Cp>All the Dockerfiles and configs for this setup are available on \u003Ca href=\"https://github.com/DaveBlooman/gatling-docker\">Github\u003C/a>\u003C/p>\n\u003Cp>To setup and run the stack, run the start script, \u003Ccode>./start\u003C/code>, this will download the images from the registry and run them.  Visit \u003Ca href=\"localhost:8081\">localhost:8081\u003C/a> or your boot2dockerIP \u003Ca href=\"http://192.168.59.103:8081/\">http://192.168.59.103:8081/\u003C/a> and you’re all setup with your metric system.\u003C/p>\n\u003Ch3 id=\"influxdb\">InfluxDB\u003C/h3>\n\u003Cp>All the options for InfluxDB are the influxdb directory, but the web interface is located at \u003Ca href=\"localhost:8083\">localhost:8083\u003C/a> or your boot2dockerIP and port e.g \u003Ca href=\"http://192.168.59.103:8083/\">http://192.168.59.103:8083/\u003C/a>\u003C/p>\n\u003Cp>Login using \u003Ccode>root:root\u003C/code> and click the database tab to view the gatling data. Once here, you can type list series to view all data in the database. InfluxDB uses a SQLish query language, so queries are straight forward once you know the data format.\u003C/p>\n\u003Ch3 id=\"feeding-data-in-from-gatling\">Feeding Data in from Gatling\u003C/h3>\n\u003Cp>I have included a sample gatling.conf in the repo which is mostly the vanilla conf except for writers section under the data block, as well as the graphite options. InfluxDB accepts data using the Graphite protocol, so by using these options, data will still go into the database as if it were Whisper database used by Graphite.\u003C/p>\n\u003Cp>Once you have your load test running, you should data like this :\u003C/p>\n\u003Cp>\u003Ca href=\"/images/grafana.jpg\" alt=\"Grafana\">Link to image\u003C/a>\u003C/p>\n\u003Cimg src=\"/images/grafana.jpg\" class=\"img-responsive\" alt=\"Grafana\">\n\u003Cp>I have created a basic dashboard, but it gives some of the key data from the load test.\u003C/p>\n\u003Ch3 id=\"making-the-choice\">Making the Choice\u003C/h3>\n\u003Cp>When making a technology choice, it can be easy to lose faith in the choice because the setup is so hard, especially when you may only want to try something for a few short period of time.  Together, Docker, InfluxDB and Grafana made the choice of using Gatling more about the capabilities and features and less about how quickly we can setup things up or get things working.\u003C/p>",{"headings":160,"localImagePaths":185,"remoteImagePaths":186,"frontmatter":187,"imagePaths":189},[161,164,167,170,173,176,179,182],{"depth":119,"slug":162,"text":163},"gatling","Gatling",{"depth":119,"slug":165,"text":166},"storing-the-data","Storing the Data",{"depth":119,"slug":168,"text":169},"view-metrics-in-grafana","View Metrics in Grafana",{"depth":119,"slug":171,"text":172},"docker","Docker?",{"depth":119,"slug":174,"text":175},"running-influxdb-and-grafana","Running InfluxDB and Grafana",{"depth":119,"slug":177,"text":178},"influxdb","InfluxDB",{"depth":119,"slug":180,"text":181},"feeding-data-in-from-gatling","Feeding Data in from Gatling",{"depth":119,"slug":183,"text":184},"making-the-choice","Making the Choice",[],[],{"title":151,"description":152,"pubDate":188},"2015-02-22",[],"2015-05-02-creating-an-alarm-service-using-aws-lambda-and-slack",{"id":190,"data":192,"body":196,"filePath":197,"digest":198,"rendered":199},{"title":193,"description":194,"pubDate":195},"Creating An Alarm Service Using AWS Lambda And Slack","Lambda http://aws.amazon.com/lambda/ by Amazon http://aws.amazon.com/ was a service launched last year and was in preview until a few weeks ago https://aws.a...",["Date","2015-05-02T00:00:00.000Z"],"[Lambda](http://aws.amazon.com/lambda/) by [Amazon](http://aws.amazon.com/) was a service launched last year and was in preview until a [few weeks ago](https://aws.amazon.com/blogs/aws/aws-lambda-update-production-status-and-a-focus-on-mobile-apps/).  Now it has general availability and arrived with support for [SNS](http://aws.amazon.com/sns/).  This opens up a lot of options for connecting AWS services to your workflow, especially for when things break.\n\n### The Idea\nConsume SNS messages from Cloudwatch Alarms into a Lambda function, parsing it and posting the data we want to Slack to notify us when something is going wrong.\n\u003Cimg src=\"/images/alert.png\" class=\"img-responsive\" alt=\"Alert\">\n\n### Lambda\nAccording to Amazon,\n> AWS Lambda is a compute service that runs your code in response to events and automatically manages the compute resources for you, making it easy to build applications that respond quickly to new information.\n\nIf you are already using AWS for things like [EC2](http://aws.amazon.com/ec2/) and [Cloudwatch](http://aws.amazon.com/cloudwatch), you can achieve things that normally would have been handled by another service cheaply and easily using Lambda.  \n\n**What Can Lambda Do?**\n\nLambda has native support for node.js and Java, but I don't know either of those and so had to find an alternative.  Although it isn't totally clear, you can essentially shell out to the file system underneath a function, which shows you have other options such as Python.  Things look good, but our team are Rubyists, so I thought about packaging Ruby and running a function.  Lambda is capped at 30MB, but I still got Ruby running in a Lambda function, with 800kb to spare....\n\nThings like Ruby gems weren't usable and it seemed clunky to use Ruby in this way, but it made me think that even node.js isn't the best solution.\n\n[Golang](https://golang.org/) is something I've been using in my personal projects and it allows for a single binary to be created, meaning you just need to shell out and call the binary in your function.  I think this is a nice solution for single process jobs such as Lambda, so that's what I ended up going with.\n\n### SNS & Lambda\nShortly after announcing SNS support for Lambda I attended the AWS Summit in London, I came away with an idea for helping us know when something was failing,\n\n**Cloudwatch > SNS > Lambda > Slack**\n\n[Slack](https://slack.com/) comes with great support for webhooks and advanced formatting, not to mention desktop and mobile apps as well as emailing you when you get a mention.  This would give us a cheap, simple way of getting alerts, so I started putting it together.\n\n### Messages\nCloudwatch alarms are a good example of event driven compute, you need to know when your server is melting.  How you get that information is actually a big deal, with many companies offering fully featured dashboards and incident management when something goes wrong.  If you decide you want to run your own service, simply to pick up alarms and send a notification to your phone, you might think Email and SNS.  SNS has the ability to send Email as well as Push notifications to apps and services, so why isn't this enough?  \n\nEmail isn't always the best option; I can't get my work Email on my phone without using the web browser.  There are also companies who don't allow for Email outside of the office.  There is a question of availability too; there are situations where our work email servers are turned off to stop fishing attacks, so relying on one information source is a mistake.  \n\n### Third Party Services\nIf there is one service that knows this, it's [PagerDuty](www.pagerduty.com), offering app push messages, phone calls, text and Email alerts.  Recently, my team launched a new website, [http://www.bbc.co.uk/newsbeat](http://www.bbc.co.uk/newsbeat) on AWS, so we are taking turns to go on-call.  Coming from a company where teams going on-call is relatively new, we still have a lot of people and procedures in place to make it a lot easier for the dev team.  When something goes wrong, our operations team try to fix the issue based on our runbook, if they can't resolve the issue, we get called.\n\nWith this in mind, services like PagerDuty were deemed unnecessary by the higher ups, so we're back to Emails and in need of something better.\n\n### Sonitus\nThe code for the Lambda function is open source and available on [Github](https://github.com/BBC-News/sonitus).  The code is split into a JavaScript file that calls the Go binary and the Go file, which posts to Slack.  There is also a debug folder with example JSON for those who want to try it out before integrating.  \n\nBuild the binary as per the readme; zip the binary and the index.js file together, and upload it as a new Lambda function.  Once you have set up your Cloudwatch alarms, create an SNS topic and create a new subscription to point to your new Lambda function, that's it.\n\nWhat you get in Slack is something that looks like this :\n\n\u003Ca href=\"/images/slack.png\" alt=\"Grafana\">Link to image\u003C/a>\n\n\u003Cimg src=\"/images/slack.png\" class=\"img-responsive\" alt=\"Slack\">\n\nYou can customise the message based on your alarm structure, but the default layout is pretty simple, the alarm state, the alarm name, the description, a link to that alarm in the AWS console and the time of the alarm going off or being resolved.  You will also see a colour based on the alarm state.\n\nThe only drawback I found was when alarms go to insufficient data, i.e. your application might be doing fine, but you have nothing coming in.  Cloudwatch will send a message indicating it has insufficient data and the alarm goes into this state.  If you have an alarm on something that maybe only metrics an event every other minute, you will get a lot of output in Slack.  For that reason, I have ignored insufficient data messages from Cloudwatch, you will only ever see messages that are in an OK or Alarm state.\n\n### Many More Possibilities\nThis use case it quite small, Lambda functions can do so much more, replacing EC2 in some cases.  What I like about Lambda is that it's event driven, only being called into service when needed, such as when an Alarm goes off.  This type of setup would have previously cost a lot more and would have probably involved a lot more setup, proving just how time saving Lambda can be.  \n\nThe next step would be to tie into other services like [goroost](https://goroost.com), which offer web push notifications, or other services that instant messaging, phone calls etc.\n\nIt will be interesting to see how the Lambda service improves over time, from my team though, it is something we will use all day, every day, but only when we need it.","src/content/blog/2015-05-02-creating-an-alarm-service-using-aws-lambda-and-slack.md","e377078c2218a27c",{"html":200,"metadata":201},"\u003Cp>\u003Ca href=\"http://aws.amazon.com/lambda/\">Lambda\u003C/a> by \u003Ca href=\"http://aws.amazon.com/\">Amazon\u003C/a> was a service launched last year and was in preview until a \u003Ca href=\"https://aws.amazon.com/blogs/aws/aws-lambda-update-production-status-and-a-focus-on-mobile-apps/\">few weeks ago\u003C/a>.  Now it has general availability and arrived with support for \u003Ca href=\"http://aws.amazon.com/sns/\">SNS\u003C/a>.  This opens up a lot of options for connecting AWS services to your workflow, especially for when things break.\u003C/p>\n\u003Ch3 id=\"the-idea\">The Idea\u003C/h3>\n\u003Cp>Consume SNS messages from Cloudwatch Alarms into a Lambda function, parsing it and posting the data we want to Slack to notify us when something is going wrong.\n\u003Cimg src=\"/images/alert.png\" class=\"img-responsive\" alt=\"Alert\">\u003C/p>\n\u003Ch3 id=\"lambda\">Lambda\u003C/h3>\n\u003Cp>According to Amazon,\u003C/p>\n\u003Cblockquote>\n\u003Cp>AWS Lambda is a compute service that runs your code in response to events and automatically manages the compute resources for you, making it easy to build applications that respond quickly to new information.\u003C/p>\n\u003C/blockquote>\n\u003Cp>If you are already using AWS for things like \u003Ca href=\"http://aws.amazon.com/ec2/\">EC2\u003C/a> and \u003Ca href=\"http://aws.amazon.com/cloudwatch\">Cloudwatch\u003C/a>, you can achieve things that normally would have been handled by another service cheaply and easily using Lambda.\u003C/p>\n\u003Cp>\u003Cstrong>What Can Lambda Do?\u003C/strong>\u003C/p>\n\u003Cp>Lambda has native support for node.js and Java, but I don’t know either of those and so had to find an alternative.  Although it isn’t totally clear, you can essentially shell out to the file system underneath a function, which shows you have other options such as Python.  Things look good, but our team are Rubyists, so I thought about packaging Ruby and running a function.  Lambda is capped at 30MB, but I still got Ruby running in a Lambda function, with 800kb to spare…\u003C/p>\n\u003Cp>Things like Ruby gems weren’t usable and it seemed clunky to use Ruby in this way, but it made me think that even node.js isn’t the best solution.\u003C/p>\n\u003Cp>\u003Ca href=\"https://golang.org/\">Golang\u003C/a> is something I’ve been using in my personal projects and it allows for a single binary to be created, meaning you just need to shell out and call the binary in your function.  I think this is a nice solution for single process jobs such as Lambda, so that’s what I ended up going with.\u003C/p>\n\u003Ch3 id=\"sns--lambda\">SNS &#x26; Lambda\u003C/h3>\n\u003Cp>Shortly after announcing SNS support for Lambda I attended the AWS Summit in London, I came away with an idea for helping us know when something was failing,\u003C/p>\n\u003Cp>\u003Cstrong>Cloudwatch > SNS > Lambda > Slack\u003C/strong>\u003C/p>\n\u003Cp>\u003Ca href=\"https://slack.com/\">Slack\u003C/a> comes with great support for webhooks and advanced formatting, not to mention desktop and mobile apps as well as emailing you when you get a mention.  This would give us a cheap, simple way of getting alerts, so I started putting it together.\u003C/p>\n\u003Ch3 id=\"messages\">Messages\u003C/h3>\n\u003Cp>Cloudwatch alarms are a good example of event driven compute, you need to know when your server is melting.  How you get that information is actually a big deal, with many companies offering fully featured dashboards and incident management when something goes wrong.  If you decide you want to run your own service, simply to pick up alarms and send a notification to your phone, you might think Email and SNS.  SNS has the ability to send Email as well as Push notifications to apps and services, so why isn’t this enough?\u003C/p>\n\u003Cp>Email isn’t always the best option; I can’t get my work Email on my phone without using the web browser.  There are also companies who don’t allow for Email outside of the office.  There is a question of availability too; there are situations where our work email servers are turned off to stop fishing attacks, so relying on one information source is a mistake.\u003C/p>\n\u003Ch3 id=\"third-party-services\">Third Party Services\u003C/h3>\n\u003Cp>If there is one service that knows this, it’s \u003Ca href=\"www.pagerduty.com\">PagerDuty\u003C/a>, offering app push messages, phone calls, text and Email alerts.  Recently, my team launched a new website, \u003Ca href=\"http://www.bbc.co.uk/newsbeat\">http://www.bbc.co.uk/newsbeat\u003C/a> on AWS, so we are taking turns to go on-call.  Coming from a company where teams going on-call is relatively new, we still have a lot of people and procedures in place to make it a lot easier for the dev team.  When something goes wrong, our operations team try to fix the issue based on our runbook, if they can’t resolve the issue, we get called.\u003C/p>\n\u003Cp>With this in mind, services like PagerDuty were deemed unnecessary by the higher ups, so we’re back to Emails and in need of something better.\u003C/p>\n\u003Ch3 id=\"sonitus\">Sonitus\u003C/h3>\n\u003Cp>The code for the Lambda function is open source and available on \u003Ca href=\"https://github.com/BBC-News/sonitus\">Github\u003C/a>.  The code is split into a JavaScript file that calls the Go binary and the Go file, which posts to Slack.  There is also a debug folder with example JSON for those who want to try it out before integrating.\u003C/p>\n\u003Cp>Build the binary as per the readme; zip the binary and the index.js file together, and upload it as a new Lambda function.  Once you have set up your Cloudwatch alarms, create an SNS topic and create a new subscription to point to your new Lambda function, that’s it.\u003C/p>\n\u003Cp>What you get in Slack is something that looks like this :\u003C/p>\n\u003Cp>\u003Ca href=\"/images/slack.png\" alt=\"Grafana\">Link to image\u003C/a>\u003C/p>\n\u003Cimg src=\"/images/slack.png\" class=\"img-responsive\" alt=\"Slack\">\n\u003Cp>You can customise the message based on your alarm structure, but the default layout is pretty simple, the alarm state, the alarm name, the description, a link to that alarm in the AWS console and the time of the alarm going off or being resolved.  You will also see a colour based on the alarm state.\u003C/p>\n\u003Cp>The only drawback I found was when alarms go to insufficient data, i.e. your application might be doing fine, but you have nothing coming in.  Cloudwatch will send a message indicating it has insufficient data and the alarm goes into this state.  If you have an alarm on something that maybe only metrics an event every other minute, you will get a lot of output in Slack.  For that reason, I have ignored insufficient data messages from Cloudwatch, you will only ever see messages that are in an OK or Alarm state.\u003C/p>\n\u003Ch3 id=\"many-more-possibilities\">Many More Possibilities\u003C/h3>\n\u003Cp>This use case it quite small, Lambda functions can do so much more, replacing EC2 in some cases.  What I like about Lambda is that it’s event driven, only being called into service when needed, such as when an Alarm goes off.  This type of setup would have previously cost a lot more and would have probably involved a lot more setup, proving just how time saving Lambda can be.\u003C/p>\n\u003Cp>The next step would be to tie into other services like \u003Ca href=\"https://goroost.com\">goroost\u003C/a>, which offer web push notifications, or other services that instant messaging, phone calls etc.\u003C/p>\n\u003Cp>It will be interesting to see how the Lambda service improves over time, from my team though, it is something we will use all day, every day, but only when we need it.\u003C/p>",{"headings":202,"localImagePaths":224,"remoteImagePaths":225,"frontmatter":226,"imagePaths":228},[203,206,209,212,215,218,221],{"depth":119,"slug":204,"text":205},"the-idea","The Idea",{"depth":119,"slug":207,"text":208},"lambda","Lambda",{"depth":119,"slug":210,"text":211},"sns--lambda","SNS & Lambda",{"depth":119,"slug":213,"text":214},"messages","Messages",{"depth":119,"slug":216,"text":217},"third-party-services","Third Party Services",{"depth":119,"slug":219,"text":220},"sonitus","Sonitus",{"depth":119,"slug":222,"text":223},"many-more-possibilities","Many More Possibilities",[],[],{"title":193,"description":194,"pubDate":227},"2015-05-02",[],"2014-05-27-proxy-auth-in-phantomjs",{"id":229,"data":231,"body":235,"filePath":236,"digest":237,"rendered":238},{"title":232,"description":233,"pubDate":234},"Proxy Auth In Phantomjs","I recently setup a Nginx proxy server on a UK based provider, this is to test UK centric features of the BBC News website. We use AWS for most of our tooling...",["Date","2014-05-27T00:00:00.000Z"],"I recently setup a Nginx proxy server on a UK based provider, this is to test UK centric features of the BBC News website.  We use AWS for most of our tooling, so adding a couple of extra options on AWS to enable this testing seemed like the best approach.  Not quite.\n\n## PhantomJS\nThe tool I was using to test was [Wraith](https://github.com/BBC-News/wraith), a visual regression testing tool.  We allow users to set command line arguments and this is passed to [PhantomJS](http://phantomjs.org/) on the command line.  One of the available options for PhantomJS is a proxy server and proxy auth, but it isn't that simple.  In testing the --proxy options on it's own, it works fine, but adding auth seems to be a bit more tricky.\nNginx supports basic auth, seeing as that is just a username and password, you'd expect the command to be something like this :\n\n```sh\nphantomjs --proxy=192.168.1.1:8080 --proxy-auth=username:password snap.js http://www.bbc.co.uk/news 320 test.png\n```\n\nThis simply doesn't work, even though using curl and a desktop browser confirmed the proxy to be working.  The next step was to see if [Slimerjs](slimerjs.org) worked, it didn't.  Assuming all Phantomjs does is take the command line argument and pass that as a header, it seemed possible to use my Javascript file to send the credentials.\n\n## In the JS file\nBy setting the following, Slimerjs authenticated and took the screenshot.  The same could not be said of Phantomjs, it still didn't work.\n\n```sh\npage.settings.userName = \"user\"\npage.settings.password = \"password\"  \n```\n\n## The Solution\nThe solution was to set a custom header, this was successful in enabling access to the proxy.  It seems that for some use cases, the proxy auth works from the command line, but for my situation, the proxy must be set by the command line and the credentials via the javascript file.  \n\n```sh\npage.customHeaders={'Authorization': 'Basic '+btoa('user:password')};\n```","src/content/blog/2014-05-27-proxy-auth-in-phantomjs.md","6f67e109b6eb0572",{"html":239,"metadata":240},"\u003Cp>I recently setup a Nginx proxy server on a UK based provider, this is to test UK centric features of the BBC News website.  We use AWS for most of our tooling, so adding a couple of extra options on AWS to enable this testing seemed like the best approach.  Not quite.\u003C/p>\n\u003Ch2 id=\"phantomjs\">PhantomJS\u003C/h2>\n\u003Cp>The tool I was using to test was \u003Ca href=\"https://github.com/BBC-News/wraith\">Wraith\u003C/a>, a visual regression testing tool.  We allow users to set command line arguments and this is passed to \u003Ca href=\"http://phantomjs.org/\">PhantomJS\u003C/a> on the command line.  One of the available options for PhantomJS is a proxy server and proxy auth, but it isn’t that simple.  In testing the —proxy options on it’s own, it works fine, but adding auth seems to be a bit more tricky.\nNginx supports basic auth, seeing as that is just a username and password, you’d expect the command to be something like this :\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">phantomjs\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --proxy=192.168.1.1:8080\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --proxy-auth=username:password\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> snap.js\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> http://www.bbc.co.uk/news\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 320\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> test.png\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This simply doesn’t work, even though using curl and a desktop browser confirmed the proxy to be working.  The next step was to see if \u003Ca href=\"slimerjs.org\">Slimerjs\u003C/a> worked, it didn’t.  Assuming all Phantomjs does is take the command line argument and pass that as a header, it seemed possible to use my Javascript file to send the credentials.\u003C/p>\n\u003Ch2 id=\"in-the-js-file\">In the JS file\u003C/h2>\n\u003Cp>By setting the following, Slimerjs authenticated and took the screenshot.  The same could not be said of Phantomjs, it still didn’t work.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">page.settings.userName\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"user\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">page.settings.password\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"password\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">  \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"the-solution\">The Solution\u003C/h2>\n\u003Cp>The solution was to set a custom header, this was successful in enabling access to the proxy.  It seems that for some use cases, the proxy auth works from the command line, but for my situation, the proxy must be set by the command line and the credentials via the javascript file.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">page.customHeaders\u003C/span>\u003Cspan style=\"color:#9ECBFF\">={\u003C/span>\u003Cspan style=\"color:#E1E4E8\">'\u003C/span>\u003Cspan style=\"color:#B392F0\">Authorization\u003C/span>\u003Cspan style=\"color:#B392F0\">': '\u003C/span>\u003Cspan style=\"color:#B392F0\">Basic\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> '+btoa('user:password')};\u003C/span>\u003C/span>\u003C/code>\u003C/pre>",{"headings":241,"localImagePaths":251,"remoteImagePaths":252,"frontmatter":253,"imagePaths":255},[242,245,248],{"depth":42,"slug":243,"text":244},"phantomjs","PhantomJS",{"depth":42,"slug":246,"text":247},"in-the-js-file","In the JS file",{"depth":42,"slug":249,"text":250},"the-solution","The Solution",[],[],{"title":232,"description":233,"pubDate":254},"2014-05-27",[],"2016-11-10-go-microservices-without-docker-say-whaaaaaat",{"id":256,"data":258,"body":262,"filePath":263,"digest":264,"rendered":265},{"title":259,"description":260,"pubDate":261},"Go Microservices Without Docker....say Whaaaaaat","Docker is pretty cool, lots of tools, big community and it goes quite nicely with Golang Go microservices. My company, FundApps, is a Fintec startup, we writ...",["Date","2016-11-10T00:00:00.000Z"],"Docker is pretty cool, lots of tools, big community and it goes quite nicely with Golang(Go) microservices.  My company, FundApps, is a Fintec startup, we write Go, we write microservices, so why wouldn't we pick Docker?\n\n### Why not Docker\n\nSome recent blog posts have pointed to the technology reasons for not using Docker, but there are also cultural reasons.  Working in a startup, technology choices have wide ranging implications.\n\nIf you are the Docker guy in your company, you are the one who drives the change, but what if you leave.  That may be the case for other technology choices, but Docker is not a single piece in your infrastructure, it is your infrastructure.  In a small team of a dozen engineers, a switch to Docker is something that you really have to embrace as a team, it can't just be an Ops thing.\n\nSome engineers will spend hours tweaking a local dev setup, hundreds for vim folks.  So what happens if you, the Docker pioneer, suggest everyone changes a massive part of the way they work and smash Docker into the mix.  Then, changing the production stack to match is also a big commitment.  In a company where there are a couple of you running the entire infrastructure, it can be a difficult decision to introduce lots of new tooling and make it production ready without lots lead time.\n\nFor FundApps, there is also a second key issue, we are primarily a dotnet shop.  You know the story by now, big monolith, trying to break it up with Go microservices etc etc.  Working in a dotnet shop often means massive amounts of manual processes, over complicated tooling as well as limited supported for software that runs on Linux/macOS.  Before Docker for Windows/Mac apps came a long, the process of running Docker on Windows was not a smooth process for all.  During a demo of Docker I ran for the team, my Windows machine locked up as I started a container, it wasn't a great insight into the stability of Docker on Windows.  Even now, the experience isn't as smooth as say, macOS.\n\nSo we have a \n\n\n\nTelling everyone in the company that you will be migrating in such a large way to a new stack is not a flippant decision, it something that everyone must be on board with.  Microservices and Docker together in production is not a Ops decision or dev decision, its a organisational decision.","src/content/blog/2016-11-10-go-microservices-without-docker-say-whaaaaaat.md","48f3031c9d60ca5a",{"html":266,"metadata":267},"\u003Cp>Docker is pretty cool, lots of tools, big community and it goes quite nicely with Golang(Go) microservices.  My company, FundApps, is a Fintec startup, we write Go, we write microservices, so why wouldn’t we pick Docker?\u003C/p>\n\u003Ch3 id=\"why-not-docker\">Why not Docker\u003C/h3>\n\u003Cp>Some recent blog posts have pointed to the technology reasons for not using Docker, but there are also cultural reasons.  Working in a startup, technology choices have wide ranging implications.\u003C/p>\n\u003Cp>If you are the Docker guy in your company, you are the one who drives the change, but what if you leave.  That may be the case for other technology choices, but Docker is not a single piece in your infrastructure, it is your infrastructure.  In a small team of a dozen engineers, a switch to Docker is something that you really have to embrace as a team, it can’t just be an Ops thing.\u003C/p>\n\u003Cp>Some engineers will spend hours tweaking a local dev setup, hundreds for vim folks.  So what happens if you, the Docker pioneer, suggest everyone changes a massive part of the way they work and smash Docker into the mix.  Then, changing the production stack to match is also a big commitment.  In a company where there are a couple of you running the entire infrastructure, it can be a difficult decision to introduce lots of new tooling and make it production ready without lots lead time.\u003C/p>\n\u003Cp>For FundApps, there is also a second key issue, we are primarily a dotnet shop.  You know the story by now, big monolith, trying to break it up with Go microservices etc etc.  Working in a dotnet shop often means massive amounts of manual processes, over complicated tooling as well as limited supported for software that runs on Linux/macOS.  Before Docker for Windows/Mac apps came a long, the process of running Docker on Windows was not a smooth process for all.  During a demo of Docker I ran for the team, my Windows machine locked up as I started a container, it wasn’t a great insight into the stability of Docker on Windows.  Even now, the experience isn’t as smooth as say, macOS.\u003C/p>\n\u003Cp>So we have a\u003C/p>\n\u003Cp>Telling everyone in the company that you will be migrating in such a large way to a new stack is not a flippant decision, it something that everyone must be on board with.  Microservices and Docker together in production is not a Ops decision or dev decision, its a organisational decision.\u003C/p>",{"headings":268,"localImagePaths":272,"remoteImagePaths":273,"frontmatter":274,"imagePaths":276},[269],{"depth":119,"slug":270,"text":271},"why-not-docker","Why not Docker",[],[],{"title":259,"description":260,"pubDate":275},"2016-11-10",[],"2016-12-27-what-am-i-copy-and-pasting-into-my-dockerfile",{"id":277,"data":279,"body":283,"filePath":284,"digest":285,"rendered":286},{"title":280,"description":281,"pubDate":282},"What Am I Copy And Pasting Into My Dockerfile","It's pretty common that when you have no idea what you are doing, you want things to \"just work\". Docker, and by extension, Linux, have a world of documentat...",["Date","2016-12-27T00:00:00.000Z"],"It's pretty common that when you have no idea what you are doing, you want things to \"just work\".  Docker, and by extension, Linux, have a world of documentation, but often it's the little things that trip people up.  When installing software into your Docker image, something may not work, a C library, some kind of XML error, something with a .so file.  You go onto a forum and see loads of commands and packages not in your Dockerfile, add them in and then it works, but you have no idea what you did.   Let's look at what you might be copying.\n\n### From the top\n\nWhat is that Docker image you're using, Ubuntu, right? You may see `FROM buildpack-deps:jessie-scm`, or `buildpack-deps:jessie-curl`, which both are based on the Debian distribution of Linux.  A couple of years ago, there was a lot of people who felt it was best to start with a clean image, no unneeded packages.\n\nHowever, it is likely you are going to want curl, wget or maybe a programming language installed, so there are some helpful images that bootstrap most commonly used packages.  `buildpack-deps:jessie` is used by Ruby for example, `buildpack-deps:jessie-scm` is used by the Go image.  \n\nRather than write a long Dockerfile with lots of packages, or use an image with packages you might not need, choosing a buildpack can offer a faster way to run your code with just the right amount of packages to worry about.\n\n### Random ENV's\n\nSo what is `export DEBIAN_FRONTEND=noninteractive` I have been seeing.  Well, it's a way of preventing interactive installers from blocking installation of software.  This is usually followed by more environment variables that set the answers to the questions you've just blocked from appearing.\n\n### Game SET and -x?\n\nBash, that thing that shits all over powershell, has many ways to ensure your Docker build is a success.  Often, when you have a long list of commands chained together, you want to exit the Docker build when something fails so that you don't waste time, this is achieved through `set`.\n\nI have seen `set -eux`, is this the norm?  \n\n```\nset -e\n```\n\nThe most common way to exit a chain, or pipeline, such as `install foo && install bar`, is to use e, which means exit immediately.\n\nBy doing this, you will get much faster feedback from broken builds.\n\n```\nset -u\n```\n\nThe `-u` option is nice if you want to exit when a variable hasn't been set.  This is useful for those who make a lot of typos.  In a shell script, if you use `-u`, an `echo $FOOBAR` will actually exit 1 if it isn't set.  The alternative is that your echo statement prints a blank line and exits 0.\n\n```\nset -x\n```\n\nThis is more of a debugging tip, so is something of an optional flag.  `-x` will output a trace of your previous command, so for example,\n\n```bash\n#!/bin/bash\n\nset -ux\necho \"testing\"\necho $DAVE\n```\nLets run it\n\n```bash\n~> ./demo.sh  \n+ echo testing  \ntesting  \n./demo.sh: line 5: DAVE: unbound variable  \n```\n\nThis will error, but at least we know that our first step was a success.  I use this in most of my shell scripts.\n\n### yum yum yum....APT!\n\nSo now we are at the main bit, the part where you may have about 40 &&'s chaining your commands together.  But what is `--no-install-recommends` doing?  Should I know what epel-release is?\n\nIf you are familiar with Debian or Centos, you will probably not know the intricacies of the package managers, apt and yum respectively.  They offer a lot of a packages, but there are some key things to note.\n\n```\nyum -y install epel-release\n```\n**yum** - The package manager CLI  \n**-y** - Accept all licenses and prompts  \n**install** - Install software  \n**epel-release** - Extra Packages for Enterprise Linux, allows installation of lots of open source packages  \n\n\n_Now APT_\n\n```\napt-get update && apt-get install -y --no-install-recommends\n```\n\n**apt-get** - CLI tool for installing packages  \n**update** - Retrieves new lists of packages  \n**&&** - Chain commands if output from first is a success  \n**install** - Install software  \n**-y** - Accept all licenses and prompts  \n\nSo `--no-install-recommends` is an interesting one.  APT can suggest packages and recommend packages, an example would be you want to install a package to write a file a format, so would be recommended a tool to read that format.  If you don't want this software, which in a Docker image, you probably don't, add this to your install command.\n\n### Cleaning Up\n\nThis is usually the following two commands for Debian and Centos based distributions\n\n```sh\napt-get clean\n\nyum clean all\n```\nThis just removes downloaded archives and caches, useful if you are about to push your image up and every byte counts.\n\nAlso, `rm -rf /var/lib/apt/lists/` on Debian can be used to clean apt package lists.  When you apt-get update, you download a long list of packages, you don't need this if you are finished installing software for good.  Next time you run apt-get update, you can get a new list.\n\n\n### The more you know\n\nSome of this is just the basics of Linux, but for those developers who have never used Linux before, it can be a big task to learn this, as well as deploying your code inside a container.  Hopefully, as you go forth into the Docker ecosystem, you will now have a little more information to help install just the software you need for your code to run.","src/content/blog/2016-12-27-what-am-i-copy-and-pasting-into-my-dockerfile.md","4fd9e638edbb20ac",{"html":287,"metadata":288},"\u003Cp>It’s pretty common that when you have no idea what you are doing, you want things to “just work”.  Docker, and by extension, Linux, have a world of documentation, but often it’s the little things that trip people up.  When installing software into your Docker image, something may not work, a C library, some kind of XML error, something with a .so file.  You go onto a forum and see loads of commands and packages not in your Dockerfile, add them in and then it works, but you have no idea what you did.   Let’s look at what you might be copying.\u003C/p>\n\u003Ch3 id=\"from-the-top\">From the top\u003C/h3>\n\u003Cp>What is that Docker image you’re using, Ubuntu, right? You may see \u003Ccode>FROM buildpack-deps:jessie-scm\u003C/code>, or \u003Ccode>buildpack-deps:jessie-curl\u003C/code>, which both are based on the Debian distribution of Linux.  A couple of years ago, there was a lot of people who felt it was best to start with a clean image, no unneeded packages.\u003C/p>\n\u003Cp>However, it is likely you are going to want curl, wget or maybe a programming language installed, so there are some helpful images that bootstrap most commonly used packages.  \u003Ccode>buildpack-deps:jessie\u003C/code> is used by Ruby for example, \u003Ccode>buildpack-deps:jessie-scm\u003C/code> is used by the Go image.\u003C/p>\n\u003Cp>Rather than write a long Dockerfile with lots of packages, or use an image with packages you might not need, choosing a buildpack can offer a faster way to run your code with just the right amount of packages to worry about.\u003C/p>\n\u003Ch3 id=\"random-envs\">Random ENV’s\u003C/h3>\n\u003Cp>So what is \u003Ccode>export DEBIAN_FRONTEND=noninteractive\u003C/code> I have been seeing.  Well, it’s a way of preventing interactive installers from blocking installation of software.  This is usually followed by more environment variables that set the answers to the questions you’ve just blocked from appearing.\u003C/p>\n\u003Ch3 id=\"game-set-and--x\">Game SET and -x?\u003C/h3>\n\u003Cp>Bash, that thing that shits all over powershell, has many ways to ensure your Docker build is a success.  Often, when you have a long list of commands chained together, you want to exit the Docker build when something fails so that you don’t waste time, this is achieved through \u003Ccode>set\u003C/code>.\u003C/p>\n\u003Cp>I have seen \u003Ccode>set -eux\u003C/code>, is this the norm?\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>set -e\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The most common way to exit a chain, or pipeline, such as \u003Ccode>install foo &#x26;&#x26; install bar\u003C/code>, is to use e, which means exit immediately.\u003C/p>\n\u003Cp>By doing this, you will get much faster feedback from broken builds.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>set -u\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The \u003Ccode>-u\u003C/code> option is nice if you want to exit when a variable hasn’t been set.  This is useful for those who make a lot of typos.  In a shell script, if you use \u003Ccode>-u\u003C/code>, an \u003Ccode>echo $FOOBAR\u003C/code> will actually exit 1 if it isn’t set.  The alternative is that your echo statement prints a blank line and exits 0.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>set -x\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This is more of a debugging tip, so is something of an optional flag.  \u003Ccode>-x\u003C/code> will output a trace of your previous command, so for example,\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">#!/bin/bash\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">set\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -ux\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">echo\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"testing\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">echo\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $DAVE\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Lets run it\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">~>\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> ./demo.sh  \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">+\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> echo\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> testing\u003C/span>\u003Cspan style=\"color:#E1E4E8\">  \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">testing\u003C/span>\u003Cspan style=\"color:#E1E4E8\">  \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">./demo.sh:\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> line\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 5:\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> DAVE:\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> unbound\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> variable\u003C/span>\u003Cspan style=\"color:#E1E4E8\">  \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This will error, but at least we know that our first step was a success.  I use this in most of my shell scripts.\u003C/p>\n\u003Ch3 id=\"yum-yum-yumapt\">yum yum yum…APT!\u003C/h3>\n\u003Cp>So now we are at the main bit, the part where you may have about 40 &#x26;&#x26;‘s chaining your commands together.  But what is \u003Ccode>--no-install-recommends\u003C/code> doing?  Should I know what epel-release is?\u003C/p>\n\u003Cp>If you are familiar with Debian or Centos, you will probably not know the intricacies of the package managers, apt and yum respectively.  They offer a lot of a packages, but there are some key things to note.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>yum -y install epel-release\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>\u003Cstrong>yum\u003C/strong> - The package manager CLI\u003Cbr>\n\u003Cstrong>-y\u003C/strong> - Accept all licenses and prompts\u003Cbr>\n\u003Cstrong>install\u003C/strong> - Install software\u003Cbr>\n\u003Cstrong>epel-release\u003C/strong> - Extra Packages for Enterprise Linux, allows installation of lots of open source packages\u003C/p>\n\u003Cp>\u003Cem>Now APT\u003C/em>\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>apt-get update &#x26;&#x26; apt-get install -y --no-install-recommends\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>\u003Cstrong>apt-get\u003C/strong> - CLI tool for installing packages\u003Cbr>\n\u003Cstrong>update\u003C/strong> - Retrieves new lists of packages\u003Cbr>\n\u003Cstrong>&#x26;&#x26;\u003C/strong> - Chain commands if output from first is a success\u003Cbr>\n\u003Cstrong>install\u003C/strong> - Install software\u003Cbr>\n\u003Cstrong>-y\u003C/strong> - Accept all licenses and prompts\u003C/p>\n\u003Cp>So \u003Ccode>--no-install-recommends\u003C/code> is an interesting one.  APT can suggest packages and recommend packages, an example would be you want to install a package to write a file a format, so would be recommended a tool to read that format.  If you don’t want this software, which in a Docker image, you probably don’t, add this to your install command.\u003C/p>\n\u003Ch3 id=\"cleaning-up\">Cleaning Up\u003C/h3>\n\u003Cp>This is usually the following two commands for Debian and Centos based distributions\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">apt-get\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> clean\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">yum\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> clean\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> all\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This just removes downloaded archives and caches, useful if you are about to push your image up and every byte counts.\u003C/p>\n\u003Cp>Also, \u003Ccode>rm -rf /var/lib/apt/lists/\u003C/code> on Debian can be used to clean apt package lists.  When you apt-get update, you download a long list of packages, you don’t need this if you are finished installing software for good.  Next time you run apt-get update, you can get a new list.\u003C/p>\n\u003Ch3 id=\"the-more-you-know\">The more you know\u003C/h3>\n\u003Cp>Some of this is just the basics of Linux, but for those developers who have never used Linux before, it can be a big task to learn this, as well as deploying your code inside a container.  Hopefully, as you go forth into the Docker ecosystem, you will now have a little more information to help install just the software you need for your code to run.\u003C/p>",{"headings":289,"localImagePaths":308,"remoteImagePaths":309,"frontmatter":310,"imagePaths":312},[290,293,296,299,302,305],{"depth":119,"slug":291,"text":292},"from-the-top","From the top",{"depth":119,"slug":294,"text":295},"random-envs","Random ENV’s",{"depth":119,"slug":297,"text":298},"game-set-and--x","Game SET and -x?",{"depth":119,"slug":300,"text":301},"yum-yum-yumapt","yum yum yum…APT!",{"depth":119,"slug":303,"text":304},"cleaning-up","Cleaning Up",{"depth":119,"slug":306,"text":307},"the-more-you-know","The more you know",[],[],{"title":280,"description":281,"pubDate":311},"2016-12-27",[],"2016-08-28-how-to-elb-with-alb",{"id":313,"data":315,"body":319,"filePath":320,"digest":321,"rendered":322},{"title":316,"description":317,"pubDate":318},"How To ELB With ALB","Amazon recently announced its new load balancer, the Application Load Balancer ALB https://aws.amazon.com/blogs/aws/new aws application load balancer/ . This...",["Date","2016-08-28T00:00:00.000Z"],"Amazon recently announced its new [load balancer, the Application Load Balancer(ALB)](https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/).  This load balancer can replace all elastic load balancers currently in service as well as offering several new features designed for container based architectures.  While normal load balancers offer support for EC2 based architectures, container based architectures often require [HAProxy](http://www.haproxy.org/) or [NGINX](https://www.nginx.com/) to function effectively.  This is in part due to how applications have been broken up for containers, often using micro services.  \n\nAn example might be one application running on port `8080` which has a single route of `/foo` and a second application running on port `9292` with routes of `/bar` and `/baz`.  \n\nIf using a single monolithic stack, this wouldn't be an issue as routing is achieved on a single port and managed in one place.  In order to achieve this kind of routing for multiple applications, another service is needed that manages the path matching for you, either on the EC2 instance itself, or externally on separate EC2 instances.  Running Apache on an EC2 instance that is setup to correctly route is quite easy, but does require updating the whole OS image or editing the config on the instance.  If running a maximum of say 3 applications, this wouldn't be so difficult to maintain, but more than that across a large fleet of instances can be slow to update.\n\n### Cool Features\nWhile many other load balancers have had these features for a while, ALB supports HTTP/2 and WebSockets out of the box.  Sticky sessions are also possible, even on WebSockets, which is a great feature for those who haven't taken the WebSocket plunge.  The ALB also comes with a few new metrics to help you target application performance.  Each target group has its own group of metrics, so tracking down which application is performing best/worst should be easier.  Four blocks of HTTP status codes now exist, 2xx, 3xx, 4xx and 5xx.  These are available on both the ALB and the target groups.\n\n### Consul/etcd\nOther solutions include running a reverse proxy with a service discovery service such as [Consul](https://www.consul.io/)/[etcd](https://coreos.com/etcd/) using HAProxy as the load balancer instead of an ELB.  This would allow for simple routing to the correct servers or containers without having to setup new load balancers.  While this has its advantages, it is more to maintain and can be difficult to set up.  For automatically adding new containers, you would need to run [consul-template](https://github.com/hashicorp/consul-template) in order to dynamically update the HAProxy config.\n\nA common solution is to use sub domains instead of complex routing setups.  This isn't ideal for applications that have very little responsibility however.  There is also URL consistency when building API's.  For example, an API that can show orders, both by ID and the most recent 10 orders.  The most recent 10 might be `example.com/api/orders/` with by ID being `example.com/api/order/:id` with each being a different application.  This could be because the recent orders endpoint is using data pushed into a cache while by ID is coming from a database.  \n\nAnother example would be to make a single orders application which handles both the routes above, then another application that handles customers with similar api structure, `example.com/api/customer/:id`.\n\nWhat is clear is that an ELB, or a Classic ELB as Amazon is now calling it, can be difficult to utilise with many separate applications.  Where you may have a single monolith, which contains everything, that is attached to a single ELB, routing is not so much an issue.\n\nAWS have often advised using ELB's to form part of service discovery because you can attach them to containers and then have a single endpoint for which to reach your application.  This isn't cheap though, if you run 20 services, you will pay $400 a month and that doesn't include data charges.  Even with ELB's in place, to correctly route your traffic, you will still need to have a service in front of all of them to route to the right load balancer.\n\nThis is where the ALB really makes sense.\n\n## The Application Load Balancer\n\nThe name given to this new load balancer varies depending on what page of AWS documentation you read.  ALB is the most common name used so far, but AWS have changed the name of ELB to the Classic Load Balancer in several places, so does this mean the ALB is the ELB V2?  Yes according to [CloudFormation](https://aws.amazon.com/cloudformation/), which had support on day 1.  Current ELB's are still ELB's in CFN, not V1, but ALB's are ELBV2.  So ELB's are classic load balancers of the V1 variety and ALB's are load balancers of the V2 variety, hope that clears up all the confusion.\n\n### New Terms\n\nTarget Groups and Listener Rules are new, they are the key differentiators to a classic ELB.  An ALB has listener rules, with listener rules having target groups.\n\nA target group, or target, is a port mapping to a container or server that has a health check settings on.  Once setup, the target will look for applications on the port you have selected, then attempt to send traffic once the application is healthy.  A target can be spread across lots of instances, or just one.  Target groups must have unique ports if you are going to run multiple targets on the same server.\n\nListener rules are where the path matching occurs.  By creating an `/api/orders` route, this is then tied to a target, which is managing the health of your application.  The pattern matching is quite powerful, supporting wildcard expressions, for example `/api/orders/*`.\n\nNormal load balancer listeners are still there to take in traffic on, for example, port 80 and 443.  Security groups then allow traffic to pass from these listeners to your EC2 instances or containers.\n\n### Root Route?\n\nIf you are hoping to get the root of every application, you are out of luck. Lets for example say you want to run a app on port `3000` and the app is a third party app that requires it to run at the root, e.g `:3000/`.  This will only work for 1 application, after this, routing must match the pattern as specified.  If you were hoping to run a [grafana](http://grafana.org/) server on `:3000/grafana` for example, you would have to run a reverse proxy on the server to make this work, which defeats the point of using an `ALB`.\n\nThe first listener rule you register will also assume the default route, so ensure you have your applications bound to the correct order and have the correct priority.  More details [here](http://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-update-rules.html)\n\n### CloudFormation on Day 1!!!\n\nAWS shipping CloudFormation support on day is a rare treat.  Usually, something like [Terraform](https://www.terraform.io/) has support with a few days, or even a few hours sometimes.  CloudFormation can often take months to catch up, which makes adoption of new features difficult.  If you decided to use the new features with the console or API, you may end up playing the \"What are we running in production game?\".  Hopefully AWS keeps CloudFormation up to date with ALB updates.\n\nAs this is such a great day in AWS land, I have created an [example stack here](https://github.com/DaveBlooman/alb-example) which uses 2 [Docker](https://www.docker.com/) containers with 2 different languages in conjunction with an ALB and ASG.  If you take away the containers, you have a regular EC2 setup, which I think is a bit more helpful than making this a pure [ECS](https://aws.amazon.com/ecs/getting-started/) setup.   \n\nA few pieces to focus on, the listener rule\n\n```json\n{\n  \"Type\": \"AWS::ElasticLoadBalancingV2::ListenerRule\",\n  \"Properties\": {\n    \"Actions\": [\n      {\n        \"TargetGroupArn\": {\n          \"Ref\": \"ELBTargetGroup\"\n        },\n        \"Type\": \"forward\"\n      }\n    ],\n    \"Conditions\": [\n      {\n        \"Field\": \"path-pattern\",\n        \"Values\": [\n          \"/golang\"\n        ]\n      }\n    ],\n    \"ListenerArn\": {\n      \"Ref\": \"ELBListen\"\n    },\n    \"Priority\": 1\n  }\n}\n```\n\nand how to attach your EC2 instances in your ASG to your ALB\n\n```json\n{\n  \"Type\": \"AWS::AutoScaling::AutoScalingGroup\",\n  \"Properties\": {\n    \"TargetGroupARNs\": [\n      {\n        \"Ref\": \"ELBTargetGroup\"\n      },\n      {\n        \"Ref\": \"ELBTargetGroup2\"\n      }\n    ]\n  }\n}\n```\n\n### ALB All the Time\n\nThe question is, should ALB's be used everywhere?  In short, if your tooling has a nice upgrade path, yes.  ALB's offer better metrics and more flexibility down the line, while offering new features like HTTP/2 and WebSockets.  \n\nDoes ALB take away the need for HAProxy or NGINX?  Maybe, but there are going to be use cases where ALB doesn't work completely and you will need something for central management.  A simple example is IP restriction, if you have 5 ALB's that are behind HAProxy, you will only change your restriction in one place, instead of 5 security group changes.  \n\n## What Next\n\nDocker recently announced a product called Docker for AWS which can tie in with ELB, but not ALB yet.  Hopefully when this is all figured out, you should be able to create a docker service and publish it on a port and have that map to a route on the ALB.  This is better than having apps run on random ports on the ELB which is what all the demos have shown so far.  Docker Cloud only supports container based load balancing using HAProxy, so that is also out.  Kubernetes has good support for ELB too, but again, not ALB, so that is still to come.  Unsurprisingly, ECS has support, so if you are using vanilla ECS, you can upgrade immediately.  For those using [Empire](https://github.com/remind101/empire), there is an early preview available at time of writing.\n\nBottom line is that the ALB is a pretty decent product from AWS, with features that make it easier to run microservices and container architectures.\n\nRead more on the [AWS Application Load Balancer](https://aws.amazon.com/elasticloadbalancing/applicationloadbalancer/)","src/content/blog/2016-08-28-how-to-elb-with-alb.md","e63f74b851967c29",{"html":323,"metadata":324},"\u003Cp>Amazon recently announced its new \u003Ca href=\"https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/\">load balancer, the Application Load Balancer(ALB)\u003C/a>.  This load balancer can replace all elastic load balancers currently in service as well as offering several new features designed for container based architectures.  While normal load balancers offer support for EC2 based architectures, container based architectures often require \u003Ca href=\"http://www.haproxy.org/\">HAProxy\u003C/a> or \u003Ca href=\"https://www.nginx.com/\">NGINX\u003C/a> to function effectively.  This is in part due to how applications have been broken up for containers, often using micro services.\u003C/p>\n\u003Cp>An example might be one application running on port \u003Ccode>8080\u003C/code> which has a single route of \u003Ccode>/foo\u003C/code> and a second application running on port \u003Ccode>9292\u003C/code> with routes of \u003Ccode>/bar\u003C/code> and \u003Ccode>/baz\u003C/code>.\u003C/p>\n\u003Cp>If using a single monolithic stack, this wouldn’t be an issue as routing is achieved on a single port and managed in one place.  In order to achieve this kind of routing for multiple applications, another service is needed that manages the path matching for you, either on the EC2 instance itself, or externally on separate EC2 instances.  Running Apache on an EC2 instance that is setup to correctly route is quite easy, but does require updating the whole OS image or editing the config on the instance.  If running a maximum of say 3 applications, this wouldn’t be so difficult to maintain, but more than that across a large fleet of instances can be slow to update.\u003C/p>\n\u003Ch3 id=\"cool-features\">Cool Features\u003C/h3>\n\u003Cp>While many other load balancers have had these features for a while, ALB supports HTTP/2 and WebSockets out of the box.  Sticky sessions are also possible, even on WebSockets, which is a great feature for those who haven’t taken the WebSocket plunge.  The ALB also comes with a few new metrics to help you target application performance.  Each target group has its own group of metrics, so tracking down which application is performing best/worst should be easier.  Four blocks of HTTP status codes now exist, 2xx, 3xx, 4xx and 5xx.  These are available on both the ALB and the target groups.\u003C/p>\n\u003Ch3 id=\"consuletcd\">Consul/etcd\u003C/h3>\n\u003Cp>Other solutions include running a reverse proxy with a service discovery service such as \u003Ca href=\"https://www.consul.io/\">Consul\u003C/a>/\u003Ca href=\"https://coreos.com/etcd/\">etcd\u003C/a> using HAProxy as the load balancer instead of an ELB.  This would allow for simple routing to the correct servers or containers without having to setup new load balancers.  While this has its advantages, it is more to maintain and can be difficult to set up.  For automatically adding new containers, you would need to run \u003Ca href=\"https://github.com/hashicorp/consul-template\">consul-template\u003C/a> in order to dynamically update the HAProxy config.\u003C/p>\n\u003Cp>A common solution is to use sub domains instead of complex routing setups.  This isn’t ideal for applications that have very little responsibility however.  There is also URL consistency when building API’s.  For example, an API that can show orders, both by ID and the most recent 10 orders.  The most recent 10 might be \u003Ccode>example.com/api/orders/\u003C/code> with by ID being \u003Ccode>example.com/api/order/:id\u003C/code> with each being a different application.  This could be because the recent orders endpoint is using data pushed into a cache while by ID is coming from a database.\u003C/p>\n\u003Cp>Another example would be to make a single orders application which handles both the routes above, then another application that handles customers with similar api structure, \u003Ccode>example.com/api/customer/:id\u003C/code>.\u003C/p>\n\u003Cp>What is clear is that an ELB, or a Classic ELB as Amazon is now calling it, can be difficult to utilise with many separate applications.  Where you may have a single monolith, which contains everything, that is attached to a single ELB, routing is not so much an issue.\u003C/p>\n\u003Cp>AWS have often advised using ELB’s to form part of service discovery because you can attach them to containers and then have a single endpoint for which to reach your application.  This isn’t cheap though, if you run 20 services, you will pay $400 a month and that doesn’t include data charges.  Even with ELB’s in place, to correctly route your traffic, you will still need to have a service in front of all of them to route to the right load balancer.\u003C/p>\n\u003Cp>This is where the ALB really makes sense.\u003C/p>\n\u003Ch2 id=\"the-application-load-balancer\">The Application Load Balancer\u003C/h2>\n\u003Cp>The name given to this new load balancer varies depending on what page of AWS documentation you read.  ALB is the most common name used so far, but AWS have changed the name of ELB to the Classic Load Balancer in several places, so does this mean the ALB is the ELB V2?  Yes according to \u003Ca href=\"https://aws.amazon.com/cloudformation/\">CloudFormation\u003C/a>, which had support on day 1.  Current ELB’s are still ELB’s in CFN, not V1, but ALB’s are ELBV2.  So ELB’s are classic load balancers of the V1 variety and ALB’s are load balancers of the V2 variety, hope that clears up all the confusion.\u003C/p>\n\u003Ch3 id=\"new-terms\">New Terms\u003C/h3>\n\u003Cp>Target Groups and Listener Rules are new, they are the key differentiators to a classic ELB.  An ALB has listener rules, with listener rules having target groups.\u003C/p>\n\u003Cp>A target group, or target, is a port mapping to a container or server that has a health check settings on.  Once setup, the target will look for applications on the port you have selected, then attempt to send traffic once the application is healthy.  A target can be spread across lots of instances, or just one.  Target groups must have unique ports if you are going to run multiple targets on the same server.\u003C/p>\n\u003Cp>Listener rules are where the path matching occurs.  By creating an \u003Ccode>/api/orders\u003C/code> route, this is then tied to a target, which is managing the health of your application.  The pattern matching is quite powerful, supporting wildcard expressions, for example \u003Ccode>/api/orders/*\u003C/code>.\u003C/p>\n\u003Cp>Normal load balancer listeners are still there to take in traffic on, for example, port 80 and 443.  Security groups then allow traffic to pass from these listeners to your EC2 instances or containers.\u003C/p>\n\u003Ch3 id=\"root-route\">Root Route?\u003C/h3>\n\u003Cp>If you are hoping to get the root of every application, you are out of luck. Lets for example say you want to run a app on port \u003Ccode>3000\u003C/code> and the app is a third party app that requires it to run at the root, e.g \u003Ccode>:3000/\u003C/code>.  This will only work for 1 application, after this, routing must match the pattern as specified.  If you were hoping to run a \u003Ca href=\"http://grafana.org/\">grafana\u003C/a> server on \u003Ccode>:3000/grafana\u003C/code> for example, you would have to run a reverse proxy on the server to make this work, which defeats the point of using an \u003Ccode>ALB\u003C/code>.\u003C/p>\n\u003Cp>The first listener rule you register will also assume the default route, so ensure you have your applications bound to the correct order and have the correct priority.  More details \u003Ca href=\"http://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-update-rules.html\">here\u003C/a>\u003C/p>\n\u003Ch3 id=\"cloudformation-on-day-1\">CloudFormation on Day 1!!!\u003C/h3>\n\u003Cp>AWS shipping CloudFormation support on day is a rare treat.  Usually, something like \u003Ca href=\"https://www.terraform.io/\">Terraform\u003C/a> has support with a few days, or even a few hours sometimes.  CloudFormation can often take months to catch up, which makes adoption of new features difficult.  If you decided to use the new features with the console or API, you may end up playing the “What are we running in production game?”.  Hopefully AWS keeps CloudFormation up to date with ALB updates.\u003C/p>\n\u003Cp>As this is such a great day in AWS land, I have created an \u003Ca href=\"https://github.com/DaveBlooman/alb-example\">example stack here\u003C/a> which uses 2 \u003Ca href=\"https://www.docker.com/\">Docker\u003C/a> containers with 2 different languages in conjunction with an ALB and ASG.  If you take away the containers, you have a regular EC2 setup, which I think is a bit more helpful than making this a pure \u003Ca href=\"https://aws.amazon.com/ecs/getting-started/\">ECS\u003C/a> setup.\u003C/p>\n\u003Cp>A few pieces to focus on, the listener rule\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Type\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"AWS::ElasticLoadBalancingV2::ListenerRule\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Properties\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"Actions\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"TargetGroupArn\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">          \"Ref\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"ELBTargetGroup\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"Type\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"forward\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"Conditions\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"Field\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"path-pattern\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"Values\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">          \"/golang\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"ListenerArn\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Ref\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"ELBListen\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"Priority\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>and how to attach your EC2 instances in your ASG to your ALB\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Type\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"AWS::AutoScaling::AutoScalingGroup\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Properties\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"TargetGroupARNs\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"Ref\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"ELBTargetGroup\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"Ref\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"ELBTargetGroup2\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"alb-all-the-time\">ALB All the Time\u003C/h3>\n\u003Cp>The question is, should ALB’s be used everywhere?  In short, if your tooling has a nice upgrade path, yes.  ALB’s offer better metrics and more flexibility down the line, while offering new features like HTTP/2 and WebSockets.\u003C/p>\n\u003Cp>Does ALB take away the need for HAProxy or NGINX?  Maybe, but there are going to be use cases where ALB doesn’t work completely and you will need something for central management.  A simple example is IP restriction, if you have 5 ALB’s that are behind HAProxy, you will only change your restriction in one place, instead of 5 security group changes.\u003C/p>\n\u003Ch2 id=\"what-next\">What Next\u003C/h2>\n\u003Cp>Docker recently announced a product called Docker for AWS which can tie in with ELB, but not ALB yet.  Hopefully when this is all figured out, you should be able to create a docker service and publish it on a port and have that map to a route on the ALB.  This is better than having apps run on random ports on the ELB which is what all the demos have shown so far.  Docker Cloud only supports container based load balancing using HAProxy, so that is also out.  Kubernetes has good support for ELB too, but again, not ALB, so that is still to come.  Unsurprisingly, ECS has support, so if you are using vanilla ECS, you can upgrade immediately.  For those using \u003Ca href=\"https://github.com/remind101/empire\">Empire\u003C/a>, there is an early preview available at time of writing.\u003C/p>\n\u003Cp>Bottom line is that the ALB is a pretty decent product from AWS, with features that make it easier to run microservices and container architectures.\u003C/p>\n\u003Cp>Read more on the \u003Ca href=\"https://aws.amazon.com/elasticloadbalancing/applicationloadbalancer/\">AWS Application Load Balancer\u003C/a>\u003C/p>",{"headings":325,"localImagePaths":350,"remoteImagePaths":351,"frontmatter":352,"imagePaths":354},[326,329,332,335,338,341,344,347],{"depth":119,"slug":327,"text":328},"cool-features","Cool Features",{"depth":119,"slug":330,"text":331},"consuletcd","Consul/etcd",{"depth":42,"slug":333,"text":334},"the-application-load-balancer","The Application Load Balancer",{"depth":119,"slug":336,"text":337},"new-terms","New Terms",{"depth":119,"slug":339,"text":340},"root-route","Root Route?",{"depth":119,"slug":342,"text":343},"cloudformation-on-day-1","CloudFormation on Day 1!!!",{"depth":119,"slug":345,"text":346},"alb-all-the-time","ALB All the Time",{"depth":42,"slug":348,"text":349},"what-next","What Next",[],[],{"title":316,"description":317,"pubDate":353},"2016-08-28",[],"2016-09-04-lambda-deploys-with-apex",{"id":355,"data":357,"body":361,"filePath":362,"digest":363,"rendered":364},{"title":358,"description":359,"pubDate":360},"Lambda Deploys With Apex","AWS Lambda https://aws.amazon.com/Lambda/details/ has been around for a couple of years, in that time the way in which you create and deploy functions has be...",["Date","2016-09-04T00:00:00.000Z"],"[AWS Lambda](https://aws.amazon.com/Lambda/details/) has been around for a couple of years, in that time the way in which you create and deploy functions has been streamlined by several tools, Apex is one of them.  [Apex](http://apex.run/) may have only hit the scene about 8 months ago, but since then, has become one of the leading tools in Lambda automation, keeping pace with AWS releases and features.  Here is how we use Apex and Lambda to create a pipeline of services.\n\n## What is Lambda\n\nAmazon say :\n\n> AWS Lambda is a serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you.\n\nThe word serverless is used to describe your usage, rather than Amazons, as everything still runs on their servers.  Another way of describing Lambda is functions as a service, running code based on events or invoke by other AWS service calls.  An example would be sending a Cloudwatch message to slack if a Cloudwatch alarm goes into alarm state for more than 5 minutes.  Another would be to trigger a Lambda function when a file is uploaded to S3.\n\n## Just paste the code into the console, right?\n\nAmazon love to demo pasting code into the AWS console as a way to deploy Lambda functions, but this isn't really practical for teams who use version control.  Git, for example, can aid in determining what code is actually running in your function.  Things become especially hard if you upload dependencies and have a zip file with 200mb of NPM dependencies.  Then you have to download the zip file to figure out what is in there.\n\nFor any kind of consistency, you want to use a central build system, a CI server, that will control the packaging and uploading of your functions and potentially, your deployment.  There are many tools to aid in this, [Gordon](https://github.com/jorgebastida/gordon) and [Apex](https://www.github.com/apex/apex) are two that focus on Lambdas.  This is of course the [Serverless framework](https://github.com/serverless/serverless), but this is more suited to full blown web apps, rather than just deploying Lambda functions.  \n\n## Going Serverless...\n\nSlight pun here, because despite being able to write code in Python, JavaScript(node.js) and Java, I write all my Lambda functions in Go.  This is not an officially supported way of running Lambda functions, but it works great none the less.  \nIf you want to run any kind of complex function, you will have to upload dependencies, AWS lets you have up to 250mb(compressed).  This will mean that using just the console is out, so using a single static binary works nicely in this situation.  \n\nI achieved the running of a Go binary by wrapping the execution in a JavaScript shim.  The only downside here is having to have node.js installed, but hopefully AWS can support Go natively and this will solve this issue.\n\nWhen Apex came along, it solved the problems I was facing, managing the JavaScript shim, packaging, uploading and building the Go binary.  \n\n## Apex\n\n[Apex](http://apex.run/) supports all the languages that Lambda supports, as well as Go, built as a command line interface(CLI) tool.  It makes it easy to create Lambdas, but also to manage the infrastructure around them.  By default it supports Terraform, a wrapper around the AWS API.  Conversely, Gordon and Serverless both use Cloudformation.\n\nApex creates the Lambda and uploads using the API, this is different to the others in that Cloudformation does everything, allowing for referencing later in other Clouformation stacks.  You can create the Lambda functions ahead of time, then deploy to them though, so your workflow should be unaffected by Apex.  For my usage, only the Lambda creation, role assumption and upload is controlled by Apex.\n\n### Example Project\n\nThe project structure for a single Lambda with one environment will be quite simple, a functions directory with a function name, function code and function.json file.  \n\n```sh\n|____project.json\n|____functions\n| |____firstLambda\n| | |____firstLambda.js\n| | |____function.json\n```\nThere is also a project.json file at the root, where the apex CLI is used from.  This containers project wide options so that you don't need to set the same options for each function.  It can also be basic, such as\n\n```json\n{\n  \"name\": \"My First Lambda Project\",\n  \"description\": \"Service glue together some AWS Services\"\n}\n```\n\nThe function.json will include all the Apex options, runtime, timeout, environment variables, it looks something like this\n\n```json\n{\n  \"name\": \"FirstLambda\",\n  \"description\": \"Some cool Lambda function\",\n  \"memory\": 128,\n  \"timeout\": 60,\n  \"environment\": {},\n  \"runtime\": \"golang\",\n  \"role\": \"arn:aws:iam::000000000:role/Lambda-function\",\n  \"vpc\": {\n    \"securityGroups\": [\n      \"sg-acb29383\"\n    ],\n    \"subnets\": [\n      \"subnet-cgh5f4e4\"\n    ]\n  }\n}\n```\n\nWhile this works for a single Lambda, if you are running multiple environments or even multiple AWS accounts, your project will look like this\n\n```sh\n|____project.dev.json\n|____project.prod.json\n|____functions\n| |____firstLambda\n| | |____firstLambda.js\n| | |____function.dev.json\n| | |____function.prod.json\n```\n\n## Multiple Environments and Environment Variables\n\nIf you decide to simply prefix your function with an environment, `Dev-FirstLambda` for example, that will allow you to test your functions before releasing them to production.  Another way to achieve this is to use different AWS accounts.  As long as the credentials to use Apex are capable, you can control the creation and uploading of Lambdas into multiple AWS accounts.   \n\nEnvironment variables can be placed into the function.json file, but this may be a too sensitive for say, Github.  For that reason, you can mix the usage of the function.json and the Apex CLI environment variable injection.  What actually happens under the hood is a yaml file is created with your variables in, this is added to the zip file and accessible as a normal environment variable in your application thanks to the JavaScript shim.  \n\nBy using Apex, you can invoke your code locally and set environment variables to test, rather than waiting to deploy to AWS to test.  This makes for a quick feedback loop locally and doesn't require any Apex specific code, which prevents too much lock in.  There is one exception with regards to the Go functions, they are wrapped in an apex handler, but that's it.\n\n## Deploying with Apex\n\nTeamcity, Jenkins, CircleCI, however you centrally build your software, they will all work with Apex, even if you are using Windows agents.  What is really nice is that they all support environment variables, so you can create very specific jobs for you Lambda deployments.  If you are running your agents on AWS, setting up your build agents for deployment will be a case of setting the right IAM policy.\n\nHere is an example, though this will give a lot of permissions initially, so restrict once you know how you are going to use Apex.  This policy will allow for VPC based Lambdas,\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"Lambda:*\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:PassRole\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeSecurityGroups\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeSubnets\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeVpc*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\nIt should be noted that multiple AWS account deploys using Apex with IAM roles is only possible via a fork, PR is [here](https://github.com/apex/apex/pull/514)\n\n### Variables Outside of CI\n\nIf you are using config management via something like etcd or [Consul](https://www.consul.io/), you will want to bring this via your CI server.  I use Consul, so when the CI job runs, a script passes the variables to the Apex CLI after storing them from a Curl request.  We need to  pass in the region that we want to deploy our function to, this is unless the region is set in the project.json.  After our variables, we use the Lambda function name, as well as the environment that we want, in this example, we deploy the FirstLambda function with the dev environment configuration to the eu-west-1 region.\n\n```sh\nRAVEN_DSN=`curl -s $consul_host/v1/kv/dev/RAVEN_DSN\\?raw`\nS3_BUCKET=`curl -s $consul_host/v1/kv/dev/S3_BUCKET\\?raw`\n\napex deploy -r eu-west-1 -s RAVEN_DSN=$RAVEN_DSN \\\n                         -s S3_BUCKET=$S3_BUCKET \\\n                         FirstLambda -e dev\n```\n\n## The Full Picture\n\nBy using Apex, we can achieve some really complex Lambda management, which AWS account, which environment, what environment variables to use, where the environment vars are stored, how we management the creation and upload all in one tool.  Below is an example of how one of my Lambda functions is set up.\n\nGithub push kicks of the CI build, config is retrieved from [Consul](https://www.consul.io/), then the Go binaries are built by Apex while all running on Teamcity.  Apex assumes the correct role to deploy to the correct AWS account uploads the zip file created.  In this setup, a cron is setup by Cloudwatch Events that invokes the Lambda function, then events are sent to Cloudwatch logs and Cloudwatch Metrics as well as our exception handling service, Sentry.  Cloudwatch logs invokes another Lambda function itself which sends all Cloudwatch log data to Sumologic for analysis.  \n\n\n\u003Cimg src=\"/images/apex_lambda.png\" class=\"img-responsive\" alt=\"Apex\">\n\u003Ca href=\"/images/apex_lambda.png\" alt=\"Apex Lambda\">Link to image\u003C/a>\n\nAWS Lambda is a great tool, but there is much more to using it than just writing code in the AWS console.  It requires some finesse in order to create a pipeline for build, deployment and testing.  Apex defines a way of doing things that makes sense, isn't overly complicated and doesn't require lots of dependencies in order to get going while fitting in with your current infrastructure.\n\nYou can read more about [Apex here](http://apex.run/)","src/content/blog/2016-09-04-lambda-deploys-with-apex.md","e90b7ceefde5bb54",{"html":365,"metadata":366},"\u003Cp>\u003Ca href=\"https://aws.amazon.com/Lambda/details/\">AWS Lambda\u003C/a> has been around for a couple of years, in that time the way in which you create and deploy functions has been streamlined by several tools, Apex is one of them.  \u003Ca href=\"http://apex.run/\">Apex\u003C/a> may have only hit the scene about 8 months ago, but since then, has become one of the leading tools in Lambda automation, keeping pace with AWS releases and features.  Here is how we use Apex and Lambda to create a pipeline of services.\u003C/p>\n\u003Ch2 id=\"what-is-lambda\">What is Lambda\u003C/h2>\n\u003Cp>Amazon say :\u003C/p>\n\u003Cblockquote>\n\u003Cp>AWS Lambda is a serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you.\u003C/p>\n\u003C/blockquote>\n\u003Cp>The word serverless is used to describe your usage, rather than Amazons, as everything still runs on their servers.  Another way of describing Lambda is functions as a service, running code based on events or invoke by other AWS service calls.  An example would be sending a Cloudwatch message to slack if a Cloudwatch alarm goes into alarm state for more than 5 minutes.  Another would be to trigger a Lambda function when a file is uploaded to S3.\u003C/p>\n\u003Ch2 id=\"just-paste-the-code-into-the-console-right\">Just paste the code into the console, right?\u003C/h2>\n\u003Cp>Amazon love to demo pasting code into the AWS console as a way to deploy Lambda functions, but this isn’t really practical for teams who use version control.  Git, for example, can aid in determining what code is actually running in your function.  Things become especially hard if you upload dependencies and have a zip file with 200mb of NPM dependencies.  Then you have to download the zip file to figure out what is in there.\u003C/p>\n\u003Cp>For any kind of consistency, you want to use a central build system, a CI server, that will control the packaging and uploading of your functions and potentially, your deployment.  There are many tools to aid in this, \u003Ca href=\"https://github.com/jorgebastida/gordon\">Gordon\u003C/a> and \u003Ca href=\"https://www.github.com/apex/apex\">Apex\u003C/a> are two that focus on Lambdas.  This is of course the \u003Ca href=\"https://github.com/serverless/serverless\">Serverless framework\u003C/a>, but this is more suited to full blown web apps, rather than just deploying Lambda functions.\u003C/p>\n\u003Ch2 id=\"going-serverless\">Going Serverless…\u003C/h2>\n\u003Cp>Slight pun here, because despite being able to write code in Python, JavaScript(node.js) and Java, I write all my Lambda functions in Go.  This is not an officially supported way of running Lambda functions, but it works great none the less.\u003Cbr>\nIf you want to run any kind of complex function, you will have to upload dependencies, AWS lets you have up to 250mb(compressed).  This will mean that using just the console is out, so using a single static binary works nicely in this situation.\u003C/p>\n\u003Cp>I achieved the running of a Go binary by wrapping the execution in a JavaScript shim.  The only downside here is having to have node.js installed, but hopefully AWS can support Go natively and this will solve this issue.\u003C/p>\n\u003Cp>When Apex came along, it solved the problems I was facing, managing the JavaScript shim, packaging, uploading and building the Go binary.\u003C/p>\n\u003Ch2 id=\"apex\">Apex\u003C/h2>\n\u003Cp>\u003Ca href=\"http://apex.run/\">Apex\u003C/a> supports all the languages that Lambda supports, as well as Go, built as a command line interface(CLI) tool.  It makes it easy to create Lambdas, but also to manage the infrastructure around them.  By default it supports Terraform, a wrapper around the AWS API.  Conversely, Gordon and Serverless both use Cloudformation.\u003C/p>\n\u003Cp>Apex creates the Lambda and uploads using the API, this is different to the others in that Cloudformation does everything, allowing for referencing later in other Clouformation stacks.  You can create the Lambda functions ahead of time, then deploy to them though, so your workflow should be unaffected by Apex.  For my usage, only the Lambda creation, role assumption and upload is controlled by Apex.\u003C/p>\n\u003Ch3 id=\"example-project\">Example Project\u003C/h3>\n\u003Cp>The project structure for a single Lambda with one environment will be quite simple, a functions directory with a function name, function code and function.json file.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____project.json\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____functions\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____firstLambda\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____firstLambda.js\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____function.json\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>There is also a project.json file at the root, where the apex CLI is used from.  This containers project wide options so that you don’t need to set the same options for each function.  It can also be basic, such as\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"name\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"My First Lambda Project\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"description\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Service glue together some AWS Services\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The function.json will include all the Apex options, runtime, timeout, environment variables, it looks something like this\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"name\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"FirstLambda\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"description\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Some cool Lambda function\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"memory\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"timeout\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">60\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"environment\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {},\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"runtime\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"golang\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"role\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"arn:aws:iam::000000000:role/Lambda-function\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"vpc\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"securityGroups\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"sg-acb29383\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"subnets\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"subnet-cgh5f4e4\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>While this works for a single Lambda, if you are running multiple environments or even multiple AWS accounts, your project will look like this\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____project.dev.json\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____project.prod.json\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____functions\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____firstLambda\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____firstLambda.js\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____function.dev.json\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____function.prod.json\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"multiple-environments-and-environment-variables\">Multiple Environments and Environment Variables\u003C/h2>\n\u003Cp>If you decide to simply prefix your function with an environment, \u003Ccode>Dev-FirstLambda\u003C/code> for example, that will allow you to test your functions before releasing them to production.  Another way to achieve this is to use different AWS accounts.  As long as the credentials to use Apex are capable, you can control the creation and uploading of Lambdas into multiple AWS accounts.\u003C/p>\n\u003Cp>Environment variables can be placed into the function.json file, but this may be a too sensitive for say, Github.  For that reason, you can mix the usage of the function.json and the Apex CLI environment variable injection.  What actually happens under the hood is a yaml file is created with your variables in, this is added to the zip file and accessible as a normal environment variable in your application thanks to the JavaScript shim.\u003C/p>\n\u003Cp>By using Apex, you can invoke your code locally and set environment variables to test, rather than waiting to deploy to AWS to test.  This makes for a quick feedback loop locally and doesn’t require any Apex specific code, which prevents too much lock in.  There is one exception with regards to the Go functions, they are wrapped in an apex handler, but that’s it.\u003C/p>\n\u003Ch2 id=\"deploying-with-apex\">Deploying with Apex\u003C/h2>\n\u003Cp>Teamcity, Jenkins, CircleCI, however you centrally build your software, they will all work with Apex, even if you are using Windows agents.  What is really nice is that they all support environment variables, so you can create very specific jobs for you Lambda deployments.  If you are running your agents on AWS, setting up your build agents for deployment will be a case of setting the right IAM policy.\u003C/p>\n\u003Cp>Here is an example, though this will give a lot of permissions initially, so restrict once you know how you are going to use Apex.  This policy will allow for VPC based Lambdas,\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Version\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"2012-10-17\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Statement\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Effect\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Allow\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Action\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"Lambda:*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Resource\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Effect\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Allow\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Action\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"iam:PassRole\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Resource\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Effect\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Allow\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Action\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"ec2:DescribeSecurityGroups\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Resource\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Effect\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Allow\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Action\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"ec2:DescribeSubnets\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Resource\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Effect\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Allow\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Action\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"ec2:DescribeVpc*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"Resource\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>It should be noted that multiple AWS account deploys using Apex with IAM roles is only possible via a fork, PR is \u003Ca href=\"https://github.com/apex/apex/pull/514\">here\u003C/a>\u003C/p>\n\u003Ch3 id=\"variables-outside-of-ci\">Variables Outside of CI\u003C/h3>\n\u003Cp>If you are using config management via something like etcd or \u003Ca href=\"https://www.consul.io/\">Consul\u003C/a>, you will want to bring this via your CI server.  I use Consul, so when the CI job runs, a script passes the variables to the Apex CLI after storing them from a Curl request.  We need to  pass in the region that we want to deploy our function to, this is unless the region is set in the project.json.  After our variables, we use the Lambda function name, as well as the environment that we want, in this example, we deploy the FirstLambda function with the dev environment configuration to the eu-west-1 region.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">RAVEN_DSN\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">`\u003C/span>\u003Cspan style=\"color:#B392F0\">curl\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -s\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $consul_host\u003C/span>\u003Cspan style=\"color:#9ECBFF\">/v1/kv/dev/RAVEN_DSN\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\?\u003C/span>\u003Cspan style=\"color:#9ECBFF\">raw`\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">S3_BUCKET\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">`\u003C/span>\u003Cspan style=\"color:#B392F0\">curl\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -s\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $consul_host\u003C/span>\u003Cspan style=\"color:#9ECBFF\">/v1/kv/dev/S3_BUCKET\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\?\u003C/span>\u003Cspan style=\"color:#9ECBFF\">raw`\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">apex\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> deploy\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -r\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> eu-west-1\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -s\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> RAVEN_DSN=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">$RAVEN_DSN \u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">                         -s\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> S3_BUCKET=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">$S3_BUCKET \u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">                         FirstLambda\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -e\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> dev\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"the-full-picture\">The Full Picture\u003C/h2>\n\u003Cp>By using Apex, we can achieve some really complex Lambda management, which AWS account, which environment, what environment variables to use, where the environment vars are stored, how we management the creation and upload all in one tool.  Below is an example of how one of my Lambda functions is set up.\u003C/p>\n\u003Cp>Github push kicks of the CI build, config is retrieved from \u003Ca href=\"https://www.consul.io/\">Consul\u003C/a>, then the Go binaries are built by Apex while all running on Teamcity.  Apex assumes the correct role to deploy to the correct AWS account uploads the zip file created.  In this setup, a cron is setup by Cloudwatch Events that invokes the Lambda function, then events are sent to Cloudwatch logs and Cloudwatch Metrics as well as our exception handling service, Sentry.  Cloudwatch logs invokes another Lambda function itself which sends all Cloudwatch log data to Sumologic for analysis.\u003C/p>\n\u003Cimg src=\"/images/apex_lambda.png\" class=\"img-responsive\" alt=\"Apex\">\n\u003Ca href=\"/images/apex_lambda.png\" alt=\"Apex Lambda\">Link to image\u003C/a>\n\u003Cp>AWS Lambda is a great tool, but there is much more to using it than just writing code in the AWS console.  It requires some finesse in order to create a pipeline for build, deployment and testing.  Apex defines a way of doing things that makes sense, isn’t overly complicated and doesn’t require lots of dependencies in order to get going while fitting in with your current infrastructure.\u003C/p>\n\u003Cp>You can read more about \u003Ca href=\"http://apex.run/\">Apex here\u003C/a>\u003C/p>",{"headings":367,"localImagePaths":395,"remoteImagePaths":396,"frontmatter":397,"imagePaths":399},[368,371,374,377,380,383,386,389,392],{"depth":42,"slug":369,"text":370},"what-is-lambda","What is Lambda",{"depth":42,"slug":372,"text":373},"just-paste-the-code-into-the-console-right","Just paste the code into the console, right?",{"depth":42,"slug":375,"text":376},"going-serverless","Going Serverless…",{"depth":42,"slug":378,"text":379},"apex","Apex",{"depth":119,"slug":381,"text":382},"example-project","Example Project",{"depth":42,"slug":384,"text":385},"multiple-environments-and-environment-variables","Multiple Environments and Environment Variables",{"depth":42,"slug":387,"text":388},"deploying-with-apex","Deploying with Apex",{"depth":119,"slug":390,"text":391},"variables-outside-of-ci","Variables Outside of CI",{"depth":42,"slug":393,"text":394},"the-full-picture","The Full Picture",[],[],{"title":358,"description":359,"pubDate":398},"2016-09-04",[],"2014-12-23-quick-uptime-tests",{"id":400,"data":402,"body":406,"filePath":407,"digest":408,"rendered":409},{"title":403,"description":404,"pubDate":405},"Quick Uptime Tests","We had a huge bug on the live site, http://www.bbc.co.uk/news http://www.bbc.co.uk/arabic , the navigation for our Asia edition was broken. Most of the links...",["Date","2014-12-23T00:00:00.000Z"],"We had a huge bug on the live site, [http://www.bbc.co.uk/news](http://www.bbc.co.uk/arabic), the navigation for our Asia edition was broken.  Most of the links in the navigation were 404ing, we had user complaints and there was a lot of \"how did this make it to Live?\"  \n\nTo understand how such a big bug made it to live, understand that BBC News isn't one website, [it's 32]( http://www.live.bbc.co.uk/ws/languages).  Some of these sites, /news for example, have multiple editions for localised content, Asia being one of them.  This introduces a huge task for link checking everything on a page.  After the fact, I create a tool to make the process of looking for 404's quick and automated, it's called [Linkey](https://github.com/DaveBlooman/linkey).....Naming things is hard.\n\n### Linkey\n\nLinkey was designed to visit a live site, scrape all the links of the page, then remove the domain and non matching links, then hit the matches against a test/sandbox environment.  It would then check the status for then given URL and return it's status code.  If all URL's had a dollar sign appended to it by mistake, it would return a 404 and you knew you had a problem.  \n\nThe matches would come from a regex, e.g www.bbc.co.uk/news/uk with the regex of /news would return a result of /news/uk.  A URL of www.bbc.co.uk/sport would be ignored.  Matches would be added to the second domain path, which will then return status codes. The command looks like this\n\n```sh\nlinkey check http://www.bbc.co.uk/arabic http://www.bbc.co.uk /arabic arabic.md\n```\nI have used the same domain above, but a localhost example\n\n```sh\nlinkey check http://www.bbc.co.uk/arabic http://localhost:8080 /arabic arabic.md\n```\n\n### Smoke\nLinkey was great at what it did, checking if a URL returned a 200 or not, but it was limited to grabbing links from a website.  This meant you didn't know every link you were going to check before hand, enter the Yaml file.  \nA simple Yaml file with a base URL and lot of pages with a new smoke feature and we could check the status of hundreds of pages in seconds.  Speed was key here, so parallel HTTP requests are made using [Faraday](https://github.com/lostisland/faraday) and [Typhoeus](https://github.com/typhoeus/typhoeus).  \n\nA config file will look something like this\n\n```yaml\nbase: 'http://www.bbc.co.uk'\n\npaths:\n  - /news/events/scotland-decides\n  - /news/england/london\n  - /news\n\n```\nWhich is run like this\n\n```sh\nlinkey smoke example.yaml\n```\n\nThe first big usage of this was the Scottish referendum, potentially the biggest news event for our site ever.  This was different than the rest of the website as we were using AWS to serve the main Scotland decides page.  We were testing endpoints for various parts of this AWS based application as well as the front end of the /news website to ensure uptime.  \n\nWe ran the job every 20 minutes and when something was down, we knew about it.  This has now changed to fit into our CI pipeline, but just getting the feedback was enough.\n\n### Pingdom?\n\nThere are services out there that will tell you when you have a service down, or that your service is returning 500's, so why not just use one of them?  Cost, ease of use and time were a factor in that we needed something that just worked.  It also was helpful to have something that any developer could run locally, ensuring that routes weren't broken.\n\n### Fast\n\nThis tool is now used across News after we do a live deploy for the main website, which only happens every 2 weeks.  We push to Test, then Stage then Live, running Linkey after each deploy.  We test around 700 URL's altogether taking less than 90 seconds to check them all.  This sort of testing is simple, but it gives you that extra bit of reassurance that your release is stable.\n\n### Code\n\nYou can view the source code for [Linkey here](https://github.com/DaveBlooman/linkey), with PR's welcome.","src/content/blog/2014-12-23-quick-uptime-tests.md","e8340c8af878bffc",{"html":410,"metadata":411},"\u003Cp>We had a huge bug on the live site, \u003Ca href=\"http://www.bbc.co.uk/arabic\">http://www.bbc.co.uk/news\u003C/a>, the navigation for our Asia edition was broken.  Most of the links in the navigation were 404ing, we had user complaints and there was a lot of “how did this make it to Live?”\u003C/p>\n\u003Cp>To understand how such a big bug made it to live, understand that BBC News isn’t one website, \u003Ca href=\"http://www.live.bbc.co.uk/ws/languages\">it’s 32\u003C/a>.  Some of these sites, /news for example, have multiple editions for localised content, Asia being one of them.  This introduces a huge task for link checking everything on a page.  After the fact, I create a tool to make the process of looking for 404’s quick and automated, it’s called \u003Ca href=\"https://github.com/DaveBlooman/linkey\">Linkey\u003C/a>…Naming things is hard.\u003C/p>\n\u003Ch3 id=\"linkey\">Linkey\u003C/h3>\n\u003Cp>Linkey was designed to visit a live site, scrape all the links of the page, then remove the domain and non matching links, then hit the matches against a test/sandbox environment.  It would then check the status for then given URL and return it’s status code.  If all URL’s had a dollar sign appended to it by mistake, it would return a 404 and you knew you had a problem.\u003C/p>\n\u003Cp>The matches would come from a regex, e.g \u003Ca href=\"http://www.bbc.co.uk/news/uk\">www.bbc.co.uk/news/uk\u003C/a> with the regex of /news would return a result of /news/uk.  A URL of \u003Ca href=\"http://www.bbc.co.uk/sport\">www.bbc.co.uk/sport\u003C/a> would be ignored.  Matches would be added to the second domain path, which will then return status codes. The command looks like this\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">linkey\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> check\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> http://www.bbc.co.uk/arabic\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> http://www.bbc.co.uk\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /arabic\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> arabic.md\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>I have used the same domain above, but a localhost example\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">linkey\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> check\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> http://www.bbc.co.uk/arabic\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> http://localhost:8080\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /arabic\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> arabic.md\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"smoke\">Smoke\u003C/h3>\n\u003Cp>Linkey was great at what it did, checking if a URL returned a 200 or not, but it was limited to grabbing links from a website.  This meant you didn’t know every link you were going to check before hand, enter the Yaml file.\u003Cbr>\nA simple Yaml file with a base URL and lot of pages with a new smoke feature and we could check the status of hundreds of pages in seconds.  Speed was key here, so parallel HTTP requests are made using \u003Ca href=\"https://github.com/lostisland/faraday\">Faraday\u003C/a> and \u003Ca href=\"https://github.com/typhoeus/typhoeus\">Typhoeus\u003C/a>.\u003C/p>\n\u003Cp>A config file will look something like this\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">base\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">'http://www.bbc.co.uk'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">paths\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/news/events/scotland-decides\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/news/england/london\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/news\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Which is run like this\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">linkey\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> smoke\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> example.yaml\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The first big usage of this was the Scottish referendum, potentially the biggest news event for our site ever.  This was different than the rest of the website as we were using AWS to serve the main Scotland decides page.  We were testing endpoints for various parts of this AWS based application as well as the front end of the /news website to ensure uptime.\u003C/p>\n\u003Cp>We ran the job every 20 minutes and when something was down, we knew about it.  This has now changed to fit into our CI pipeline, but just getting the feedback was enough.\u003C/p>\n\u003Ch3 id=\"pingdom\">Pingdom?\u003C/h3>\n\u003Cp>There are services out there that will tell you when you have a service down, or that your service is returning 500’s, so why not just use one of them?  Cost, ease of use and time were a factor in that we needed something that just worked.  It also was helpful to have something that any developer could run locally, ensuring that routes weren’t broken.\u003C/p>\n\u003Ch3 id=\"fast\">Fast\u003C/h3>\n\u003Cp>This tool is now used across News after we do a live deploy for the main website, which only happens every 2 weeks.  We push to Test, then Stage then Live, running Linkey after each deploy.  We test around 700 URL’s altogether taking less than 90 seconds to check them all.  This sort of testing is simple, but it gives you that extra bit of reassurance that your release is stable.\u003C/p>\n\u003Ch3 id=\"code\">Code\u003C/h3>\n\u003Cp>You can view the source code for \u003Ca href=\"https://github.com/DaveBlooman/linkey\">Linkey here\u003C/a>, with PR’s welcome.\u003C/p>",{"headings":412,"localImagePaths":428,"remoteImagePaths":429,"frontmatter":430,"imagePaths":432},[413,416,419,422,425],{"depth":119,"slug":414,"text":415},"linkey","Linkey",{"depth":119,"slug":417,"text":418},"smoke","Smoke",{"depth":119,"slug":420,"text":421},"pingdom","Pingdom?",{"depth":119,"slug":423,"text":424},"fast","Fast",{"depth":119,"slug":426,"text":427},"code","Code",[],[],{"title":403,"description":404,"pubDate":431},"2014-12-23",[],"2016-11-08-deploying-go-microservices-with-aws-codedeploy",{"id":433,"data":435,"body":439,"filePath":440,"digest":441,"rendered":442},{"title":436,"description":437,"pubDate":438},"Deploying Go Microservices With AWS CodeDeploy","We have a familiar story at FundApps https://www.fundapps.co/ , big monolith, trying to break into microservices, same old same old. We have a dotnet stack,...",["Date","2016-11-08T00:00:00.000Z"],"We have a familiar story at [FundApps](https://www.fundapps.co/), big monolith, trying to break into microservices, same old same old.  We have a dotnet stack, on Windows, which is deployed using Octopus and TeamCity CI.  We wanted to use Go, but weren't interested in the Docker route, so what do you choose in that situation?\n\n### The Change\n\nWhen deciding to move away from dotnet for some new applications, there are some questions around culture, workflow and learning.  If you are building and testing code on a CI server, then deploying via CI to your environments, it makes to keep the same approach.  The change came in the tooling and technologies, so lots of change, but lots of familiarity.\n\nWe did some direct replacements, Linux for Windows, Golang for C# and Codedeploy for Octopus.  This may seem like an odd choice, but it actually allowed us to achieve our core goal.\n\n`How quickly can we get into production and deploy whenever we want`\n\nConsidering there was no existing system for doing this on Linux within the company, we had a lot of opportunities, we felt CodeDeploy was a good fit.\n\n### CodeDeploy\n\nFor those unfamiliar with [CodeDeploy](https://aws.amazon.com/codedeploy/), its a system that allows for deploying anything to a bunch of servers using simple scripts and uses a straight forward YAML file to describe the deploy.  \n\nThe key components are applications, deployment groups, appspec files and revisions.  Deployment groups are the servers you are going to deploy to, based on AWS EC2 tags or auto scaling group name.  An appspec file is the YAML file you will use to describe your deployment, before and after the code is updated, what files to cleanup, where to put new files etc.  Revisions are typically zip or tar files in S3 containing your code ready to be unpacked onto the servers.\n\n### Why CodeDeploy\n\nIt sounds like a straight forward service, and it is.  For a startup like FundApps with a small engineering team, having a tool that is hosted, has a solid API and has plugins for CI is essential.  We don't want to spend a lot of time building tooling.  \n\nOne way of deploying servers is the bakery model, packaging up AWS images, AMI's, then rolling them out in autoscaling groups.  This is nice because you can use golden images, something that you can roll back to if the new image is not working for example.  This can be quite slow though, especially for a 10MB Go binary, it is actually quite a costly process.\n\nCodeDeploy takes under a minute to deploy and logs all the information about your deployment as it happens, if something goes wrong, it can roll back.  There are options for how you would like your deployment to go as well, all servers at once, one at a time or custom roll outs.  If a one at a time deployment failed on server 1, it will not roll out to server 2 and 3 for example, which gives an opportunity to investigate the deployment.\n\n### App Spec\n\nThe App spec file for a sample deploy is simple, the binary, some db migrations, consul service and systemd service file.  Scripts make everything work, stopping the existing service, starting the new once, clean up of files and finally, validating the service is running.  \n\nThis is the core part of CodeDeploy, the ordering, user and timeouts are important to get right for consistently smooth deploys.\n\n```yaml\nversion: 0.0\nos: linux\nfiles:\n  - source: go_binary\n    destination: /opt/go_binary\n  - source: go_service.service\n    destination: /usr/lib/systemd/system/\n  - source: go_service.json\n    destination: /etc/consul.d/config\n  - source: migrations/\n    destination: /opt/go_service/migrations/\nhooks:\n  BeforeInstall:\n    - location: stop_server.sh\n      timeout: 300\n      runas: root\n  AfterInstall:\n    - location: setup_db.sh\n    - location: start_server.sh\n      timeout: 180\n      runas: root\n  ValidateService:\n    - location: check_service.sh\n      timeout: 180\n      runas: root\n```\n\n### The Console\n\nThe UI for CodeDeploy is quite clumsy, there are many clicks to get to the information you want and it is hard to jump to the logs quickly.  We use Teamcity for all our deploys though, which makes continuous delivery with CodeDeploy simple to setup.  We will only use the UI when we have to debug something that is prevent a deploy.\n\nImprovements I'd like to see include better cross server inspecting, how a deploy went in total, not just per instance which is currently the case.  \n\nI made a simple UI with a server that has a Go backend and came up with this, which gives much more of a snapshot view of CodeDeploy deployments.\n\n\u003Cimg src=\"/images/codedeploy.jpg\" class=\"img-responsive\" alt=\"Apex\">\n\u003Ca href=\"/images/codedeploy.png\" alt=\"Apex Lambda\">Link to image\u003C/a>\n\n### Multiple Accounts And Regions\n\nOne practice that I'm a firm believer in is multiple AWS accounts, one for production and one for development and testing.  This is made easy by using Terraform of Cloudformation for templating infrastructure.\n\nCodeDeploy sort of works in the way, but it is still not ideal.  Essentially, an AWS account region is its own little world, it doesn't know about other regions or accounts for that matter, so you are left on your own to make that work.  In our setup, we use CI, so each deploy job takes the same zip built in an earlier step, then uploads it to AWS S3.  It is then deployed onto the servers.\n\nIf you want to deploy to 3 regions in 2 AWS accounts, you are going to have to script it or utilise some clever build jobs, the TeamCity plugin for CodeDeploy doesn't work for multiple regions, so even using a nice plugin is out.\n\nThis comes down to the concept of an environment, which CodeDeploy doesn't support, which is a shame.  If you aren't using a second AWS account, you are probably going to be prefixing your applications with test-app or dev-app.  \n\nI would like to have the ability to \"promote\" a release from a test environment to a prod environment.\n\n### Building the zip File\n\nCodeDeploy works by essentially taking a zip file copying the contents onto disk, then copy those files to another directory that you decide in your appspec file.  The structure is important, with the appspec being in the root of your zip file, with everything relative to that file in file copy terms.\n\nFor our Go services, we want to build the Go binary into a predefined directory with all our other files needed for the deploy.  We use a Makefile for this with the following commands\n\n```sh\ngo build -ldflags \"-X main.version=$(BUILD_NUMBER)\" -o infrastructure/go_service\nzip -r -j go_app.zip src/github.com/fundapps/go_service/src/infrastructure/*\n```\n\nThis does a couple of things, the build step sets the build version to the build number of TeamCity.  This is good for 2 reasons, we can access that number in the status endpoint to see what version is on what environment, but it\nis also the version that is pushed to Github releases page.  \n\nFor debugging, hit the status endpoint, check the version, then you can simply `git checkout 1.214`.\n\nThe status endpoint is quite simple, but looks like this\n\n```json\n{\n  \"Database\": \"Good\",\n  \"Status for SQS queue\": \"Healthy\",\n  \"version\": \"1.214\"\n}\n```\n\nWe only build this zip file once, then deploy it to both environments assuming that the deployment is successful.  \n\n### Artefacts\n\nEvery time we build a zip file, it is uploaded to S3, because of the size we can persist these for a long period of time without incurring huge cost.  This is counter to the bakery model I mentioned earlier, storing a few MB of zips is a lot cheaper than storing terabytes of EBS volumes for AMIs.\n\n### ....Sounds Boring\n\nI know this is not the most exciting way of deploying Go services, I haven't said Docker enough times for good SEO, but that is actually a good point for us.  Boring deployment stories are nice for me because I don't have to worry about them not working.  Deployments using CodeDeploy usually fail because of a bad appspec file, not because the service is doing something wrong.\n\nWe are actually doing some really cool stuff.\n\n**Cool Bits**  \nWe use Consul for service discovery, this drives dynamic config for our apps as well as reloading load balancer configs.  When a new service comes up, either because of deploy or an autoscaling action, Consul will discover the new service, add it to the load balancer using [Consul Template](https://github.com/hashicorp/consul-template) when its healthy and take it out when its not.\n\nWe use [envconsul](https://github.com/hashicorp/envconsul) to drive dynamic config changes from the Consul KV store.  If you are currently having to rebuild your server or Docker image because you bake in config, you probably know how a single character mistake can cause you to rebuild and redeploy.  envconsul is the best solution I have seen to prevent that, by giving you lots of options in how the app restart, what values from Consul are going to be injected into your app and what the behaviour should be if something fails.\n\nFinally, we use [Telegraf](https://github.com/influxdata/telegraf) with StatsD in the Go services to send all metrics to Cloudwatch, making it really easy to setup custom metrics in any Go service.\n\n### Conclusion\n\nConsidering where our company was 6 months ago, we have done a lot of new things in our code and infrastructure.  This is going to be an iterative process for us, perhaps we'll use ECS and Docker at some point, but for now, the not as sexy way of doing deployments is working great for us, it probably will for you too.","src/content/blog/2016-11-08-deploying-go-microservices-with-aws-codedeploy.md","a68393f1ec742547",{"html":443,"metadata":444},"\u003Cp>We have a familiar story at \u003Ca href=\"https://www.fundapps.co/\">FundApps\u003C/a>, big monolith, trying to break into microservices, same old same old.  We have a dotnet stack, on Windows, which is deployed using Octopus and TeamCity CI.  We wanted to use Go, but weren’t interested in the Docker route, so what do you choose in that situation?\u003C/p>\n\u003Ch3 id=\"the-change\">The Change\u003C/h3>\n\u003Cp>When deciding to move away from dotnet for some new applications, there are some questions around culture, workflow and learning.  If you are building and testing code on a CI server, then deploying via CI to your environments, it makes to keep the same approach.  The change came in the tooling and technologies, so lots of change, but lots of familiarity.\u003C/p>\n\u003Cp>We did some direct replacements, Linux for Windows, Golang for C# and Codedeploy for Octopus.  This may seem like an odd choice, but it actually allowed us to achieve our core goal.\u003C/p>\n\u003Cp>\u003Ccode>How quickly can we get into production and deploy whenever we want\u003C/code>\u003C/p>\n\u003Cp>Considering there was no existing system for doing this on Linux within the company, we had a lot of opportunities, we felt CodeDeploy was a good fit.\u003C/p>\n\u003Ch3 id=\"codedeploy\">CodeDeploy\u003C/h3>\n\u003Cp>For those unfamiliar with \u003Ca href=\"https://aws.amazon.com/codedeploy/\">CodeDeploy\u003C/a>, its a system that allows for deploying anything to a bunch of servers using simple scripts and uses a straight forward YAML file to describe the deploy.\u003C/p>\n\u003Cp>The key components are applications, deployment groups, appspec files and revisions.  Deployment groups are the servers you are going to deploy to, based on AWS EC2 tags or auto scaling group name.  An appspec file is the YAML file you will use to describe your deployment, before and after the code is updated, what files to cleanup, where to put new files etc.  Revisions are typically zip or tar files in S3 containing your code ready to be unpacked onto the servers.\u003C/p>\n\u003Ch3 id=\"why-codedeploy\">Why CodeDeploy\u003C/h3>\n\u003Cp>It sounds like a straight forward service, and it is.  For a startup like FundApps with a small engineering team, having a tool that is hosted, has a solid API and has plugins for CI is essential.  We don’t want to spend a lot of time building tooling.\u003C/p>\n\u003Cp>One way of deploying servers is the bakery model, packaging up AWS images, AMI’s, then rolling them out in autoscaling groups.  This is nice because you can use golden images, something that you can roll back to if the new image is not working for example.  This can be quite slow though, especially for a 10MB Go binary, it is actually quite a costly process.\u003C/p>\n\u003Cp>CodeDeploy takes under a minute to deploy and logs all the information about your deployment as it happens, if something goes wrong, it can roll back.  There are options for how you would like your deployment to go as well, all servers at once, one at a time or custom roll outs.  If a one at a time deployment failed on server 1, it will not roll out to server 2 and 3 for example, which gives an opportunity to investigate the deployment.\u003C/p>\n\u003Ch3 id=\"app-spec\">App Spec\u003C/h3>\n\u003Cp>The App spec file for a sample deploy is simple, the binary, some db migrations, consul service and systemd service file.  Scripts make everything work, stopping the existing service, starting the new once, clean up of files and finally, validating the service is running.\u003C/p>\n\u003Cp>This is the core part of CodeDeploy, the ordering, user and timeouts are important to get right for consistently smooth deploys.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">version\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.0\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">os\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">linux\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">files\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">source\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">go_binary\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    destination\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/opt/go_binary\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">source\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">go_service.service\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    destination\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/usr/lib/systemd/system/\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">source\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">go_service.json\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    destination\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/etc/consul.d/config\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  - \u003C/span>\u003Cspan style=\"color:#85E89D\">source\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">migrations/\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    destination\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">/opt/go_service/migrations/\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">hooks\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  BeforeInstall\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    - \u003C/span>\u003Cspan style=\"color:#85E89D\">location\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">stop_server.sh\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      timeout\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">300\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      runas\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">root\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  AfterInstall\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    - \u003C/span>\u003Cspan style=\"color:#85E89D\">location\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">setup_db.sh\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    - \u003C/span>\u003Cspan style=\"color:#85E89D\">location\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">start_server.sh\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      timeout\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">180\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      runas\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">root\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  ValidateService\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    - \u003C/span>\u003Cspan style=\"color:#85E89D\">location\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">check_service.sh\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      timeout\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">180\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      runas\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">root\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"the-console\">The Console\u003C/h3>\n\u003Cp>The UI for CodeDeploy is quite clumsy, there are many clicks to get to the information you want and it is hard to jump to the logs quickly.  We use Teamcity for all our deploys though, which makes continuous delivery with CodeDeploy simple to setup.  We will only use the UI when we have to debug something that is prevent a deploy.\u003C/p>\n\u003Cp>Improvements I’d like to see include better cross server inspecting, how a deploy went in total, not just per instance which is currently the case.\u003C/p>\n\u003Cp>I made a simple UI with a server that has a Go backend and came up with this, which gives much more of a snapshot view of CodeDeploy deployments.\u003C/p>\n\u003Cimg src=\"/images/codedeploy.jpg\" class=\"img-responsive\" alt=\"Apex\">\n\u003Ca href=\"/images/codedeploy.png\" alt=\"Apex Lambda\">Link to image\u003C/a>\n\u003Ch3 id=\"multiple-accounts-and-regions\">Multiple Accounts And Regions\u003C/h3>\n\u003Cp>One practice that I’m a firm believer in is multiple AWS accounts, one for production and one for development and testing.  This is made easy by using Terraform of Cloudformation for templating infrastructure.\u003C/p>\n\u003Cp>CodeDeploy sort of works in the way, but it is still not ideal.  Essentially, an AWS account region is its own little world, it doesn’t know about other regions or accounts for that matter, so you are left on your own to make that work.  In our setup, we use CI, so each deploy job takes the same zip built in an earlier step, then uploads it to AWS S3.  It is then deployed onto the servers.\u003C/p>\n\u003Cp>If you want to deploy to 3 regions in 2 AWS accounts, you are going to have to script it or utilise some clever build jobs, the TeamCity plugin for CodeDeploy doesn’t work for multiple regions, so even using a nice plugin is out.\u003C/p>\n\u003Cp>This comes down to the concept of an environment, which CodeDeploy doesn’t support, which is a shame.  If you aren’t using a second AWS account, you are probably going to be prefixing your applications with test-app or dev-app.\u003C/p>\n\u003Cp>I would like to have the ability to “promote” a release from a test environment to a prod environment.\u003C/p>\n\u003Ch3 id=\"building-the-zip-file\">Building the zip File\u003C/h3>\n\u003Cp>CodeDeploy works by essentially taking a zip file copying the contents onto disk, then copy those files to another directory that you decide in your appspec file.  The structure is important, with the appspec being in the root of your zip file, with everything relative to that file in file copy terms.\u003C/p>\n\u003Cp>For our Go services, we want to build the Go binary into a predefined directory with all our other files needed for the deploy.  We use a Makefile for this with the following commands\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">go\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> build\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -ldflags\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"-X main.version=$(\u003C/span>\u003Cspan style=\"color:#B392F0\">BUILD_NUMBER\u003C/span>\u003Cspan style=\"color:#9ECBFF\">)\"\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -o\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> infrastructure/go_service\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">zip\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -r\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -j\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> go_app.zip\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> src/github.com/fundapps/go_service/src/infrastructure/\u003C/span>\u003Cspan style=\"color:#79B8FF\">*\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This does a couple of things, the build step sets the build version to the build number of TeamCity.  This is good for 2 reasons, we can access that number in the status endpoint to see what version is on what environment, but it\nis also the version that is pushed to Github releases page.\u003C/p>\n\u003Cp>For debugging, hit the status endpoint, check the version, then you can simply \u003Ccode>git checkout 1.214\u003C/code>.\u003C/p>\n\u003Cp>The status endpoint is quite simple, but looks like this\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Database\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Good\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Status for SQS queue\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Healthy\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"version\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"1.214\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>We only build this zip file once, then deploy it to both environments assuming that the deployment is successful.\u003C/p>\n\u003Ch3 id=\"artefacts\">Artefacts\u003C/h3>\n\u003Cp>Every time we build a zip file, it is uploaded to S3, because of the size we can persist these for a long period of time without incurring huge cost.  This is counter to the bakery model I mentioned earlier, storing a few MB of zips is a lot cheaper than storing terabytes of EBS volumes for AMIs.\u003C/p>\n\u003Ch3 id=\"sounds-boring\">…Sounds Boring\u003C/h3>\n\u003Cp>I know this is not the most exciting way of deploying Go services, I haven’t said Docker enough times for good SEO, but that is actually a good point for us.  Boring deployment stories are nice for me because I don’t have to worry about them not working.  Deployments using CodeDeploy usually fail because of a bad appspec file, not because the service is doing something wrong.\u003C/p>\n\u003Cp>We are actually doing some really cool stuff.\u003C/p>\n\u003Cp>\u003Cstrong>Cool Bits\u003C/strong>\u003Cbr>\nWe use Consul for service discovery, this drives dynamic config for our apps as well as reloading load balancer configs.  When a new service comes up, either because of deploy or an autoscaling action, Consul will discover the new service, add it to the load balancer using \u003Ca href=\"https://github.com/hashicorp/consul-template\">Consul Template\u003C/a> when its healthy and take it out when its not.\u003C/p>\n\u003Cp>We use \u003Ca href=\"https://github.com/hashicorp/envconsul\">envconsul\u003C/a> to drive dynamic config changes from the Consul KV store.  If you are currently having to rebuild your server or Docker image because you bake in config, you probably know how a single character mistake can cause you to rebuild and redeploy.  envconsul is the best solution I have seen to prevent that, by giving you lots of options in how the app restart, what values from Consul are going to be injected into your app and what the behaviour should be if something fails.\u003C/p>\n\u003Cp>Finally, we use \u003Ca href=\"https://github.com/influxdata/telegraf\">Telegraf\u003C/a> with StatsD in the Go services to send all metrics to Cloudwatch, making it really easy to setup custom metrics in any Go service.\u003C/p>\n\u003Ch3 id=\"conclusion\">Conclusion\u003C/h3>\n\u003Cp>Considering where our company was 6 months ago, we have done a lot of new things in our code and infrastructure.  This is going to be an iterative process for us, perhaps we’ll use ECS and Docker at some point, but for now, the not as sexy way of doing deployments is working great for us, it probably will for you too.\u003C/p>",{"headings":445,"localImagePaths":474,"remoteImagePaths":475,"frontmatter":476,"imagePaths":478},[446,449,452,455,458,461,464,467,470,473],{"depth":119,"slug":447,"text":448},"the-change","The Change",{"depth":119,"slug":450,"text":451},"codedeploy","CodeDeploy",{"depth":119,"slug":453,"text":454},"why-codedeploy","Why CodeDeploy",{"depth":119,"slug":456,"text":457},"app-spec","App Spec",{"depth":119,"slug":459,"text":460},"the-console","The Console",{"depth":119,"slug":462,"text":463},"multiple-accounts-and-regions","Multiple Accounts And Regions",{"depth":119,"slug":465,"text":466},"building-the-zip-file","Building the zip File",{"depth":119,"slug":468,"text":469},"artefacts","Artefacts",{"depth":119,"slug":471,"text":472},"sounds-boring","…Sounds Boring",{"depth":119,"slug":55,"text":56},[],[],{"title":436,"description":437,"pubDate":477},"2016-11-08",[],"2017-02-07-cross-account-aws-lambda-deployments",{"id":479,"data":481,"body":485,"filePath":486,"digest":487,"rendered":488},{"title":482,"description":483,"pubDate":484},"Cross Account AWS Lambda Deployments","I recently talked at the Serverless London meetup https://www.meetup.com/Serverless London/events/236664340/ , I was asked about how we do cross AWS account...",["Date","2017-02-07T00:00:00.000Z"],"I recently talked at the [Serverless London meetup](https://www.meetup.com/Serverless-London/events/236664340/), I was asked about how we do cross AWS account deploys with Lambda functions.  You can see the [slides here](https://speakerdeck.com/daveblooman/deploying-with-apex) and the [video here](https://www.twitch.tv/videos/119142356­).\n\nThis way of working is great for teams that have many accounts for dev, test, stage or prod.  By using multiple AWS accounts you can benefit from perform isolated testing, locked down prod environments and testing CI integration.  \n\nHere's how we do it.\n\n### Accounts, Accounts, Accounts\n\nTo start, you want to be running your deployments from an AWS EC2 instance.  This is usually via a CI server like Jenkins, Teamcity or GOCD.  By using a CI server on EC2, you can use the benefits of IAM roles.\n\nThis sets up the deployment model, with one account, A, deploying to account A, then use A to deploy to another account, B.  For example.  \n\n```\nAccount A CI server -> Account A  \nAccount A CI server -> Account B  \n```\nYou could also deploy from account A, which could be a tooling account, to other accounts.\n\n```\nAccount A CI server -> Account B   \nAccount A CI server -> Account C  \n```\nYour account setup will depend on the structure of the IAM policies you will need, but follows the same basic structure.\n\n### Identity Access Management\n\nThe way the IAM works with cross account deploys is that the CI server will assume a new role, one that enables the deployments.  This may be in the same account or another AWS account.  \n\nFor our example, we will be using the example above, with 2 AWS accounts with account A deploying to account A and account A deploying to account B.\n\n#### Policy\nThis is example templates for Terraform, if you would like the full working example, [visit the Github repo](https://github.com/DaveBlooman/cross_account_deploys).\n\nWith Terraform, the first step is to create a role with account A, set the principal to be that of account A in the terraform below.  \n\n```hcl\n\nresource \"aws_iam_role\" \"lambda_assume_role\" {\n  name = \"lambda-assume-role\"\n  path = \"/\"\n\n  assume_role_policy = \u003C\u003CPOLICY\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::000000000001:root\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nPOLICY\n}\n\nresource \"aws_iam_role_policy\" \"lambda_assume_policy\" {\n  name = \"lambda-assume-policy\"\n  role = \"${aws_iam_role.lambda_assume_role.id}\"\n\n  policy = \u003C\u003CEOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"lambda:*\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:PassRole\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeSecurityGroups\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeSubnets\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeVpc*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\nEOF\n}\n```\n\n### Create Again For Second Account\n\nNow you have done that, duplicate the process except this time, when creating Account B IAM, use Account A AWS ID in the principal section.  This will signal to AWS that you want Account A to be able to assume this role.  You will now have two IAM roles, both can be used to deploy Lambdas with Account A acting as the primary account.\n\nIf you are using tool such as [Apex](http://apex.run) that manages all your Lambdas, you will need quite open permissions to create, delete and update functions etc.\n\n### CI Server Role\n\n\n```hcl\nresource \"aws_iam_role\" \"build_agent_access\" {\n  name = \"build_agent_access\"\n\n  assume_role_policy = \u003C\u003CEOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Effect\": \"Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}\nEOF\n}\n\nresource \"aws_iam_role_policy\" \"build_agent_access\" {\n  name = \"ci-agent-access-policy\"\n  role = \"${aws_iam_role.build_agent_access.id}\"\n\n  policy = \u003C\u003CEOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"sts:AssumeRole\",\n      \"Resource\": \"arn:aws:iam::000000000001:role/apex_lambda_assume_role\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"sts:AssumeRole\",\n      \"Resource\": \"arn:aws:iam::000000000002:role/apex_lambda_assume_role\"\n    }\n  ]\n}\nEOF\n}\n```\n\nIf you are a cloudformation user, all of the IAM JSON show so far is compatible, just remove the variables and add it to your template.\n\n### Deploy with Apex\n\nNow you have two roles in two AWS accounts, both which can be assumed by the CI build server for deployments.  For Apex, you can deploy easily by using the IAM role option on the command line.\n\n```sh\napex deploy -i arn:aws:iam::000000000002:role/lambda_assume_role\n```\n\nWhat happens in this example is that Apex assumes the role, whether it be A or B, this then hands Apex the permissions to deploy Lambdas, describe VPCs etc.  This scopes all the permissions to just the assumed role, which is typical of how most CI plugins work.  Jenkins and Teamcity plugins that are focused on AWS usually have a box that allows for role based usage for this exact use case.  So whether it be on the command line or using a plugin, the process is the same.\n\n## Conclusion\n\nSetting this up is not that difficult, often the main blockers are permissions cross accounts for specific resources, such as PassRole.  What this process is enables is a tighter focus on security as your roles will be scoped exactly on the permissions you need, which is often counter to some CI setups that use full access.\n\nDon't do that.\n\nFull code used [on Github](https://github.com/DaveBlooman/cross_account_deploys)","src/content/blog/2017-02-07-cross-account-aws-lambda-deployments.md","b2bb2ad76a8fa9a7",{"html":489,"metadata":490},"\u003Cp>I recently talked at the \u003Ca href=\"https://www.meetup.com/Serverless-London/events/236664340/\">Serverless London meetup\u003C/a>, I was asked about how we do cross AWS account deploys with Lambda functions.  You can see the \u003Ca href=\"https://speakerdeck.com/daveblooman/deploying-with-apex\">slides here\u003C/a> and the \u003Ca href=\"https://www.twitch.tv/videos/119142356%C2%AD\">video here\u003C/a>.\u003C/p>\n\u003Cp>This way of working is great for teams that have many accounts for dev, test, stage or prod.  By using multiple AWS accounts you can benefit from perform isolated testing, locked down prod environments and testing CI integration.\u003C/p>\n\u003Cp>Here’s how we do it.\u003C/p>\n\u003Ch3 id=\"accounts-accounts-accounts\">Accounts, Accounts, Accounts\u003C/h3>\n\u003Cp>To start, you want to be running your deployments from an AWS EC2 instance.  This is usually via a CI server like Jenkins, Teamcity or GOCD.  By using a CI server on EC2, you can use the benefits of IAM roles.\u003C/p>\n\u003Cp>This sets up the deployment model, with one account, A, deploying to account A, then use A to deploy to another account, B.  For example.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Account A CI server -> Account A  \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Account A CI server -> Account B  \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>You could also deploy from account A, which could be a tooling account, to other accounts.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Account A CI server -> Account B   \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Account A CI server -> Account C  \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Your account setup will depend on the structure of the IAM policies you will need, but follows the same basic structure.\u003C/p>\n\u003Ch3 id=\"identity-access-management\">Identity Access Management\u003C/h3>\n\u003Cp>The way the IAM works with cross account deploys is that the CI server will assume a new role, one that enables the deployments.  This may be in the same account or another AWS account.\u003C/p>\n\u003Cp>For our example, we will be using the example above, with 2 AWS accounts with account A deploying to account A and account A deploying to account B.\u003C/p>\n\u003Ch4 id=\"policy\">Policy\u003C/h4>\n\u003Cp>This is example templates for Terraform, if you would like the full working example, \u003Ca href=\"https://github.com/DaveBlooman/cross_account_deploys\">visit the Github repo\u003C/a>.\u003C/p>\n\u003Cp>With Terraform, the first step is to create a role with account A, set the principal to be that of account A in the terraform below.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"hcl\">\u003Ccode>\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">resource\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \"aws_iam_role\"\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \"lambda_assume_role\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  name\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"lambda-assume-role\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  path\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"/\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  assume_role_policy\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#F97583\"> &#x3C;&#x3C;POLICY\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  \"Version\": \"2012-10-17\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  \"Statement\": [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Effect\": \"Allow\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Principal\": {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"AWS\": \"arn:aws:iam::000000000001:root\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Action\": \"sts:AssumeRole\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">POLICY\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">resource\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \"aws_iam_role_policy\"\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \"lambda_assume_policy\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  name\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"lambda-assume-policy\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  role\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"\u003C/span>\u003Cspan style=\"color:#F97583\">${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">aws_iam_role\u003C/span>\u003Cspan style=\"color:#F97583\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">lambda_assume_role\u003C/span>\u003Cspan style=\"color:#F97583\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">id\u003C/span>\u003Cspan style=\"color:#F97583\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  policy\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#F97583\"> &#x3C;&#x3C;EOF\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  \"Version\": \"2012-10-17\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  \"Statement\": [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Effect\": \"Allow\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Action\": [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"lambda:*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Resource\": \"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Effect\": \"Allow\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Action\": [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"iam:PassRole\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Resource\": \"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Effect\": \"Allow\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Action\": [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"ec2:DescribeSecurityGroups\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Resource\": \"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Effect\": \"Allow\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Action\": [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"ec2:DescribeSubnets\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Resource\": \"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Effect\": \"Allow\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Action\": [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"ec2:DescribeVpc*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Resource\": \"*\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">EOF\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"create-again-for-second-account\">Create Again For Second Account\u003C/h3>\n\u003Cp>Now you have done that, duplicate the process except this time, when creating Account B IAM, use Account A AWS ID in the principal section.  This will signal to AWS that you want Account A to be able to assume this role.  You will now have two IAM roles, both can be used to deploy Lambdas with Account A acting as the primary account.\u003C/p>\n\u003Cp>If you are using tool such as \u003Ca href=\"http://apex.run\">Apex\u003C/a> that manages all your Lambdas, you will need quite open permissions to create, delete and update functions etc.\u003C/p>\n\u003Ch3 id=\"ci-server-role\">CI Server Role\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"hcl\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">resource\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \"aws_iam_role\"\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \"build_agent_access\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  name\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"build_agent_access\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  assume_role_policy\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#F97583\"> &#x3C;&#x3C;EOF\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  \"Version\": \"2012-10-17\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  \"Statement\": [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Action\": \"sts:AssumeRole\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Principal\": {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"Service\": \"ec2.amazonaws.com\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Effect\": \"Allow\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Sid\": \"\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">EOF\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">resource\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \"aws_iam_role_policy\"\u003C/span>\u003Cspan style=\"color:#79B8FF\"> \"build_agent_access\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  name\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"ci-agent-access-policy\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  role\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"\u003C/span>\u003Cspan style=\"color:#F97583\">${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">aws_iam_role\u003C/span>\u003Cspan style=\"color:#F97583\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">build_agent_access\u003C/span>\u003Cspan style=\"color:#F97583\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">id\u003C/span>\u003Cspan style=\"color:#F97583\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  policy\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#F97583\"> &#x3C;&#x3C;EOF\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  \"Version\": \"2012-10-17\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  \"Statement\": [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Effect\": \"Allow\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Action\": \"sts:AssumeRole\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Resource\": \"arn:aws:iam::000000000001:role/apex_lambda_assume_role\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Effect\": \"Allow\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Action\": \"sts:AssumeRole\",\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"Resource\": \"arn:aws:iam::000000000002:role/apex_lambda_assume_role\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">EOF\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>If you are a cloudformation user, all of the IAM JSON show so far is compatible, just remove the variables and add it to your template.\u003C/p>\n\u003Ch3 id=\"deploy-with-apex\">Deploy with Apex\u003C/h3>\n\u003Cp>Now you have two roles in two AWS accounts, both which can be assumed by the CI build server for deployments.  For Apex, you can deploy easily by using the IAM role option on the command line.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">apex\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> deploy\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -i\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> arn:aws:iam::000000000002:role/lambda_assume_role\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>What happens in this example is that Apex assumes the role, whether it be A or B, this then hands Apex the permissions to deploy Lambdas, describe VPCs etc.  This scopes all the permissions to just the assumed role, which is typical of how most CI plugins work.  Jenkins and Teamcity plugins that are focused on AWS usually have a box that allows for role based usage for this exact use case.  So whether it be on the command line or using a plugin, the process is the same.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Setting this up is not that difficult, often the main blockers are permissions cross accounts for specific resources, such as PassRole.  What this process is enables is a tighter focus on security as your roles will be scoped exactly on the permissions you need, which is often counter to some CI setups that use full access.\u003C/p>\n\u003Cp>Don’t do that.\u003C/p>\n\u003Cp>Full code used \u003Ca href=\"https://github.com/DaveBlooman/cross_account_deploys\">on Github\u003C/a>\u003C/p>",{"headings":491,"localImagePaths":512,"remoteImagePaths":513,"frontmatter":514,"imagePaths":516},[492,495,498,502,505,508,511],{"depth":119,"slug":493,"text":494},"accounts-accounts-accounts","Accounts, Accounts, Accounts",{"depth":119,"slug":496,"text":497},"identity-access-management","Identity Access Management",{"depth":499,"slug":500,"text":501},4,"policy","Policy",{"depth":119,"slug":503,"text":504},"create-again-for-second-account","Create Again For Second Account",{"depth":119,"slug":506,"text":507},"ci-server-role","CI Server Role",{"depth":119,"slug":509,"text":510},"deploy-with-apex","Deploy with Apex",{"depth":42,"slug":55,"text":56},[],[],{"title":482,"description":483,"pubDate":515},"2017-02-07",[],"2016-10-09-cloudformation-updates-what-s-happened-since-you-started-using-terraform",{"id":517,"data":519,"body":523,"filePath":524,"digest":525,"rendered":526},{"title":520,"description":521,"pubDate":522},"Cloudformation Updates, What's Happened Since You Started Using Terraform","CloudFormation, the AWS specific infrastructure as code service, I don't use it anymore....but maybe I should. Maybe you should too.",["Date","2016-10-09T00:00:00.000Z"],"CloudFormation, the AWS specific infrastructure as code service, I don't use it anymore....but maybe I should.  Maybe you should too.\n\n## Common Problems.\n\nA couple of years ago, if you were automating your infrastructure, you were probably using CloudFormation.  AWS looked after the state of your application, handled rollbacks, validated your template, it was the best.  You could build multiple AWS services into 1 JSON template, EC2, ELB, Cloudwatch, Route53 all came together to launch an application.  Sounds great, but it actually was really difficult to manage.  \n\nHardcoding VPC ID's, subnets, Hosted Zone ID's, it was all a bit....non cloud, if that's such a thing.  Many tools were built to take on this challenge, [CFNDSL](https://github.com/stevenjack/cfndsl) and Troposphere were two that BBC used while I worked there.  CFNDSL was so heavily used in our team, a colleague, [Steve Jack](https://twitter.com/stevenjack85), became the maintainer of the project.  We then extended this with another tool for internal use due to the fact we had multiple AWS accounts with multiple environments.  We used a YAML file that was environment specific which could share VPC, Route53, ASG specific variables across multiple stacks, this would have a directory such as :\n\n```sh\n|____int\n| |____Newsbeat.yaml\n|____live\n| |____Newsbeat.yaml\n|____test\n| |____Newsbeat.yaml\n```\n\nwith our stacks, written in CFNDSL Ruby syntax, looking like this.\n\n```sh\n|____cloudfrontdns\n| |____aws\n| | |____route_53\n| | | |____record_set.rb\n|____dns\n| |____aws\n| | |____route_53\n| | | |____record_set.rb\n|____main\n| |____aws\n| | |____auto_scaling\n| | | |____group.rb\n| | | |____launch_config.rb\n| | | |____scaling_policy.rb\n| | |____cloud_watch\n| | | |____alarm.rb\n| | |____cloudfront\n| | | |____distribution.rb\n| | |____ec2\n| | | |____security_group.rb\n| | |____elastic_load_balancing\n| | | |____load_balancer.rb\n| | |____iam\n| | | |____instance_profile.rb\n| | | |____policy.rb\n| | |____route_53\n| | | |____record_set.rb\n| |____template.rb\n```\n\nAs you can see, we were doing a lot to manage CloudFormation, Ruby DSL with 2 sets of tooling in order to create some JSON.  What we needed was a tool that would manage cross stack resources, template out stacks that are identical(except for parameters) and make it easy to write infrastructure, JSON is not as great for the task.\n\nTurns out Hashicorp figured the same and wrote Terraform.\n\n## Terraform\n\nTerraform is a common syntax for multiple \"providers\", a provider could be AWS, Azure etc.  This means that the basic concepts, servers, load balancers, databases, take vendor specific parameters while Terraform manages the glue that binds it all together.  It also means multi cloud setups are using the same language to describe infrastructure.  Terraform is now maturing, but a couple of years ago, it was lacking a lot of the AWS resources needed to make it production viable, but it's come a long way.  \n\n## Infrastructure is a living thing\n\nWhen I started using Terraform, CloudFormation began to look quite primitive.  This was especially clear with the concept of infrastructure as an organism that Terraform embraces, not a collection of CloudFormation stacks that have hardcoded parameters.  \n\nTerraform also allowed for changes to be displayed not just for a single stack, but for changes that impact all your infrastructure.  If you went ahead and tried to delete a VPC that was created in a CloudFormation stack, AWS will happily go head and try to do that for you.  The problem is that 20 other stacks rely on that VPC, so your operation is going to fail.  But you may not know that up front, so you waste time and potentially break infrastructure.  \n\nWith Terraform, you can see how a delete VPC event will impact the whole infrastructure, this feature was one of the catalysts for many to transition to Terraform from CloudFormation.\n\n## Is that JSON?\n\nThis is a stack I created to setup a service, as you can see, lots of weirdness going on here.  This is because we are generating user data, an EC2 feature that lets scripts and commands be run on boot up of an instance.  In this case, a parameter is being used in conjunction with an RDS instance that is also being setup in the same CloudFormation stack.  There is some syntax to get the service port which is being joined with that parameter, as well as a new line, also loads of spaces.  This sucks.\n\n```json\n{\n  \"Fn::Join\": [\n    \"\",\n    [\n      \"        - CATTLE_DB_CATTLE_MYSQL_PORT=\",\n      {\n        \"Fn::GetAtt\": [\n          \"ComponentRDS\",\n          \"Endpoint.Port\"\n        ]\n      },\n      \"\\n\"\n    ]\n  ]\n}\n```\n\nTerraform has the ability to use template files that render user data files using variables that are injected into the template and then out comes a block of user data for use with an instance.  This means at runtime, I can get any part of my infrastructure and use it to build user data.  Further, I can also track changes, meaning if my RDS endpoint changes, my user data for another server will be updated and by extension, the server will be updated.\n\n## Terraform is not a silver bullet\n\nI could write an entire post on why I have a love/hate relationship with Terraform, it's coming soon, but suffice to say, it has issues.  State isn't handled by a service like AWS, so you have to deal with that yourself, S3 for example.  Your state is an actual file, but you can't check it into version control if you create AWS credentials as the credentials will be listed in the state file.  It lacks some CloudFormation features, like rolling updates.  One of the great things about CloudFormation is the ability to update an AMI in a stack and have it roll out that AMI in a controlled way.  In order to do this exactly the same way in Terraform, you need to learn how to do....CloudFormation.  There are ways around this, but the best way is to use CloudFormation from within Terraform to manage rolling updates, which brings us to the point where we ask, haven't AWS realised what we want?\n\n## CloudFormation, it's about time.\n\nI know the CloudFormation team do a great job, the service is good, but it could be better if more effort was made to understand what 2016 infrastructure design is like. In the last few months, AWS have made a big effort to solve most of the hacky solutions people have been using to resolve dependencies in CloudFormation, but also to probably stop people moving to Terraform.\n\nFirstly, you can use YAML.  As someone that only writes stacks in YAML, this is a good move.  In doing this, there are some new features in the AWS CloudFormation syntax.\n\n```yaml\nMappings:\n  RegionMap:\n    us-east-1:\n      32: \"ami-6411e20d\"\n      64: \"ami-7a11e213\"\nResources:\n  myEC2Instance:\n    Type: \"AWS::EC2::Instance\"\n    Properties:\n      ImageId: !FindInMap [ RegionMap, !Ref \"AWS::Region\", 32 ]\n      InstanceType: m1.small\n```\n\nThis new !FindInMap function is nice because you no longer need to use multiple lines, though if you didn't know, the bang(!) is a part of the syntax, rather than a logical not operation.\n\n## Cross Stack Referencing!!\n\nYes, my stacks can know about each other.  This is one of the best things released this year by AWS.\n\nYou can get started by using the outputs functionality.  In the example below, you can also see the new !Sub syntax, great for interpolation.\n\n```yaml\nOutputs:\n  VPCId:\n    Description: VPC ID\n    Value:\n      Ref: VPC\n    Export:\n      Name:\n        !Sub '${AWS::StackName}-VPCID'\n  PublicSubnet:\n    Description: The subnet ID to use for public web servers\n    Value:\n      Ref: PublicSubnet\n    Export:\n      Name:\n        !Sub '{AWS::StackName}-SubnetID'\n  WebServerSecurityGroup:\n    Description: The security group ID to use for public web servers\n    Value:\n      !GetAtt\n        - WebServerSecurityGroup\n        - GroupId\n    Export:\n      Name:\n        !Sub '${AWS::StackName}-SecurityGroupID'\n```\n\nWatch as your YAML linter goes crazy with these syntax tags.  In any case, we can see the stack name being used to create a \"name\" for the variable to be used in another stack.  Interestingly, you can't see the name values in the AWS console.\n\nLet's assume this stack is the first stack you created and it's called CoreStack.  Your second stack will look something like this\n\n```yaml\nAWSTemplateFormatVersion: '2010-09-09'\nResources:\n  AppServerSecurityGroup:\n    Type: 'AWS::EC2::SecurityGroup'\n    Properties:\n      GroupDescription: Enable HTTP ingress\n      VpcId: !ImportValue CoreStack-VPCID\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: '80'\n          ToPort: '80'\n          SourceSecurityGroupId: !ImportValue CoreStack-SecurityGroupID\n\n```\n\nAs you can see, for the life of this stack, there will be a relationship between the two stacks.  The syntax is quite nice too, the shorthand YAML syntax is simple and has the potential to support strings, arrays etc.  There is also a more secure feeling here, in Terraform, you can go ahead and create a reference to something anywhere in your infrastructure.  With these outputs, you are defining what should be accessible by other stacks.  Obviously, if you reference lots of things, you are going to build up a huge list of outputs, but that is more likely on core stacks and not on application/generic stacks.\n\nAlso note that you can use dynamic variables in imports, this uses `Fn::ImportValue`.  For example, importing the value of a security group ID from another stack.\n\n```json\n{\n  \"Fn::ImportValue\": {\n    \"Fn::Sub\": \"${CoreStack}-SecurityGroupID\"\n  }\n}\n```\n\n## Looking good, but that mess from earlier?\n\nCloudFormation doesn't have baggage, it travels light, it wants a complete stack at upload time.  I still feel the CLI tool could implement a file templating feature, but the CloudFormation team have a solution that does make it easier to substitute variables, sort of.\n\n```json\n{\n  \"Fn::Join\": [\n    \"\\n\",\n    [\n      {\n        \"Fn::Sub\": [\n          \"        - CATTLE_DB_CATTLE_MYSQL_PORT=${RDSEndpoint}\",\n          {\n            \"RDSEndpoint\": {\n              \"Fn::GetAtt\": [\n                \"ComponentRDS\",\n                \"Endpoint.Port\"\n              ]\n            }\n          }\n        ]\n      }\n    ]\n  ]\n}\n```\n\nSo we actually have more code than previously, but it is clear what is going on, a variable of RDSEndpoint is going to be added to the string preceding it.  The YAML layout is actually better, but I still feel this is an enhancement enough for shorter substitution.\n\nHere is what a simple reference sub looks like, much nicer.\n\n```json\n{\n  \"Fn::Sub\": \"/opt/aws/bin/CloudFormation-init -v --stack ${AWS::StackName}\"\n}\n```\n\nOK, it's never going to be as good as Terraform, but if you are staying away from cloud init because of the syntax, you should try this new style.\n\n### CloudFormation Plan...I mean Change sets\n\nCloudFormation has seen the power of Terraform plan, the ability to view your changes ahead of time and created change sets.  This is a much much much, much * 1000 feature.  \n\nChange sets work by indicating what you want to change and AWS will actually store that change set for you to apply.  What that means is that you can create a change set, have AWS store it for review, then determine if it is good to execute.  This is a great workflow choice and is great for Pull Requests.\n\nIn case you are wondering what happens if you delete a stack that is being used by another stack, the answer is nothing.  You will get an error message of \"Export CoreStack-VPCID cannot be deleted as it is in use by OtherStack\".  \n\nIf you're using any automation around CloudFormation, use the create changes API, it will probably make you wonder how you did infra before.\n\n## So, I'm going back to CloudFormation, right?\n\nNo.  Not yet.\n\nWhile I have problems with Terraform, it still wins, for now.  CloudFormation still has a long way to go to become the best way to orchestrate AWS, which is quite a thing to say given how short a time Terraform has been around.\n\nI would like to see CloudFormation as a broader tool.  As an example, I decide to change 4 CloudFormation stacks at the same time, now I have 4 change sets and I can't see what the impact is across those 4 stacks from all the changes, only the changes from the stack in the change set.  So yes, reviewing is nicer, but it is a one at a time approach.\n\nThis is where the 'infrastructure as a living organism' comes into the fore.  If you need to replace the kidneys in a human, you don't do 3 surgeries, taking 1 out at a time and then putting 1 back in, you do one surgery.  The idea of stacks is great, but as people begin to have dosens of stacks that are dependant on each other, changes are going to become more difficult.  What might happen is people don't use other stacks because it becomes difficult to scope the changes, particularly for IAM and S3 policy files.  I'm sure the team at AWS see the potential, but it is a concept Terraform got right from very early on, so perhaps CloudFormation needs to become something new, something designed for the post Lambda, Docker, microservice world.","src/content/blog/2016-10-09-cloudformation-updates-what-s-happened-since-you-started-using-terraform.md","4256c3be0e7692ad",{"html":527,"metadata":528},"\u003Cp>CloudFormation, the AWS specific infrastructure as code service, I don’t use it anymore…but maybe I should.  Maybe you should too.\u003C/p>\n\u003Ch2 id=\"common-problems\">Common Problems.\u003C/h2>\n\u003Cp>A couple of years ago, if you were automating your infrastructure, you were probably using CloudFormation.  AWS looked after the state of your application, handled rollbacks, validated your template, it was the best.  You could build multiple AWS services into 1 JSON template, EC2, ELB, Cloudwatch, Route53 all came together to launch an application.  Sounds great, but it actually was really difficult to manage.\u003C/p>\n\u003Cp>Hardcoding VPC ID’s, subnets, Hosted Zone ID’s, it was all a bit…non cloud, if that’s such a thing.  Many tools were built to take on this challenge, \u003Ca href=\"https://github.com/stevenjack/cfndsl\">CFNDSL\u003C/a> and Troposphere were two that BBC used while I worked there.  CFNDSL was so heavily used in our team, a colleague, \u003Ca href=\"https://twitter.com/stevenjack85\">Steve Jack\u003C/a>, became the maintainer of the project.  We then extended this with another tool for internal use due to the fact we had multiple AWS accounts with multiple environments.  We used a YAML file that was environment specific which could share VPC, Route53, ASG specific variables across multiple stacks, this would have a directory such as :\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____int\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____Newsbeat.yaml\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____live\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____Newsbeat.yaml\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____test\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____Newsbeat.yaml\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>with our stacks, written in CFNDSL Ruby syntax, looking like this.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____cloudfrontdns\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____aws\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____route_53\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____record_set.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____dns\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____aws\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____route_53\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____record_set.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____main\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____aws\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____auto_scaling\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____group.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____launch_config.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____scaling_policy.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____cloud_watch\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____alarm.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____cloudfront\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____distribution.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____ec2\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____security_group.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____elastic_load_balancing\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____load_balancer.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____iam\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____instance_profile.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____policy.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____route_53\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____record_set.rb\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____template.rb\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>As you can see, we were doing a lot to manage CloudFormation, Ruby DSL with 2 sets of tooling in order to create some JSON.  What we needed was a tool that would manage cross stack resources, template out stacks that are identical(except for parameters) and make it easy to write infrastructure, JSON is not as great for the task.\u003C/p>\n\u003Cp>Turns out Hashicorp figured the same and wrote Terraform.\u003C/p>\n\u003Ch2 id=\"terraform\">Terraform\u003C/h2>\n\u003Cp>Terraform is a common syntax for multiple “providers”, a provider could be AWS, Azure etc.  This means that the basic concepts, servers, load balancers, databases, take vendor specific parameters while Terraform manages the glue that binds it all together.  It also means multi cloud setups are using the same language to describe infrastructure.  Terraform is now maturing, but a couple of years ago, it was lacking a lot of the AWS resources needed to make it production viable, but it’s come a long way.\u003C/p>\n\u003Ch2 id=\"infrastructure-is-a-living-thing\">Infrastructure is a living thing\u003C/h2>\n\u003Cp>When I started using Terraform, CloudFormation began to look quite primitive.  This was especially clear with the concept of infrastructure as an organism that Terraform embraces, not a collection of CloudFormation stacks that have hardcoded parameters.\u003C/p>\n\u003Cp>Terraform also allowed for changes to be displayed not just for a single stack, but for changes that impact all your infrastructure.  If you went ahead and tried to delete a VPC that was created in a CloudFormation stack, AWS will happily go head and try to do that for you.  The problem is that 20 other stacks rely on that VPC, so your operation is going to fail.  But you may not know that up front, so you waste time and potentially break infrastructure.\u003C/p>\n\u003Cp>With Terraform, you can see how a delete VPC event will impact the whole infrastructure, this feature was one of the catalysts for many to transition to Terraform from CloudFormation.\u003C/p>\n\u003Ch2 id=\"is-that-json\">Is that JSON?\u003C/h2>\n\u003Cp>This is a stack I created to setup a service, as you can see, lots of weirdness going on here.  This is because we are generating user data, an EC2 feature that lets scripts and commands be run on boot up of an instance.  In this case, a parameter is being used in conjunction with an RDS instance that is also being setup in the same CloudFormation stack.  There is some syntax to get the service port which is being joined with that parameter, as well as a new line, also loads of spaces.  This sucks.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Fn::Join\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    \"\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"        - CATTLE_DB_CATTLE_MYSQL_PORT=\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"Fn::GetAtt\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">          \"ComponentRDS\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">          \"Endpoint.Port\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\n\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Terraform has the ability to use template files that render user data files using variables that are injected into the template and then out comes a block of user data for use with an instance.  This means at runtime, I can get any part of my infrastructure and use it to build user data.  Further, I can also track changes, meaning if my RDS endpoint changes, my user data for another server will be updated and by extension, the server will be updated.\u003C/p>\n\u003Ch2 id=\"terraform-is-not-a-silver-bullet\">Terraform is not a silver bullet\u003C/h2>\n\u003Cp>I could write an entire post on why I have a love/hate relationship with Terraform, it’s coming soon, but suffice to say, it has issues.  State isn’t handled by a service like AWS, so you have to deal with that yourself, S3 for example.  Your state is an actual file, but you can’t check it into version control if you create AWS credentials as the credentials will be listed in the state file.  It lacks some CloudFormation features, like rolling updates.  One of the great things about CloudFormation is the ability to update an AMI in a stack and have it roll out that AMI in a controlled way.  In order to do this exactly the same way in Terraform, you need to learn how to do…CloudFormation.  There are ways around this, but the best way is to use CloudFormation from within Terraform to manage rolling updates, which brings us to the point where we ask, haven’t AWS realised what we want?\u003C/p>\n\u003Ch2 id=\"cloudformation-its-about-time\">CloudFormation, it’s about time.\u003C/h2>\n\u003Cp>I know the CloudFormation team do a great job, the service is good, but it could be better if more effort was made to understand what 2016 infrastructure design is like. In the last few months, AWS have made a big effort to solve most of the hacky solutions people have been using to resolve dependencies in CloudFormation, but also to probably stop people moving to Terraform.\u003C/p>\n\u003Cp>Firstly, you can use YAML.  As someone that only writes stacks in YAML, this is a good move.  In doing this, there are some new features in the AWS CloudFormation syntax.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">Mappings\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  RegionMap\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    us-east-1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      32\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"ami-6411e20d\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      64\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"ami-7a11e213\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">Resources\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  myEC2Instance\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Type\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"AWS::EC2::Instance\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Properties\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      ImageId\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#F97583\">!FindInMap\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [ \u003C/span>\u003Cspan style=\"color:#9ECBFF\">RegionMap\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#F97583\">!Ref\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"AWS::Region\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">32\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      InstanceType\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">m1.small\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This new !FindInMap function is nice because you no longer need to use multiple lines, though if you didn’t know, the bang(!) is a part of the syntax, rather than a logical not operation.\u003C/p>\n\u003Ch2 id=\"cross-stack-referencing\">Cross Stack Referencing!!\u003C/h2>\n\u003Cp>Yes, my stacks can know about each other.  This is one of the best things released this year by AWS.\u003C/p>\n\u003Cp>You can get started by using the outputs functionality.  In the example below, you can also see the new !Sub syntax, great for interpolation.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">Outputs\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  VPCId\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Description\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">VPC ID\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Value\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      Ref\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">VPC\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Export\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      Name\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        !Sub\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> '${AWS::StackName}-VPCID'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  PublicSubnet\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Description\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">The subnet ID to use for public web servers\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Value\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      Ref\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">PublicSubnet\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Export\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      Name\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        !Sub\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> '{AWS::StackName}-SubnetID'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  WebServerSecurityGroup\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Description\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">The security group ID to use for public web servers\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Value\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">      !GetAtt\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        - \u003C/span>\u003Cspan style=\"color:#9ECBFF\">WebServerSecurityGroup\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        - \u003C/span>\u003Cspan style=\"color:#9ECBFF\">GroupId\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Export\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      Name\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        !Sub\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> '${AWS::StackName}-SecurityGroupID'\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Watch as your YAML linter goes crazy with these syntax tags.  In any case, we can see the stack name being used to create a “name” for the variable to be used in another stack.  Interestingly, you can’t see the name values in the AWS console.\u003C/p>\n\u003Cp>Let’s assume this stack is the first stack you created and it’s called CoreStack.  Your second stack will look something like this\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"yaml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">AWSTemplateFormatVersion\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">'2010-09-09'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">Resources\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">  AppServerSecurityGroup\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Type\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">'AWS::EC2::SecurityGroup'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">    Properties\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      GroupDescription\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">Enable HTTP ingress\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      VpcId\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#F97583\">!ImportValue\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> CoreStack-VPCID\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">      SecurityGroupIngress\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        - \u003C/span>\u003Cspan style=\"color:#85E89D\">IpProtocol\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">tcp\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">          FromPort\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">'80'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">          ToPort\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">'80'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#85E89D\">          SourceSecurityGroupId\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#F97583\">!ImportValue\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> CoreStack-SecurityGroupID\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>As you can see, for the life of this stack, there will be a relationship between the two stacks.  The syntax is quite nice too, the shorthand YAML syntax is simple and has the potential to support strings, arrays etc.  There is also a more secure feeling here, in Terraform, you can go ahead and create a reference to something anywhere in your infrastructure.  With these outputs, you are defining what should be accessible by other stacks.  Obviously, if you reference lots of things, you are going to build up a huge list of outputs, but that is more likely on core stacks and not on application/generic stacks.\u003C/p>\n\u003Cp>Also note that you can use dynamic variables in imports, this uses \u003Ccode>Fn::ImportValue\u003C/code>.  For example, importing the value of a security group ID from another stack.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Fn::ImportValue\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"Fn::Sub\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"${CoreStack}-SecurityGroupID\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"looking-good-but-that-mess-from-earlier\">Looking good, but that mess from earlier?\u003C/h2>\n\u003Cp>CloudFormation doesn’t have baggage, it travels light, it wants a complete stack at upload time.  I still feel the CLI tool could implement a file templating feature, but the CloudFormation team have a solution that does make it easier to substitute variables, sort of.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Fn::Join\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">    \"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\n\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"Fn::Sub\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">          \"        - CATTLE_DB_CATTLE_MYSQL_PORT=${RDSEndpoint}\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">          {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">            \"RDSEndpoint\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">              \"Fn::GetAtt\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">                \"ComponentRDS\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">                \"Endpoint.Port\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">              ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">          }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>So we actually have more code than previously, but it is clear what is going on, a variable of RDSEndpoint is going to be added to the string preceding it.  The YAML layout is actually better, but I still feel this is an enhancement enough for shorter substitution.\u003C/p>\n\u003Cp>Here is what a simple reference sub looks like, much nicer.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"Fn::Sub\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"/opt/aws/bin/CloudFormation-init -v --stack ${AWS::StackName}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>OK, it’s never going to be as good as Terraform, but if you are staying away from cloud init because of the syntax, you should try this new style.\u003C/p>\n\u003Ch3 id=\"cloudformation-plani-mean-change-sets\">CloudFormation Plan…I mean Change sets\u003C/h3>\n\u003Cp>CloudFormation has seen the power of Terraform plan, the ability to view your changes ahead of time and created change sets.  This is a much much much, much * 1000 feature.\u003C/p>\n\u003Cp>Change sets work by indicating what you want to change and AWS will actually store that change set for you to apply.  What that means is that you can create a change set, have AWS store it for review, then determine if it is good to execute.  This is a great workflow choice and is great for Pull Requests.\u003C/p>\n\u003Cp>In case you are wondering what happens if you delete a stack that is being used by another stack, the answer is nothing.  You will get an error message of “Export CoreStack-VPCID cannot be deleted as it is in use by OtherStack”.\u003C/p>\n\u003Cp>If you’re using any automation around CloudFormation, use the create changes API, it will probably make you wonder how you did infra before.\u003C/p>\n\u003Ch2 id=\"so-im-going-back-to-cloudformation-right\">So, I’m going back to CloudFormation, right?\u003C/h2>\n\u003Cp>No.  Not yet.\u003C/p>\n\u003Cp>While I have problems with Terraform, it still wins, for now.  CloudFormation still has a long way to go to become the best way to orchestrate AWS, which is quite a thing to say given how short a time Terraform has been around.\u003C/p>\n\u003Cp>I would like to see CloudFormation as a broader tool.  As an example, I decide to change 4 CloudFormation stacks at the same time, now I have 4 change sets and I can’t see what the impact is across those 4 stacks from all the changes, only the changes from the stack in the change set.  So yes, reviewing is nicer, but it is a one at a time approach.\u003C/p>\n\u003Cp>This is where the ‘infrastructure as a living organism’ comes into the fore.  If you need to replace the kidneys in a human, you don’t do 3 surgeries, taking 1 out at a time and then putting 1 back in, you do one surgery.  The idea of stacks is great, but as people begin to have dosens of stacks that are dependant on each other, changes are going to become more difficult.  What might happen is people don’t use other stacks because it becomes difficult to scope the changes, particularly for IAM and S3 policy files.  I’m sure the team at AWS see the potential, but it is a concept Terraform got right from very early on, so perhaps CloudFormation needs to become something new, something designed for the post Lambda, Docker, microservice world.\u003C/p>",{"headings":529,"localImagePaths":560,"remoteImagePaths":561,"frontmatter":562,"imagePaths":564},[530,533,536,539,542,545,548,551,554,557],{"depth":42,"slug":531,"text":532},"common-problems","Common Problems.",{"depth":42,"slug":534,"text":535},"terraform","Terraform",{"depth":42,"slug":537,"text":538},"infrastructure-is-a-living-thing","Infrastructure is a living thing",{"depth":42,"slug":540,"text":541},"is-that-json","Is that JSON?",{"depth":42,"slug":543,"text":544},"terraform-is-not-a-silver-bullet","Terraform is not a silver bullet",{"depth":42,"slug":546,"text":547},"cloudformation-its-about-time","CloudFormation, it’s about time.",{"depth":42,"slug":549,"text":550},"cross-stack-referencing","Cross Stack Referencing!!",{"depth":42,"slug":552,"text":553},"looking-good-but-that-mess-from-earlier","Looking good, but that mess from earlier?",{"depth":119,"slug":555,"text":556},"cloudformation-plani-mean-change-sets","CloudFormation Plan…I mean Change sets",{"depth":42,"slug":558,"text":559},"so-im-going-back-to-cloudformation-right","So, I’m going back to CloudFormation, right?",[],[],{"title":520,"description":521,"pubDate":563},"2016-10-09",[],"2017-09-22-terraform-in-production-lessons-learned",{"id":565,"data":567,"body":571,"filePath":572,"digest":573,"rendered":574},{"title":568,"description":569,"pubDate":570},"Terraform In Production, Lessons Learned","Terraform has a lot of power, but often even heavy users of terraform miss some of the simple things that can make managing infrastructure easier than vendor...",["Date","2017-09-22T00:00:00.000Z"],"Terraform has a lot of power, but often even heavy users of terraform miss some of the simple things that can make managing infrastructure easier than vendor specific configuration like Cloudformation.\n\n### Terraform modules\n\nWe jump straight into modules, the thing that should make Terraform work for you, reducing duplication in your code.  I use the below structure for applications.  Write your Terraform in the same repo as your code and use remote state for anything you need to import in, VPCs, subnets etc.  Unless you only run a single environment, within your Terraform directory, you will probably have a set of modules.  These modules should only include infrastructure that is used for this application, you wont be creating VPC's here.\n\nThis split is between application infrastructure and core infrastructure, core infrastructure often being shared between many services/applications\n\n### The Structure\n\nThis is a typical application, a notifications service in this case.\n\n```bash\n|____dev\n| |____security-groups-rules.tf\n| |____main.tf\n| |____outputs.tf\n| |____remotes.tf\n| |____variables.tf\n|____modules\n| |____cms\n| | |____alarms.tf\n| | |____api-gateway.tf\n| | |____api.tf\n| | |____dynamo.tf\n| | |____s3.tf\n| | |____outputs.tf\n| | |____variables.tf\n| |____notifications\n| | |____alarms.tf\n| | |____variables.tf\n| | |____outputs.tf\n|____prod\n| |____security-groups-rules.tf\n| |____main.tf\n| |____outputs.tf\n| |____remotes.tf\n| |____variables.tf\n```\nAs you can see, there are some common files.  \n\n - remotes\n - variables\n - outputs\n\nThese are in every module and environment, with CMS & notifications being the modules, dev & prod being the environments.  Remotes are used for accessing core infrastructure and other applications.  An example is the notifications service needs to talk to a web service somewhere, that needs a security group ID to be passed so an inbound rule can be added.  \n\nVariables are used mostly for things like database passwords and region information with outputs being the mechanism that allows for each service to share its resource properties.  Variables will often set defaults too, meaning that modules don't require a large amount of variables to be handed to them.  \n\nModule outputs bubble up to the environment outputs and then are accessible from other environments.\n\nOne optional thing I shown is removing the security groups from the module, this is usually because you may want to give different kinds of access to dev than prod, CI server, different SSH access etc.  \n\n### The Module Block\n```bash\nmodule \"cms\" {\n  source                    = \"../modules/cms\"\n  account_id                = \"${var.account_id\"\n  vpc_id                    = \"${data.terraform_remote_state.core_eu_west.vpc_id}\"\n  environment               = \"${var.environment}\"\n  front_end_instance_id     = \"${data.terraform_remote_state.core_eu_west.instance_id}\"\n  remote_access_cidr_blocks = \"${jsonencode(var.remote_access_cidr_blocks)}\"\n  bucket_resource_name      = \"hello-world\"\n  instance_security_group   = \"${module.cms_instance.sg_id}\"\n  instance_role_id          = \"${module.cms_instance.instance_role_id}\"\n  describe_tags_policy_arn  = \"${data.terraform_remote_state.core_eu_west.describe_tags_policy_arn}\"\n  services_front_end_elb_id = \"${data.terraform_remote_state.core_eu_west.front_end_elb_id}\"\n}\n```\nAll modules are included from the main.tf file in an environment, that file should only have module imports in.  Everything in an environment directory that isn't a module should be named well and be there for a reason.  \n\nThis module include use the file path source, this is because it is a private module, only public github modules can be used with Terraform right now, so consider that.  As you can see from our module, the only thing that is not interpolated is the bucket name, everything else either comes from remote state, another module or a variable.  The variables in this case allow for you to use this module in any account and any region assuming the remote state/modules allows for it.\n\nYou may be asking, is this useful, interpolate everything?  If you think about say, account ID, it isn't going to change in your environment, so why write it down a bunch of times.  With variables, enforcing there usage actually means using a standard set of defaults.  Think about ASG pause time or route 53 TTL, having a team that agrees on what the default is and then ignoring that and writing a hard coded value in anyway is an easy path to problems.  \n\nUsing variables in this manner is sort of the same approach for programming languages, if you look at Go, the HTTP package contains a bunch of constants, [https://golang.org/pkg/net/http/#pkg-constants](https://golang.org/pkg/net/http/#pkg-constants).  Instead of writing the value `500`, you would write `http.StatusInternalServerError`.  \n\n### The Count Trick?\n\nYou may see some code like, so what is this?\n\n```bash\nresource \"aws_s3_bucket_policy\" \"foo_bucket\" {\n  count  = \"${var.use_policy ? 1 : 0}\"\n  bucket = \"${aws_s3_bucket.foo_bucket.id}\"\n  policy = \"${file(\"${path.module}/files/${var.environment_abbr}-policy.json\")}\"\n}\n```\nLets say that you have some setup that requires an S3 policy to be attached only when in prod, something that prevents developers from doing something that shouldn't.  In this instance the count function is a special function that if it evaluates to 0, will not create the resource.  In this case, a variable of `use_policy` that is `false` will not provision this infrastructure.  This is actually wrapped in a ternary, so looks like some Ruby in the middle of my Terraform, but is actually a really neat way of conditionally adding infrastructure.  \n\nI call it a trick because it goes against what i mentioned earlier about infra in modules being the same.\n\n### Template files are great for bootstrapping\n\nI want to create an elastic IP and attach it to the server, but I want the server to be in an autoscaling group so I can use health checks to ensure there is always a server in service.  \n\nYou can't do that with AWS directly, you will have to attach an EIP to the instance using the API.  If we install the AWS CLI, we can attach using user data, but if the EIP changes when we run Terraform, we want that change reflected, so hard coding is not an option.  \n\nTemplate files allow us to take Terraform resources and write them to files, then add those files to AWS user data for execution on the server.  This looks something like this\n\n```bash\nresource \"aws_eip\" \"bastion\" {\n  vpc = true\n}\n\nresource \"aws_launch_configuration\" \"bastion\" {\n  name_prefix = \"bastion-\"\n\n  image_id                    = \"${var.ami}\"\n  instance_type               = \"t2.small\"\n  associate_public_ip_address = true\n  key_name                    = \"${var.key_pair}\"\n\n  iam_instance_profile = \"${aws_iam_instance_profile.bastion.id}\"\n  user_data            = \"${data.template_file.eip.rendered}\"\n  security_groups      = [\"${aws_security_group.bastion.id}\"]\n}\n\ndata \"template_file\" \"eip\" {\n  template = \"${file(\"${path.module}/user_data.tpl\")}\"\n\n  vars {\n    allocation_id = \"${aws_eip.bastion.id}\"\n    region        = \"${var.region}\"\n  }\n}\n```\nOur template file looks like\n\n```bash\n#!/bin/bash\n\n# Associate Bastion Elastic IP managed by Terraform\nALLOCATION_ID=\"${allocation_id}\"\nINSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\nREGION=\"${region}\"\n\naws ec2 associate-address --region $REGION --instance-id $INSTANCE_ID --allocation-id $ALLOCATION_ID --allow-reassociation\n```\n### Data Types\n\nLets say you want to apply tags to a load of resources, billing tags for example.  You might want to do\n\ndept      = Customer Service  \ncost-code = 12345\n\nIf you are working with modules, this might become very difficult to work with as different teams will have different tags.  Using a terraform map, otherwise known as a directory, will let you do this.\n\n```bash\ntags = {\n  dept      = Customer Service\n  cost-code = 12345  \n}\n```\nResources will state if they take a map,\n\n```bash\ntags = \"${merge(var.tags, map(\"Name\", format(\"%s\", var.name)))}\"\n```","src/content/blog/2017-09-22-terraform-in-production-lessons-learned.md","1664fcdc1b19f182",{"html":575,"metadata":576},"\u003Cp>Terraform has a lot of power, but often even heavy users of terraform miss some of the simple things that can make managing infrastructure easier than vendor specific configuration like Cloudformation.\u003C/p>\n\u003Ch3 id=\"terraform-modules\">Terraform modules\u003C/h3>\n\u003Cp>We jump straight into modules, the thing that should make Terraform work for you, reducing duplication in your code.  I use the below structure for applications.  Write your Terraform in the same repo as your code and use remote state for anything you need to import in, VPCs, subnets etc.  Unless you only run a single environment, within your Terraform directory, you will probably have a set of modules.  These modules should only include infrastructure that is used for this application, you wont be creating VPC’s here.\u003C/p>\n\u003Cp>This split is between application infrastructure and core infrastructure, core infrastructure often being shared between many services/applications\u003C/p>\n\u003Ch3 id=\"the-structure\">The Structure\u003C/h3>\n\u003Cp>This is a typical application, a notifications service in this case.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____dev\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____security-groups-rules.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____main.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____outputs.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____remotes.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____variables.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____modules\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____cms\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____alarms.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____api-gateway.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____api.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____dynamo.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____s3.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____outputs.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____variables.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____notifications\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____alarms.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____variables.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____outputs.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#B392F0\">____prod\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____security-groups-rules.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____main.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____outputs.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____remotes.tf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#F97583\"> |\u003C/span>\u003Cspan style=\"color:#B392F0\">____variables.tf\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>As you can see, there are some common files.\u003C/p>\n\u003Cul>\n\u003Cli>remotes\u003C/li>\n\u003Cli>variables\u003C/li>\n\u003Cli>outputs\u003C/li>\n\u003C/ul>\n\u003Cp>These are in every module and environment, with CMS &#x26; notifications being the modules, dev &#x26; prod being the environments.  Remotes are used for accessing core infrastructure and other applications.  An example is the notifications service needs to talk to a web service somewhere, that needs a security group ID to be passed so an inbound rule can be added.\u003C/p>\n\u003Cp>Variables are used mostly for things like database passwords and region information with outputs being the mechanism that allows for each service to share its resource properties.  Variables will often set defaults too, meaning that modules don’t require a large amount of variables to be handed to them.\u003C/p>\n\u003Cp>Module outputs bubble up to the environment outputs and then are accessible from other environments.\u003C/p>\n\u003Cp>One optional thing I shown is removing the security groups from the module, this is usually because you may want to give different kinds of access to dev than prod, CI server, different SSH access etc.\u003C/p>\n\u003Ch3 id=\"the-module-block\">The Module Block\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">module\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"cms\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  source\u003C/span>\u003Cspan style=\"color:#9ECBFF\">                    =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"../modules/cms\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  account_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">                =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">account_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  vpc_id                    = \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">data\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">terraform_remote_state\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">core_eu_west\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">vpc_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  environment               = \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">environment\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  front_end_instance_id     = \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">data\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">terraform_remote_state\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">core_eu_west\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">instance_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  remote_access_cidr_blocks = \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">jsonencode\u003C/span>\u003Cspan style=\"color:#9ECBFF\">(\u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">remote_access_cidr_blocks\u003C/span>\u003Cspan style=\"color:#9ECBFF\">)}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  bucket_resource_name      = \"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">hello-world\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  instance_security_group   = \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">module\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">cms_instance\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">sg_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  instance_role_id          = \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">module\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">cms_instance\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">instance_role_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  describe_tags_policy_arn  = \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">data\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">terraform_remote_state\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">core_eu_west\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">describe_tags_policy_arn\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">  services_front_end_elb_id = \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">data\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">terraform_remote_state\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">core_eu_west\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">front_end_elb_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>All modules are included from the main.tf file in an environment, that file should only have module imports in.  Everything in an environment directory that isn’t a module should be named well and be there for a reason.\u003C/p>\n\u003Cp>This module include use the file path source, this is because it is a private module, only public github modules can be used with Terraform right now, so consider that.  As you can see from our module, the only thing that is not interpolated is the bucket name, everything else either comes from remote state, another module or a variable.  The variables in this case allow for you to use this module in any account and any region assuming the remote state/modules allows for it.\u003C/p>\n\u003Cp>You may be asking, is this useful, interpolate everything?  If you think about say, account ID, it isn’t going to change in your environment, so why write it down a bunch of times.  With variables, enforcing there usage actually means using a standard set of defaults.  Think about ASG pause time or route 53 TTL, having a team that agrees on what the default is and then ignoring that and writing a hard coded value in anyway is an easy path to problems.\u003C/p>\n\u003Cp>Using variables in this manner is sort of the same approach for programming languages, if you look at Go, the HTTP package contains a bunch of constants, \u003Ca href=\"https://golang.org/pkg/net/http/#pkg-constants\">https://golang.org/pkg/net/http/#pkg-constants\u003C/a>.  Instead of writing the value \u003Ccode>500\u003C/code>, you would write \u003Ccode>http.StatusInternalServerError\u003C/code>.\u003C/p>\n\u003Ch3 id=\"the-count-trick\">The Count Trick?\u003C/h3>\n\u003Cp>You may see some code like, so what is this?\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">resource\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"aws_s3_bucket_policy\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"foo_bucket\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  count\u003C/span>\u003Cspan style=\"color:#9ECBFF\">  =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">use_policy\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> ? \u003C/span>\u003Cspan style=\"color:#E1E4E8\">1\u003C/span>\u003Cspan style=\"color:#F97583\"> :\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> 0\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  bucket\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">aws_s3_bucket\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">foo_bucket\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  policy\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">file\u003C/span>\u003Cspan style=\"color:#9ECBFF\">(\"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">path\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">module\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}/files/${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">environment_abbr\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}-policy.json\")}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Lets say that you have some setup that requires an S3 policy to be attached only when in prod, something that prevents developers from doing something that shouldn’t.  In this instance the count function is a special function that if it evaluates to 0, will not create the resource.  In this case, a variable of \u003Ccode>use_policy\u003C/code> that is \u003Ccode>false\u003C/code> will not provision this infrastructure.  This is actually wrapped in a ternary, so looks like some Ruby in the middle of my Terraform, but is actually a really neat way of conditionally adding infrastructure.\u003C/p>\n\u003Cp>I call it a trick because it goes against what i mentioned earlier about infra in modules being the same.\u003C/p>\n\u003Ch3 id=\"template-files-are-great-for-bootstrapping\">Template files are great for bootstrapping\u003C/h3>\n\u003Cp>I want to create an elastic IP and attach it to the server, but I want the server to be in an autoscaling group so I can use health checks to ensure there is always a server in service.\u003C/p>\n\u003Cp>You can’t do that with AWS directly, you will have to attach an EIP to the instance using the API.  If we install the AWS CLI, we can attach using user data, but if the EIP changes when we run Terraform, we want that change reflected, so hard coding is not an option.\u003C/p>\n\u003Cp>Template files allow us to take Terraform resources and write them to files, then add those files to AWS user data for execution on the server.  This looks something like this\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">resource\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"aws_eip\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"bastion\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  vpc\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> true\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">resource\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"aws_launch_configuration\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"bastion\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  name_prefix\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"bastion-\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  image_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">                    =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">ami\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  instance_type\u003C/span>\u003Cspan style=\"color:#9ECBFF\">               =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"t2.small\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  associate_public_ip_address\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> true\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  key_name\u003C/span>\u003Cspan style=\"color:#9ECBFF\">                    =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">key_pair\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  iam_instance_profile\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">aws_iam_instance_profile\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">bastion\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  user_data\u003C/span>\u003Cspan style=\"color:#9ECBFF\">            =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">data\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">template_file\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">eip\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">rendered\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  security_groups\u003C/span>\u003Cspan style=\"color:#9ECBFF\">      =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">aws_security_group\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">bastion\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">data\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"template_file\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"eip\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  template\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">file\u003C/span>\u003Cspan style=\"color:#9ECBFF\">(\"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">path\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">module\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}/user_data.tpl\")}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  vars\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">    allocation_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">aws_eip\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">bastion\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">    region\u003C/span>\u003Cspan style=\"color:#9ECBFF\">        =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">region\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Our template file looks like\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">#!/bin/bash\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Associate Bastion Elastic IP managed by Terraform\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">ALLOCATION_ID\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">allocation_id\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">INSTANCE_ID\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">$(\u003C/span>\u003Cspan style=\"color:#B392F0\">curl\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -s\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> http://169.254.169.254/latest/meta-data/instance-id\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">REGION\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">region\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">aws\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> ec2\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> associate-address\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --region\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $REGION \u003C/span>\u003Cspan style=\"color:#79B8FF\">--instance-id\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $INSTANCE_ID \u003C/span>\u003Cspan style=\"color:#79B8FF\">--allocation-id\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $ALLOCATION_ID \u003C/span>\u003Cspan style=\"color:#79B8FF\">--allow-reassociation\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"data-types\">Data Types\u003C/h3>\n\u003Cp>Lets say you want to apply tags to a load of resources, billing tags for example.  You might want to do\u003C/p>\n\u003Cp>dept      = Customer Service\u003Cbr>\ncost-code = 12345\u003C/p>\n\u003Cp>If you are working with modules, this might become very difficult to work with as different teams will have different tags.  Using a terraform map, otherwise known as a directory, will let you do this.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">tags\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  dept\u003C/span>\u003Cspan style=\"color:#9ECBFF\">      =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Customer\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> Service\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  cost-code\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 12345\u003C/span>\u003Cspan style=\"color:#E1E4E8\">  \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Resources will state if they take a map,\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">tags\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">merge\u003C/span>\u003Cspan style=\"color:#9ECBFF\">(\u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">tags\u003C/span>\u003Cspan style=\"color:#9ECBFF\">, \u003C/span>\u003Cspan style=\"color:#E1E4E8\">map\u003C/span>\u003Cspan style=\"color:#9ECBFF\">(\"Name\", \u003C/span>\u003Cspan style=\"color:#E1E4E8\">format\u003C/span>\u003Cspan style=\"color:#9ECBFF\">(\"%s\", \u003C/span>\u003Cspan style=\"color:#E1E4E8\">var\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">name\u003C/span>\u003Cspan style=\"color:#9ECBFF\">)))}\"\u003C/span>\u003C/span>\u003C/code>\u003C/pre>",{"headings":577,"localImagePaths":596,"remoteImagePaths":597,"frontmatter":598,"imagePaths":600},[578,581,584,587,590,593],{"depth":119,"slug":579,"text":580},"terraform-modules","Terraform modules",{"depth":119,"slug":582,"text":583},"the-structure","The Structure",{"depth":119,"slug":585,"text":586},"the-module-block","The Module Block",{"depth":119,"slug":588,"text":589},"the-count-trick","The Count Trick?",{"depth":119,"slug":591,"text":592},"template-files-are-great-for-bootstrapping","Template files are great for bootstrapping",{"depth":119,"slug":594,"text":595},"data-types","Data Types",[],[],{"title":568,"description":569,"pubDate":599},"2017-09-22",[],"2017-09-10-deploying-to-a-vps-without-the-hassle",{"id":601,"data":603,"body":607,"filePath":608,"digest":609,"rendered":610},{"title":604,"description":605,"pubDate":606},"Deploying To A VPS Without The Hassle","It's a question that gets asked with quite a lot of answers, Docker, Ansible, CodeDeploy, Chef, the list goes on. The idea should be simple, taking code from...",["Date","2017-09-10T00:00:00.000Z"],"It's a question that gets asked with quite a lot of answers, Docker, Ansible, CodeDeploy, Chef, the list goes on.  The idea should be simple, taking code from a VCS and running it on a server, deploying from a laptop or CI, it should be straight forward.  With the advent of Serverless, deploying code quickly has become the norm, the feedback loop is fast and the entire process has moved into the developers realm.\n\nA while ago, AWS released a service called Lightsail, basically a $5 a month VPS.  The service came along with a few prepackaged services light Wordpress, so new AWS users could come to AWS without having to learn much about AWS or setting up a server.  This is great up until the point you want to do the main thing you do with a VPS, deploy some code.\n\nMy main annoyance is that tutorials using a VPS usually focus on server side work, git cloning, install a bunch of development software, adding more credentials etc.  In reality, you don't need all that if you want to deploy a few lines of Ruby or a simple Node app.  \n\nMy aim with Fasten was to solve that problem with simple Unix commands, SSH and SCP.\n\n### What are we currently doing\n\nIn a development feedback loop, you are probably going to be adding dependencies, files, folders etc, so all that needs to also exist on the server.  So we need to be able to have a blank server, copy some code to it, install the dependencies, then run the application.  Then we need to repeat.  \n\nThis means OS specific requirements, such as languages and C libraries that are needed for normal Ruby/Node/Go apps.  That is handled in the installation phase of Fasten, by detecting what languages you are running and installing the runtimes.\n\nApplication specific updates, such as adding a new npm package since the last time you deployed means running `npm install` on every deploy.  If you think about all the manual steps you would normally run here, imagine having to debug live on a remote server, it can be tedious.\n\nFinally, we have process management, SystemD, init.d, which do you use, how do they work etc, this is becoming a devops challenge very quickly.  Once we want to deploy our code, we would have to kill the running application, then start it, so a bit of `ps aux | grep` commands.  \n\n### What Fasten does\n\nFasten is a command line tool that uses a simple Yaml file to manage the deployment, process, installation and logs of your application without any remote code running on the server.  It runs everything over SCP and SSH and ensures you don't have to every SSH onto your server.  \n\nStart by running `fasten init`, this will walk you through general setup.  Once you have the setup sorted, run `fasten install`, this will update the OS and install all the language runtimes.  Finally, deploy your code by running `fasten deploy`.  From now on, you will only have to run fasten deploy.\n\nBelow is an example of deploying to a server, be aware, the nokogiri gem is very slow to install, but it does complete.\n\n\u003Cimg src=\"/images/fasten-deploy.gif\" class=\"img-responsive\" alt=\"fasten\">\n\u003Ca href=\"/images/fasten-deploy.gif\" alt=\"fasten\">Link to image\u003C/a>\n\n### Try it out\nIf you want to given some feedback on Fasten, you can get the code here [https://github.com/DaveBlooman/fasten](https://github.com/DaveBlooman/fasten)","src/content/blog/2017-09-10-deploying-to-a-vps-without-the-hassle.md","5c5637846b4eac3b",{"html":611,"metadata":612},"\u003Cp>It’s a question that gets asked with quite a lot of answers, Docker, Ansible, CodeDeploy, Chef, the list goes on.  The idea should be simple, taking code from a VCS and running it on a server, deploying from a laptop or CI, it should be straight forward.  With the advent of Serverless, deploying code quickly has become the norm, the feedback loop is fast and the entire process has moved into the developers realm.\u003C/p>\n\u003Cp>A while ago, AWS released a service called Lightsail, basically a $5 a month VPS.  The service came along with a few prepackaged services light Wordpress, so new AWS users could come to AWS without having to learn much about AWS or setting up a server.  This is great up until the point you want to do the main thing you do with a VPS, deploy some code.\u003C/p>\n\u003Cp>My main annoyance is that tutorials using a VPS usually focus on server side work, git cloning, install a bunch of development software, adding more credentials etc.  In reality, you don’t need all that if you want to deploy a few lines of Ruby or a simple Node app.\u003C/p>\n\u003Cp>My aim with Fasten was to solve that problem with simple Unix commands, SSH and SCP.\u003C/p>\n\u003Ch3 id=\"what-are-we-currently-doing\">What are we currently doing\u003C/h3>\n\u003Cp>In a development feedback loop, you are probably going to be adding dependencies, files, folders etc, so all that needs to also exist on the server.  So we need to be able to have a blank server, copy some code to it, install the dependencies, then run the application.  Then we need to repeat.\u003C/p>\n\u003Cp>This means OS specific requirements, such as languages and C libraries that are needed for normal Ruby/Node/Go apps.  That is handled in the installation phase of Fasten, by detecting what languages you are running and installing the runtimes.\u003C/p>\n\u003Cp>Application specific updates, such as adding a new npm package since the last time you deployed means running \u003Ccode>npm install\u003C/code> on every deploy.  If you think about all the manual steps you would normally run here, imagine having to debug live on a remote server, it can be tedious.\u003C/p>\n\u003Cp>Finally, we have process management, SystemD, init.d, which do you use, how do they work etc, this is becoming a devops challenge very quickly.  Once we want to deploy our code, we would have to kill the running application, then start it, so a bit of \u003Ccode>ps aux | grep\u003C/code> commands.\u003C/p>\n\u003Ch3 id=\"what-fasten-does\">What Fasten does\u003C/h3>\n\u003Cp>Fasten is a command line tool that uses a simple Yaml file to manage the deployment, process, installation and logs of your application without any remote code running on the server.  It runs everything over SCP and SSH and ensures you don’t have to every SSH onto your server.\u003C/p>\n\u003Cp>Start by running \u003Ccode>fasten init\u003C/code>, this will walk you through general setup.  Once you have the setup sorted, run \u003Ccode>fasten install\u003C/code>, this will update the OS and install all the language runtimes.  Finally, deploy your code by running \u003Ccode>fasten deploy\u003C/code>.  From now on, you will only have to run fasten deploy.\u003C/p>\n\u003Cp>Below is an example of deploying to a server, be aware, the nokogiri gem is very slow to install, but it does complete.\u003C/p>\n\u003Cimg src=\"/images/fasten-deploy.gif\" class=\"img-responsive\" alt=\"fasten\">\n\u003Ca href=\"/images/fasten-deploy.gif\" alt=\"fasten\">Link to image\u003C/a>\n\u003Ch3 id=\"try-it-out\">Try it out\u003C/h3>\n\u003Cp>If you want to given some feedback on Fasten, you can get the code here \u003Ca href=\"https://github.com/DaveBlooman/fasten\">https://github.com/DaveBlooman/fasten\u003C/a>\u003C/p>",{"headings":613,"localImagePaths":623,"remoteImagePaths":624,"frontmatter":625,"imagePaths":627},[614,617,620],{"depth":119,"slug":615,"text":616},"what-are-we-currently-doing","What are we currently doing",{"depth":119,"slug":618,"text":619},"what-fasten-does","What Fasten does",{"depth":119,"slug":621,"text":622},"try-it-out","Try it out",[],[],{"title":604,"description":605,"pubDate":626},"2017-09-10",[],"2016-09-11-consul-and-service-discovery-how-it-can-help",{"id":628,"data":630,"body":634,"filePath":635,"digest":636,"rendered":637},{"title":631,"description":632,"pubDate":633},"Consul And Service Discovery, How It Can Help","Consul https://www.consul.io/ is a tool I've been using the last few months to get a handle on our expanding platform. Consul is a tool built by Hashicorp to...",["Date","2016-09-11T00:00:00.000Z"],"[Consul](https://www.consul.io/) is a tool I've been using the last few months to get a handle on our expanding platform.  Consul is a tool built by Hashicorp to help with service discovery and configuration management featuring key/value store, health checks and DNS forwarding.  The service as a whole is open source, with other tools plugging into the Consul HTTP API.  It's these other tools that have made Consul worthwhile for us, this post will focus on [envconsul](https://github.com/hashicorp/envconsul) and [consul-template](https://github.com/hashicorp/consul-template).\n\n### Sounds great, why do I need it?\n\nIf you are running a single app, you are probably very aware of where it is running, what class of server it is on, how many instances, what the up time is, how to deploy it etc.  There comes a point in a lot of companies, big and small, where a single app just isn't the right fit anymore.  \n\nYou might go down the route of building micro services or try a service orientated architecture, whatever you decide to do, you want another app to complement your monolith.  That is to say, your monolith is staying around, but you want to be able to build things that are not really in the scope of that project anymore.  In a situation where you are completely cloud, this isn't something you can jump right into without some thought.  Is this new thing you are building an internal service, is it public facing, do I need to have walls between the new app and the old, or does it need to be highly connected.\n\nThese sorts of decisions are essential to deciding on whether you need a service like Consul, as not everyone will.  \n\nWhen you decide you need something like Consul, it is probably because you come to one of the following questions.\n\n - How do we know when something fails?  \n - How do we know when a new server comes up?  \n - How do we check multiple parts of my internal apps?  \n - How do we update configs from servers coming up and down?  \n - How do we store things like database names, S3 bucket names, other variables?  \n - How do we update variables when they change, do we need to redeploy the app?  \n\nEssentially it boils down to\n\n - How do we automatically know and control what my apps and infrastructure are doing at any given time\n\n## Discover all the things\n\nService discovery is quite a big topic, with lots of books written on discovery in distributed systems.  To not repeat what others have already gone into detail about, [here](https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/) is a post by NGINX about micro services and service discovery which will help in understanding different service discovery approaches.\n\nSome of the take away points are knowing what port, IP address, DC/VPC and whether the service is healthy or not is great for automation with other systems, like NGINX.\n\nSome cloud providers allow you to utilise service discovery without having to run something like Consul.  AWS have the Application Load Balancer(ALB), this joins up nicely with an auto scaling group so that when your servers need to scale due to demand, they are automatically added to the load balancer.  This technique means you can leave your infrastructure to respond to application demands without human intervention.\n\nIf you are manually editing NGINX configs, updating server lists or using fixed IP address, you could benefit from using service discovery via Consul.\n\n## OK, but I'm still not sold\n\nIt can often be the more practical details that really get people on board with something like Consul, so let's get to the cool stuff first.\n\nThere are 3 tools to talk about, Consul, consul-template and envconsul.  Hopefully you know roughly what Consul is and it's main features, but here is a quick refresher.  Consul is a two-part system, server and agent.  The Servers run in a cluster of 3 nodes for high availability and are the single point of truth for any node that connects to it.  Consul has a K/V store, supports DNS forwarding, service health checks and has a fully featured HTTP API.  Consul can host a web UI that is quite nice for viewing the KV store and looking at health checks.  The agent is a service that runs every server in your infrastructure and communicates with the other nodes in your cluster.  This means that Consul is constantly seeding data out to make Consul API calls fast and up to date.\n\nConsul's API is core to the other two tools, consul-template and envconsul.\n\n### envconsul\nenvconsul can be used to drive dynamic config changes from the Consul KV store and restart your application to take the values.  If you are currently having to rebuild your server or Docker image because you bake in config, you probably know how a single character mistake can cause you to rebuild and redeploy.  envconsul is the best solution I have seen to prevent that, by giving you lots of options in how the app restart, what values from Consul are going to be injected into your app and what the behaviour should be if something fails.  \n\nenvconsul is written in Go and is deployed as a single binary.  You wrap your call to your app with envconsul, such as\n\n```sh\nenvconsul -consul 127.0.0.1:8500 -sanitize -upcase -prefix myvalues/ /opt/my_application\n```\nWhat happens here is we specify the consul agent, then use envconsul to force the environment variables to a certain case, upcase, then pull in KV from Consul from a folder, `myvalues`.  The my_application can be anything, so you can print out all of the values from your Consul values using\n\n```sh\nenvconsul -consul 127.0.0.1:8500 -sanitize -upcase -prefix myvalues env\n\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/\nSHLVL=1\nHOME=/root\nno_proxy=*.local, 169.254/16\n_=/usr/local/bin/envconsul\nTESTAPP_API_KEY=3234234235sosefse\nTESTAPP_S3_BUCKET=testing_bucket\n```\n\nHere is a capture from the Consul UI to show the original form of the values.\n\n\u003Cimg src=\"/images/consul-ui.png\" class=\"img-responsive\" alt=\"Consul-UI\">\n\u003Ca href=\"/images/consul-ui.png\" alt=\"Apex Lambda\">Link to image\u003C/a>\n\nYou can obviously do some simple things to prevent failure if Consul wasn't contactable at some point, such as cache the output into a JSON file.  This pattern is used in some Docker systems using a \"side car\" container, but this could also apply to standard on server apps.  \n\n### Service Definition\n\nConsul has covered a few of the questions I posed earlier, but this is the tool that really puts Consul at the core of you service discovery.  Consul supports the concept of services, which can be anything that you want it to be.  A service has a few basics and is described in a JSON file.\n\n```json\n{\n  \"service\": {\n    \"name\": \"some-service\",\n    \"tags\": [\n      \"golang\"\n    ],\n    \"port\": 8080,\n    \"check\": {\n      \"name\": \"some-api\",\n      \"script\": \"curl -s localhost:8080/status\",\n      \"interval\": \"10s\"\n    }\n  }\n}\n```\n\nIn reality, a service definition can just be a file like this\n\n```json\n{\n  \"service\": {\n    \"name\": \"some-service\",\n    \"port\": 8080\n  }\n}\n```\n\nEither way, your Consul cluster will know about `some-service`, what port it's on and the meta data from the agent.  The agent data will include IP address, datacenter etc.  \n\nWhen a Consul agent is started and this file is found in its config.d directory, it will register with the cluster.  This will occur for every instance of the service, so you will know which servers your app is running on, what their IP's are and what port they are running on.   Taking a closer look at the first example, there is a health check, this combined with the service information means that if your health check fails, the instance of the app will not be included when queried by consul-template.  This is useful when you are reliant on a DB, cache, external service and it goes down for that instance, you don't want it to be in service anymore.  This type of service availability is much more powerful than just a TCP or HTTP check, meaning much more thorough checking before a service is considered healthy.\n\n### consul-template\n\n[consul-template](https://github.com/hashicorp/consul-template) provides a convenient way to populate values from Consul into the file system using the consul-template daemon.  In the same way we saw values injected into an app using envconsul, we can dynamically update files and reload applications such as [HAProxy](http://www.haproxy.org/).\n\nconsul-template will only write healthy checks of apps, so you can be sure that your services are not going to be full of unhealthy apps.  consul-template has a large array of options, with either a CLI or config file driven configuration.\n\nHere is an example of using consul-template with HAProxy\n\n```sh\nconsul-template -consul 127.0.0.1:8500 -template \"/etc/haproxy/haproxy.ctmpl:/etc/haproxy/haproxy.cfg:service haproxy restart\" -retry 30s\n```\n\nThe `ctmpl` file contains the Go template syntax, so it is quite simple to write complex configs.  In my example, the Consul dashboard is being exposed but the Consul health check operates on port 8300, so the web UI wouldn't be accessible.  With a little conditional logic, we can view the dashboard on a different port, 8500 in this case.  \n\nThe template is also full of ranges, populated by Consul defined services that fill out the server's, port and name sections.  This is where the service definition file data is used, building a full HAProxy config.\n\n\n```go\n{{range services}}\nacl is_{{.Name}} hdr(host) -i {{.Name}}.example.com\n{{end}}\n{{range services}}\nuse_backend {{.Name}} if is_{{.Name}}\n{{end}}\n\n{{range services}}\n{{ if .Name | contains \"consul\" }}\nbackend {{.Name}}\n        mode http\n        balance roundrobin\n        {{range service .Name}}\n        server {{.Name}} {{.Address}}:8500 {{end}}\n{{else}}\nbackend {{.Name}}\n\tmode http\n\tbalance roundrobin\n\t{{range service .Name}}\n\tserver {{.Name}} {{.Address}}:{{.Port}} {{end}}\n{{end}}{{end}}\n\nfrontend stats\n\tbind *:1936\n\tdefault_backend stats\n\nbackend stats\n\tmode http\n\tstats enable\n\tstats uri /\n```\n\nThis will generate a file at `/etc/haproxy/haproxy.cfg` that should be thought as read only.  If you need to make hard coded entries, edit the template file and restart the consul-template wrapper service.  \n\n### Looks good, right?\n\nAt this point, you will hopefully be thinking, this all sounds great, centralised health checks, config values for injecting, dynamic config generation, but where does it fit in with my company.\n\nTaking a step back for a moment, this post is aimed at those who don't have a service like this currently and are thinking about it.  Or perhaps you are a company that wants to know if it is worth the learning time and want to justify implementing a new system like Consul.  As i've mentioned previously, there are a few reasons to run Consul, but it comes down to how much you already have in place.  Do you have a health check service, central config store, how do you do load balancing and are you wanting to run [HAProxy](http://www.haproxy.org/) or [NGINX](https://www.nginx.com/) instead of a cloud provider?\n\nI like to think about it like this, once your cluster is up, all you need to do is write a 10 like JSON file and your service can be automatically discovered, load balanced, be DNS reachable and have health checks.  I recently rolled out a [Grafana](http://grafana.org/) server, it was based on a AMI bake and once it started I could see it passed health check and could visit the app immediately.  I could have easily created an route53 entry on an ELB with ASG and EC2 instance and made it the same way, but this assumes a lot about my infrastructure knowledge.  \n\nI recently was setting up a service with another developer and once we had the health check file in place, our goal was simply to get a check light in Consul.   Once we did, the app was working and we could hit the apps API.  There are always going to be resources needed for deploying on cloud which may be a more infra team task, but once you have them in place, creating a new service can be as easy as creating a CI job, Docker image or baking an AMI.\n\n## Lots More\n\nConsul has a lot more to offer than I've mentioned, with most people picking and choosing which features to take advantage of. If you feel like building your own service, have a look at the [Watches](https://www.consul.io/docs/agent/watches.html) part of the API, it can really help to run custom scripts based on events.  An example would be sending a slack message when a service goes critical, [example](https://github.com/AcalephStorage/consul-alerts).  There are also lots of things that plug into Consul, Hashicorp's own [Vault](https://www.vaultproject.io/), but also some cool projects like [Fabio](https://github.com/eBay/fabio) and [Traefik](https://github.com/containous/traefik).  \n\nYou can read the full documentation of Consul here [https://www.consul.io](https://www.consul.io/)","src/content/blog/2016-09-11-consul-and-service-discovery-how-it-can-help.md","246cdb651574ee54",{"html":638,"metadata":639},"\u003Cp>\u003Ca href=\"https://www.consul.io/\">Consul\u003C/a> is a tool I’ve been using the last few months to get a handle on our expanding platform.  Consul is a tool built by Hashicorp to help with service discovery and configuration management featuring key/value store, health checks and DNS forwarding.  The service as a whole is open source, with other tools plugging into the Consul HTTP API.  It’s these other tools that have made Consul worthwhile for us, this post will focus on \u003Ca href=\"https://github.com/hashicorp/envconsul\">envconsul\u003C/a> and \u003Ca href=\"https://github.com/hashicorp/consul-template\">consul-template\u003C/a>.\u003C/p>\n\u003Ch3 id=\"sounds-great-why-do-i-need-it\">Sounds great, why do I need it?\u003C/h3>\n\u003Cp>If you are running a single app, you are probably very aware of where it is running, what class of server it is on, how many instances, what the up time is, how to deploy it etc.  There comes a point in a lot of companies, big and small, where a single app just isn’t the right fit anymore.\u003C/p>\n\u003Cp>You might go down the route of building micro services or try a service orientated architecture, whatever you decide to do, you want another app to complement your monolith.  That is to say, your monolith is staying around, but you want to be able to build things that are not really in the scope of that project anymore.  In a situation where you are completely cloud, this isn’t something you can jump right into without some thought.  Is this new thing you are building an internal service, is it public facing, do I need to have walls between the new app and the old, or does it need to be highly connected.\u003C/p>\n\u003Cp>These sorts of decisions are essential to deciding on whether you need a service like Consul, as not everyone will.\u003C/p>\n\u003Cp>When you decide you need something like Consul, it is probably because you come to one of the following questions.\u003C/p>\n\u003Cul>\n\u003Cli>How do we know when something fails?\u003C/li>\n\u003Cli>How do we know when a new server comes up?\u003C/li>\n\u003Cli>How do we check multiple parts of my internal apps?\u003C/li>\n\u003Cli>How do we update configs from servers coming up and down?\u003C/li>\n\u003Cli>How do we store things like database names, S3 bucket names, other variables?\u003C/li>\n\u003Cli>How do we update variables when they change, do we need to redeploy the app?\u003C/li>\n\u003C/ul>\n\u003Cp>Essentially it boils down to\u003C/p>\n\u003Cul>\n\u003Cli>How do we automatically know and control what my apps and infrastructure are doing at any given time\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"discover-all-the-things\">Discover all the things\u003C/h2>\n\u003Cp>Service discovery is quite a big topic, with lots of books written on discovery in distributed systems.  To not repeat what others have already gone into detail about, \u003Ca href=\"https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/\">here\u003C/a> is a post by NGINX about micro services and service discovery which will help in understanding different service discovery approaches.\u003C/p>\n\u003Cp>Some of the take away points are knowing what port, IP address, DC/VPC and whether the service is healthy or not is great for automation with other systems, like NGINX.\u003C/p>\n\u003Cp>Some cloud providers allow you to utilise service discovery without having to run something like Consul.  AWS have the Application Load Balancer(ALB), this joins up nicely with an auto scaling group so that when your servers need to scale due to demand, they are automatically added to the load balancer.  This technique means you can leave your infrastructure to respond to application demands without human intervention.\u003C/p>\n\u003Cp>If you are manually editing NGINX configs, updating server lists or using fixed IP address, you could benefit from using service discovery via Consul.\u003C/p>\n\u003Ch2 id=\"ok-but-im-still-not-sold\">OK, but I’m still not sold\u003C/h2>\n\u003Cp>It can often be the more practical details that really get people on board with something like Consul, so let’s get to the cool stuff first.\u003C/p>\n\u003Cp>There are 3 tools to talk about, Consul, consul-template and envconsul.  Hopefully you know roughly what Consul is and it’s main features, but here is a quick refresher.  Consul is a two-part system, server and agent.  The Servers run in a cluster of 3 nodes for high availability and are the single point of truth for any node that connects to it.  Consul has a K/V store, supports DNS forwarding, service health checks and has a fully featured HTTP API.  Consul can host a web UI that is quite nice for viewing the KV store and looking at health checks.  The agent is a service that runs every server in your infrastructure and communicates with the other nodes in your cluster.  This means that Consul is constantly seeding data out to make Consul API calls fast and up to date.\u003C/p>\n\u003Cp>Consul’s API is core to the other two tools, consul-template and envconsul.\u003C/p>\n\u003Ch3 id=\"envconsul\">envconsul\u003C/h3>\n\u003Cp>envconsul can be used to drive dynamic config changes from the Consul KV store and restart your application to take the values.  If you are currently having to rebuild your server or Docker image because you bake in config, you probably know how a single character mistake can cause you to rebuild and redeploy.  envconsul is the best solution I have seen to prevent that, by giving you lots of options in how the app restart, what values from Consul are going to be injected into your app and what the behaviour should be if something fails.\u003C/p>\n\u003Cp>envconsul is written in Go and is deployed as a single binary.  You wrap your call to your app with envconsul, such as\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">envconsul\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -consul\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 127.0.0.1:8500\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -sanitize\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -upcase\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -prefix\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> myvalues/\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> /opt/my_application\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>What happens here is we specify the consul agent, then use envconsul to force the environment variables to a certain case, upcase, then pull in KV from Consul from a folder, \u003Ccode>myvalues\u003C/code>.  The my_application can be anything, so you can print out all of the values from your Consul values using\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">envconsul\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -consul\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 127.0.0.1:8500\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -sanitize\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -upcase\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -prefix\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> myvalues\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> env\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">PATH\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">PWD\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">/\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">SHLVL\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">1\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">HOME\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">/root\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">no_proxy\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">*.local,\u003C/span>\u003Cspan style=\"color:#B392F0\"> 169.254/16\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">_\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">/usr/local/bin/envconsul\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">TESTAPP_API_KEY\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">3234234235sosefse\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">TESTAPP_S3_BUCKET\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">testing_bucket\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Here is a capture from the Consul UI to show the original form of the values.\u003C/p>\n\u003Cimg src=\"/images/consul-ui.png\" class=\"img-responsive\" alt=\"Consul-UI\">\n\u003Ca href=\"/images/consul-ui.png\" alt=\"Apex Lambda\">Link to image\u003C/a>\n\u003Cp>You can obviously do some simple things to prevent failure if Consul wasn’t contactable at some point, such as cache the output into a JSON file.  This pattern is used in some Docker systems using a “side car” container, but this could also apply to standard on server apps.\u003C/p>\n\u003Ch3 id=\"service-definition\">Service Definition\u003C/h3>\n\u003Cp>Consul has covered a few of the questions I posed earlier, but this is the tool that really puts Consul at the core of you service discovery.  Consul supports the concept of services, which can be anything that you want it to be.  A service has a few basics and is described in a JSON file.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"service\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"name\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"some-service\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"tags\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">      \"golang\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    ],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"port\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">8080\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"check\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"name\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"some-api\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"script\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"curl -s localhost:8080/status\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"interval\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"10s\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>In reality, a service definition can just be a file like this\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"service\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"name\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"some-service\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"port\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">8080\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Either way, your Consul cluster will know about \u003Ccode>some-service\u003C/code>, what port it’s on and the meta data from the agent.  The agent data will include IP address, datacenter etc.\u003C/p>\n\u003Cp>When a Consul agent is started and this file is found in its config.d directory, it will register with the cluster.  This will occur for every instance of the service, so you will know which servers your app is running on, what their IP’s are and what port they are running on.   Taking a closer look at the first example, there is a health check, this combined with the service information means that if your health check fails, the instance of the app will not be included when queried by consul-template.  This is useful when you are reliant on a DB, cache, external service and it goes down for that instance, you don’t want it to be in service anymore.  This type of service availability is much more powerful than just a TCP or HTTP check, meaning much more thorough checking before a service is considered healthy.\u003C/p>\n\u003Ch3 id=\"consul-template\">consul-template\u003C/h3>\n\u003Cp>\u003Ca href=\"https://github.com/hashicorp/consul-template\">consul-template\u003C/a> provides a convenient way to populate values from Consul into the file system using the consul-template daemon.  In the same way we saw values injected into an app using envconsul, we can dynamically update files and reload applications such as \u003Ca href=\"http://www.haproxy.org/\">HAProxy\u003C/a>.\u003C/p>\n\u003Cp>consul-template will only write healthy checks of apps, so you can be sure that your services are not going to be full of unhealthy apps.  consul-template has a large array of options, with either a CLI or config file driven configuration.\u003C/p>\n\u003Cp>Here is an example of using consul-template with HAProxy\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">consul-template\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -consul\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 127.0.0.1:8500\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -template\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"/etc/haproxy/haproxy.ctmpl:/etc/haproxy/haproxy.cfg:service haproxy restart\"\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -retry\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 30s\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The \u003Ccode>ctmpl\u003C/code> file contains the Go template syntax, so it is quite simple to write complex configs.  In my example, the Consul dashboard is being exposed but the Consul health check operates on port 8300, so the web UI wouldn’t be accessible.  With a little conditional logic, we can view the dashboard on a different port, 8500 in this case.\u003C/p>\n\u003Cp>The template is also full of ranges, populated by Consul defined services that fill out the server’s, port and name sections.  This is where the service definition file data is used, building a full HAProxy config.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"go\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{{\u003C/span>\u003Cspan style=\"color:#F97583\">range\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> services}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">acl \u003C/span>\u003Cspan style=\"color:#B392F0\">is_\u003C/span>\u003Cspan style=\"color:#E1E4E8\">{{.Name}} \u003C/span>\u003Cspan style=\"color:#B392F0\">hdr\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(host) \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\">i {{.Name}}.example.com\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{{end}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{{\u003C/span>\u003Cspan style=\"color:#F97583\">range\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> services}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">use_backend {{.Name}} \u003C/span>\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> is_{{.Name}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{{end}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{{\u003C/span>\u003Cspan style=\"color:#F97583\">range\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> services}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{{ \u003C/span>\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> .Name \u003C/span>\u003Cspan style=\"color:#F97583\">|\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> contains \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"consul\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> }}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">backend {{.Name}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        mode http\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        balance roundrobin\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        {{\u003C/span>\u003Cspan style=\"color:#F97583\">range\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> service .Name}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        server {{.Name}} {{.Address}}:\u003C/span>\u003Cspan style=\"color:#79B8FF\">8500\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {{end}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{{\u003C/span>\u003Cspan style=\"color:#F97583\">else\u003C/span>\u003Cspan style=\"color:#E1E4E8\">}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">backend {{.Name}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tmode http\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tbalance roundrobin\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\t{{\u003C/span>\u003Cspan style=\"color:#F97583\">range\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> service .Name}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tserver {{.Name}} {{.Address}}:{{.Port}} {{end}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{{end}}{{end}}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">frontend stats\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tbind \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003Cspan style=\"color:#79B8FF\">1936\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tdefault_backend stats\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">backend stats\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tmode http\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tstats enable\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tstats uri \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This will generate a file at \u003Ccode>/etc/haproxy/haproxy.cfg\u003C/code> that should be thought as read only.  If you need to make hard coded entries, edit the template file and restart the consul-template wrapper service.\u003C/p>\n\u003Ch3 id=\"looks-good-right\">Looks good, right?\u003C/h3>\n\u003Cp>At this point, you will hopefully be thinking, this all sounds great, centralised health checks, config values for injecting, dynamic config generation, but where does it fit in with my company.\u003C/p>\n\u003Cp>Taking a step back for a moment, this post is aimed at those who don’t have a service like this currently and are thinking about it.  Or perhaps you are a company that wants to know if it is worth the learning time and want to justify implementing a new system like Consul.  As i’ve mentioned previously, there are a few reasons to run Consul, but it comes down to how much you already have in place.  Do you have a health check service, central config store, how do you do load balancing and are you wanting to run \u003Ca href=\"http://www.haproxy.org/\">HAProxy\u003C/a> or \u003Ca href=\"https://www.nginx.com/\">NGINX\u003C/a> instead of a cloud provider?\u003C/p>\n\u003Cp>I like to think about it like this, once your cluster is up, all you need to do is write a 10 like JSON file and your service can be automatically discovered, load balanced, be DNS reachable and have health checks.  I recently rolled out a \u003Ca href=\"http://grafana.org/\">Grafana\u003C/a> server, it was based on a AMI bake and once it started I could see it passed health check and could visit the app immediately.  I could have easily created an route53 entry on an ELB with ASG and EC2 instance and made it the same way, but this assumes a lot about my infrastructure knowledge.\u003C/p>\n\u003Cp>I recently was setting up a service with another developer and once we had the health check file in place, our goal was simply to get a check light in Consul.   Once we did, the app was working and we could hit the apps API.  There are always going to be resources needed for deploying on cloud which may be a more infra team task, but once you have them in place, creating a new service can be as easy as creating a CI job, Docker image or baking an AMI.\u003C/p>\n\u003Ch2 id=\"lots-more\">Lots More\u003C/h2>\n\u003Cp>Consul has a lot more to offer than I’ve mentioned, with most people picking and choosing which features to take advantage of. If you feel like building your own service, have a look at the \u003Ca href=\"https://www.consul.io/docs/agent/watches.html\">Watches\u003C/a> part of the API, it can really help to run custom scripts based on events.  An example would be sending a slack message when a service goes critical, \u003Ca href=\"https://github.com/AcalephStorage/consul-alerts\">example\u003C/a>.  There are also lots of things that plug into Consul, Hashicorp’s own \u003Ca href=\"https://www.vaultproject.io/\">Vault\u003C/a>, but also some cool projects like \u003Ca href=\"https://github.com/eBay/fabio\">Fabio\u003C/a> and \u003Ca href=\"https://github.com/containous/traefik\">Traefik\u003C/a>.\u003C/p>\n\u003Cp>You can read the full documentation of Consul here \u003Ca href=\"https://www.consul.io/\">https://www.consul.io\u003C/a>\u003C/p>",{"headings":640,"localImagePaths":663,"remoteImagePaths":664,"frontmatter":665,"imagePaths":667},[641,644,647,650,652,655,657,660],{"depth":119,"slug":642,"text":643},"sounds-great-why-do-i-need-it","Sounds great, why do I need it?",{"depth":42,"slug":645,"text":646},"discover-all-the-things","Discover all the things",{"depth":42,"slug":648,"text":649},"ok-but-im-still-not-sold","OK, but I’m still not sold",{"depth":119,"slug":651,"text":651},"envconsul",{"depth":119,"slug":653,"text":654},"service-definition","Service Definition",{"depth":119,"slug":656,"text":656},"consul-template",{"depth":119,"slug":658,"text":659},"looks-good-right","Looks good, right?",{"depth":42,"slug":661,"text":662},"lots-more","Lots More",[],[],{"title":631,"description":632,"pubDate":666},"2016-09-11",[],"2025_05_27-observability",{"id":668,"data":670,"body":674,"filePath":675,"digest":676,"rendered":677},{"title":671,"description":672,"pubDate":673},"Stop Guessing, Start Tracing: Pinpointing Service Bottlenecks with OpenTelemetry","Diagnosing performance issues in software can often feel more like detective work than engineering. In this post, we explore how tracing with OpenTelemetry helped us identify and resolve a critical bottleneck in our service, transforming our approach to observability and performance optimization.",["Date","2025-05-27T00:00:00.000Z"],"\u003Cfigure class=\"hero\">\n    \u003Cimg src=\"/images/otel_hero.webp\" alt=\"OpenTelemetry Hero Image\" class=\"img-responsive\" loading=\"eager\" />\n    \u003Cfigcaption>OpenTelemetry\u003C/figcaption>\n\u003C/figure>\n\n---\n\nDiagnosing issues in software can often feel more like detective work than engineering. Imagine sitting in a doctor’s office. They ask, “How can I help?” You describe your symptoms, they perform their usual checks, and eventually, diagnose your condition. Simple enough, right?\n\nBut as software engineers, our “patient” might be a Java, Go, or Node service that isn’t performing well. We can’t exactly chat with it — at least, not yet. So, what’s the best way to understand why a service isn’t behaving as expected?\n\n## Diagnosing Services — More Detective Work Than Medicine\n\nA few weeks ago, our team faced a classic issue: a service wasn’t consuming messages fast enough. Naturally, we gathered as a team and started asking questions. Initially, we individually dove into the codebase, while I added tracing to a few suspect areas. During the next meeting, a section of the code was quickly flagged as suspicious — admittedly, none of us clearly remembered why — but tracing would validate our hunch.\n\nThis scenario is surprisingly common:\n\n> - “The code looks like it could be slowing things down; it must be the problem.”\n> - Refactor the suspect code.\n> - No noticeable performance improvement.\n>\n> We needed concrete data to identify the real bottleneck — that’s where tracing comes in.\n\n## Tracing vs. Logging vs. Metrics\n\nBefore we dive deeper, let’s clarify what tracing actually is.\n\nLogging captures what happened in a system, often with verbose detail. Logs are great for debugging specific issues, but also when well structured, can drive alerts.\nMetrics provide a high-level, numerical view of system behavior over time. Metrics are excellent for tracking trends (e.g. request rates, CPU usage, latencies) and triggering alerts when thresholds are crossed. Modern metrics systems can break down latency per component, giving you detailed performance insights. However, they often lack request-level context and causality — they tell you what is slow, but not necessarily why.\nTracing bridges this gap. It links together individual operations within a request, showing the full path that request takes through your services. Each trace includes timing information, metadata, and hierarchical relationships between operations, making it easy to see not just where time is spent, but how different services and components interact.\nTracing is especially powerful for complex interactions involving multiple services, external API calls, or interactions with cloud providers.\n\n## Case Study: Chasing Performance Ghosts\n\nOur tracing revealed that the initial suspect — the so-called “bad” code — was actually lightning-fast. The real culprit was less exciting but more impactful: we were spending too much time uploading large files to a cloud storage bucket. Recent data growth caused file sizes to balloon. Optimising file copying improved things, but still wasn’t sufficient.\n\nTracing provided granular, timestamped visibility into each step of our system’s workflow. By instrumenting the upload process, we observed unusually long spans specifically during the file compression phase. Grafana highlighted that compression accounted for over 80% of the total processing time, while our original hypothesis focused on the upload network speed.\n\nFurther examination showed that recent data growth had ballooned our JSON Lines (JSONL) files to around 40GB. We were using Zstandard (ZSTD), a compression algorithm known for high compression ratios but comparatively slower speeds when dealing with massive datasets.\n\nWrite on Medium\nWithout tracing, identifying precisely which operation — uploading or compression — was causing delays would have been guesswork. Instead, tracing visualized this bottleneck directly, enabling us to confidently shift our optimization efforts toward compression rather than file transfers.\n\n## Our Observability Stack — Tools of the Trade\n\nThis experience solidified our observability approach, leading us to rely on a carefully chosen stack centered around OpenTelemetry:\n\nTraces → OpenTelemetry SDK → OTel Collector → Tempo\nMetrics → Prometheus scraping /metrics endpoint → Mimir\nLogs → Promtail → Loki\nFrontend Visualization → Grafana\nError Tracking → Sentry\nWe specifically route only tracing data through the OpenTelemetry collector primarily for performance efficiency and because our existing infrastructure for metrics and logs already provides optimal handling.\n\n## Beyond Basic Spans — Getting More Value from Tracing\n\nWhen engineers first explore tracing, they typically just create a few basic spans and stop there. However, tracing can offer far richer insights if used effectively:\n\nDon’t overload traces: Keep spans lean, focused on core data: function execution time, outcomes, and key events.\nCapture errors and events: Explicitly record errors and notable events for better debugging.\nHere’s a simplified example in Go using OpenTelemetry:\n\n```go\n// Start a new span for tracing\nctx, span := observability.StartSpan(ctx, \"UploadFile\")\ndefer span.End()\n\n// Perform the file upload\nresp, err := uploadFileToBucket()\nif err != nil {\n    span.RecordError(err, trace.WithStackTrace(true))\n    span.SetStatus(codes.Error, err.Error())\n    return\n}\n// Add important event details to span\nspan.AddEvent(\"File uploaded\", trace.WithAttributes(attribute.String(\"file_id\", resp.ID)))\n```\n\nOur custom wrapper function, observability.StartSpan, initializes spans and provides standardized error handling. Context (ctx) propagation ensures continuity across distributed services.\n\n## Connecting the Dots — Tracing, Logs, and Metrics\n\n_Click the image to view full size._\n\nGrafana significantly enhances our workflow with its “trace-to-logs” integration. By embedding trace IDs directly into our logs (using Zerolog hooks), we can easily jump from a trace directly to the corresponding log entries.\n\nBy scoping logs to a specific span, we have a much faster way of getting the right log information. I often see engineers doing a generic search over a large logset, rather than determining where to start, then refining the search. This approach, in a pinch when on-call, reduces the time to resolution of issues.\n\nHere’s how we integrate trace IDs with our logs in Go:\n\n```go\nfunc setupZeroLog(level zerolog.Level) {\n    sentryWriter := SentryWriter{hub: sentry.CurrentHub()}\n    multi := zerolog.MultiLevelWriter(os.Stdout, sentryWriter)\n    log.Logger = zerolog.New(multi).\n        Level(level).\n        Hook(TraceHook{}).\n        With().\n        Timestamp().\n        Logger()\n    zerolog.TimeFieldFormat = zerolog.TimeFormatUnix\n}\n\ntype TraceHook struct{}\n\nfunc (h TraceHook) Run(e *zerolog.Event, level zerolog.Level, msg string) {\n    ctx := e.GetCtx()\n    if ctx == nil {\n        return\n    }\n    if span := trace.SpanFromContext(ctx); span != nil {\n        sc := span.SpanContext()\n        if sc.HasTraceID() {\n            e.Str(\"trace_id\", sc.TraceID().String())\n            e.Str(\"span_id\", sc.SpanID().String())\n        }\n    }\n}\n```\n\n## Practical Tips for Effective Tracing\n\nHere are some best practices to maximize the value of tracing:\n\nAvoid excessive spans in loops: Don’t flood your tracing system; aggregate repetitive tasks.\nLeverage built-in tracing: Many cloud/database SDKs have built-in OpenTelemetry support — use them!\nStandardise span attributes: Add consistent, searchable attributes (like IDs) to spans for easy filtering.\nWherever possible, follow the OpenTelemetry semantic conventions for naming span attributes and operations. This helps ensure consistency across services and tools, and makes your traces more interoperable and readable, especially when visualized in tools like Grafana.\n\n## Observability Empowers Better Decisions\n\nTracing turned a suggestion of a problem into precise insights, transforming how we diagnose and solve performance issues. With OpenTelemetry and a well-designed observability stack, you can use data and analysis to drive decisions.\n\nWhen your service starts feeling under the weather, tracing empowers you to quickly diagnose, understand, and heal your digital patient.","src/content/blog/2025_05_27-observability.md","d70c3ca9f34f0709",{"html":678,"metadata":679},"\u003Cfigure class=\"hero\">\n    \u003Cimg src=\"/images/otel_hero.webp\" alt=\"OpenTelemetry Hero Image\" class=\"img-responsive\" loading=\"eager\">\n    \u003Cfigcaption>OpenTelemetry\u003C/figcaption>\n\u003C/figure>\n\u003Chr>\n\u003Cp>Diagnosing issues in software can often feel more like detective work than engineering. Imagine sitting in a doctor’s office. They ask, “How can I help?” You describe your symptoms, they perform their usual checks, and eventually, diagnose your condition. Simple enough, right?\u003C/p>\n\u003Cp>But as software engineers, our “patient” might be a Java, Go, or Node service that isn’t performing well. We can’t exactly chat with it — at least, not yet. So, what’s the best way to understand why a service isn’t behaving as expected?\u003C/p>\n\u003Ch2 id=\"diagnosing-services--more-detective-work-than-medicine\">Diagnosing Services — More Detective Work Than Medicine\u003C/h2>\n\u003Cp>A few weeks ago, our team faced a classic issue: a service wasn’t consuming messages fast enough. Naturally, we gathered as a team and started asking questions. Initially, we individually dove into the codebase, while I added tracing to a few suspect areas. During the next meeting, a section of the code was quickly flagged as suspicious — admittedly, none of us clearly remembered why — but tracing would validate our hunch.\u003C/p>\n\u003Cp>This scenario is surprisingly common:\u003C/p>\n\u003Cblockquote>\n\u003Cul>\n\u003Cli>“The code looks like it could be slowing things down; it must be the problem.”\u003C/li>\n\u003Cli>Refactor the suspect code.\u003C/li>\n\u003Cli>No noticeable performance improvement.\u003C/li>\n\u003C/ul>\n\u003Cp>We needed concrete data to identify the real bottleneck — that’s where tracing comes in.\u003C/p>\n\u003C/blockquote>\n\u003Ch2 id=\"tracing-vs-logging-vs-metrics\">Tracing vs. Logging vs. Metrics\u003C/h2>\n\u003Cp>Before we dive deeper, let’s clarify what tracing actually is.\u003C/p>\n\u003Cp>Logging captures what happened in a system, often with verbose detail. Logs are great for debugging specific issues, but also when well structured, can drive alerts.\nMetrics provide a high-level, numerical view of system behavior over time. Metrics are excellent for tracking trends (e.g. request rates, CPU usage, latencies) and triggering alerts when thresholds are crossed. Modern metrics systems can break down latency per component, giving you detailed performance insights. However, they often lack request-level context and causality — they tell you what is slow, but not necessarily why.\nTracing bridges this gap. It links together individual operations within a request, showing the full path that request takes through your services. Each trace includes timing information, metadata, and hierarchical relationships between operations, making it easy to see not just where time is spent, but how different services and components interact.\nTracing is especially powerful for complex interactions involving multiple services, external API calls, or interactions with cloud providers.\u003C/p>\n\u003Ch2 id=\"case-study-chasing-performance-ghosts\">Case Study: Chasing Performance Ghosts\u003C/h2>\n\u003Cp>Our tracing revealed that the initial suspect — the so-called “bad” code — was actually lightning-fast. The real culprit was less exciting but more impactful: we were spending too much time uploading large files to a cloud storage bucket. Recent data growth caused file sizes to balloon. Optimising file copying improved things, but still wasn’t sufficient.\u003C/p>\n\u003Cp>Tracing provided granular, timestamped visibility into each step of our system’s workflow. By instrumenting the upload process, we observed unusually long spans specifically during the file compression phase. Grafana highlighted that compression accounted for over 80% of the total processing time, while our original hypothesis focused on the upload network speed.\u003C/p>\n\u003Cp>Further examination showed that recent data growth had ballooned our JSON Lines (JSONL) files to around 40GB. We were using Zstandard (ZSTD), a compression algorithm known for high compression ratios but comparatively slower speeds when dealing with massive datasets.\u003C/p>\n\u003Cp>Write on Medium\nWithout tracing, identifying precisely which operation — uploading or compression — was causing delays would have been guesswork. Instead, tracing visualized this bottleneck directly, enabling us to confidently shift our optimization efforts toward compression rather than file transfers.\u003C/p>\n\u003Ch2 id=\"our-observability-stack--tools-of-the-trade\">Our Observability Stack — Tools of the Trade\u003C/h2>\n\u003Cp>This experience solidified our observability approach, leading us to rely on a carefully chosen stack centered around OpenTelemetry:\u003C/p>\n\u003Cp>Traces → OpenTelemetry SDK → OTel Collector → Tempo\nMetrics → Prometheus scraping /metrics endpoint → Mimir\nLogs → Promtail → Loki\nFrontend Visualization → Grafana\nError Tracking → Sentry\nWe specifically route only tracing data through the OpenTelemetry collector primarily for performance efficiency and because our existing infrastructure for metrics and logs already provides optimal handling.\u003C/p>\n\u003Ch2 id=\"beyond-basic-spans--getting-more-value-from-tracing\">Beyond Basic Spans — Getting More Value from Tracing\u003C/h2>\n\u003Cp>When engineers first explore tracing, they typically just create a few basic spans and stop there. However, tracing can offer far richer insights if used effectively:\u003C/p>\n\u003Cp>Don’t overload traces: Keep spans lean, focused on core data: function execution time, outcomes, and key events.\nCapture errors and events: Explicitly record errors and notable events for better debugging.\nHere’s a simplified example in Go using OpenTelemetry:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"go\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Start a new span for tracing\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">ctx, span \u003C/span>\u003Cspan style=\"color:#F97583\">:=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> observability.\u003C/span>\u003Cspan style=\"color:#B392F0\">StartSpan\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(ctx, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"UploadFile\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">defer\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> span.\u003C/span>\u003Cspan style=\"color:#B392F0\">End\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Perform the file upload\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">resp, err \u003C/span>\u003Cspan style=\"color:#F97583\">:=\u003C/span>\u003Cspan style=\"color:#B392F0\"> uploadFileToBucket\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> err \u003C/span>\u003Cspan style=\"color:#F97583\">!=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> nil\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    span.\u003C/span>\u003Cspan style=\"color:#B392F0\">RecordError\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(err, trace.\u003C/span>\u003Cspan style=\"color:#B392F0\">WithStackTrace\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">true\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    span.\u003C/span>\u003Cspan style=\"color:#B392F0\">SetStatus\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(codes.Error, err.\u003C/span>\u003Cspan style=\"color:#B392F0\">Error\u003C/span>\u003Cspan style=\"color:#E1E4E8\">())\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Add important event details to span\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">span.\u003C/span>\u003Cspan style=\"color:#B392F0\">AddEvent\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"File uploaded\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, trace.\u003C/span>\u003Cspan style=\"color:#B392F0\">WithAttributes\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(attribute.\u003C/span>\u003Cspan style=\"color:#B392F0\">String\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"file_id\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, resp.ID)))\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Our custom wrapper function, observability.StartSpan, initializes spans and provides standardized error handling. Context (ctx) propagation ensures continuity across distributed services.\u003C/p>\n\u003Ch2 id=\"connecting-the-dots--tracing-logs-and-metrics\">Connecting the Dots — Tracing, Logs, and Metrics\u003C/h2>\n\u003Cp>\u003Cem>Click the image to view full size.\u003C/em>\u003C/p>\n\u003Cp>Grafana significantly enhances our workflow with its “trace-to-logs” integration. By embedding trace IDs directly into our logs (using Zerolog hooks), we can easily jump from a trace directly to the corresponding log entries.\u003C/p>\n\u003Cp>By scoping logs to a specific span, we have a much faster way of getting the right log information. I often see engineers doing a generic search over a large logset, rather than determining where to start, then refining the search. This approach, in a pinch when on-call, reduces the time to resolution of issues.\u003C/p>\n\u003Cp>Here’s how we integrate trace IDs with our logs in Go:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"go\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">func\u003C/span>\u003Cspan style=\"color:#B392F0\"> setupZeroLog\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#FFAB70\">level\u003C/span>\u003Cspan style=\"color:#B392F0\"> zerolog\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Level\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    sentryWriter \u003C/span>\u003Cspan style=\"color:#F97583\">:=\u003C/span>\u003Cspan style=\"color:#B392F0\"> SentryWriter\u003C/span>\u003Cspan style=\"color:#E1E4E8\">{hub: sentry.\u003C/span>\u003Cspan style=\"color:#B392F0\">CurrentHub\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    multi \u003C/span>\u003Cspan style=\"color:#F97583\">:=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> zerolog.\u003C/span>\u003Cspan style=\"color:#B392F0\">MultiLevelWriter\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(os.Stdout, sentryWriter)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    log.Logger \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> zerolog.\u003C/span>\u003Cspan style=\"color:#B392F0\">New\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(multi).\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">        Level\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(level).\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">        Hook\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#B392F0\">TraceHook\u003C/span>\u003Cspan style=\"color:#E1E4E8\">{}).\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">        With\u003C/span>\u003Cspan style=\"color:#E1E4E8\">().\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">        Timestamp\u003C/span>\u003Cspan style=\"color:#E1E4E8\">().\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">        Logger\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    zerolog.TimeFieldFormat \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> zerolog.TimeFormatUnix\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">type\u003C/span>\u003Cspan style=\"color:#B392F0\"> TraceHook\u003C/span>\u003Cspan style=\"color:#F97583\"> struct\u003C/span>\u003Cspan style=\"color:#E1E4E8\">{}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">func\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#FFAB70\">h \u003C/span>\u003Cspan style=\"color:#B392F0\">TraceHook\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#B392F0\">Run\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#FFAB70\">e\u003C/span>\u003Cspan style=\"color:#F97583\"> *\u003C/span>\u003Cspan style=\"color:#B392F0\">zerolog\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Event\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">level\u003C/span>\u003Cspan style=\"color:#B392F0\"> zerolog\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Level\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">msg\u003C/span>\u003Cspan style=\"color:#F97583\"> string\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    ctx \u003C/span>\u003Cspan style=\"color:#F97583\">:=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> e.\u003C/span>\u003Cspan style=\"color:#B392F0\">GetCtx\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> ctx \u003C/span>\u003Cspan style=\"color:#F97583\">==\u003C/span>\u003Cspan style=\"color:#79B8FF\"> nil\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> span \u003C/span>\u003Cspan style=\"color:#F97583\">:=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> trace.\u003C/span>\u003Cspan style=\"color:#B392F0\">SpanFromContext\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(ctx); span \u003C/span>\u003Cspan style=\"color:#F97583\">!=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> nil\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        sc \u003C/span>\u003Cspan style=\"color:#F97583\">:=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> span.\u003C/span>\u003Cspan style=\"color:#B392F0\">SpanContext\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> sc.\u003C/span>\u003Cspan style=\"color:#B392F0\">HasTraceID\u003C/span>\u003Cspan style=\"color:#E1E4E8\">() {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            e.\u003C/span>\u003Cspan style=\"color:#B392F0\">Str\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"trace_id\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, sc.\u003C/span>\u003Cspan style=\"color:#B392F0\">TraceID\u003C/span>\u003Cspan style=\"color:#E1E4E8\">().\u003C/span>\u003Cspan style=\"color:#B392F0\">String\u003C/span>\u003Cspan style=\"color:#E1E4E8\">())\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            e.\u003C/span>\u003Cspan style=\"color:#B392F0\">Str\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"span_id\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, sc.\u003C/span>\u003Cspan style=\"color:#B392F0\">SpanID\u003C/span>\u003Cspan style=\"color:#E1E4E8\">().\u003C/span>\u003Cspan style=\"color:#B392F0\">String\u003C/span>\u003Cspan style=\"color:#E1E4E8\">())\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"practical-tips-for-effective-tracing\">Practical Tips for Effective Tracing\u003C/h2>\n\u003Cp>Here are some best practices to maximize the value of tracing:\u003C/p>\n\u003Cp>Avoid excessive spans in loops: Don’t flood your tracing system; aggregate repetitive tasks.\nLeverage built-in tracing: Many cloud/database SDKs have built-in OpenTelemetry support — use them!\nStandardise span attributes: Add consistent, searchable attributes (like IDs) to spans for easy filtering.\nWherever possible, follow the OpenTelemetry semantic conventions for naming span attributes and operations. This helps ensure consistency across services and tools, and makes your traces more interoperable and readable, especially when visualized in tools like Grafana.\u003C/p>\n\u003Ch2 id=\"observability-empowers-better-decisions\">Observability Empowers Better Decisions\u003C/h2>\n\u003Cp>Tracing turned a suggestion of a problem into precise insights, transforming how we diagnose and solve performance issues. With OpenTelemetry and a well-designed observability stack, you can use data and analysis to drive decisions.\u003C/p>\n\u003Cp>When your service starts feeling under the weather, tracing empowers you to quickly diagnose, understand, and heal your digital patient.\u003C/p>",{"headings":680,"localImagePaths":705,"remoteImagePaths":706,"frontmatter":707,"imagePaths":709},[681,684,687,690,693,696,699,702],{"depth":42,"slug":682,"text":683},"diagnosing-services--more-detective-work-than-medicine","Diagnosing Services — More Detective Work Than Medicine",{"depth":42,"slug":685,"text":686},"tracing-vs-logging-vs-metrics","Tracing vs. Logging vs. Metrics",{"depth":42,"slug":688,"text":689},"case-study-chasing-performance-ghosts","Case Study: Chasing Performance Ghosts",{"depth":42,"slug":691,"text":692},"our-observability-stack--tools-of-the-trade","Our Observability Stack — Tools of the Trade",{"depth":42,"slug":694,"text":695},"beyond-basic-spans--getting-more-value-from-tracing","Beyond Basic Spans — Getting More Value from Tracing",{"depth":42,"slug":697,"text":698},"connecting-the-dots--tracing-logs-and-metrics","Connecting the Dots — Tracing, Logs, and Metrics",{"depth":42,"slug":700,"text":701},"practical-tips-for-effective-tracing","Practical Tips for Effective Tracing",{"depth":42,"slug":703,"text":704},"observability-empowers-better-decisions","Observability Empowers Better Decisions",[],[],{"title":671,"description":672,"pubDate":708},"2025-05-27",[],"2026-01-02-golang-zero-value",{"id":710,"data":712,"body":716,"filePath":717,"digest":718,"rendered":719},{"title":713,"description":714,"pubDate":715},"The Golang Zero Value Trap: How pgx v5 Exposed Silent Corruption","Exploring a subtle bug in Golang's pgx v5 library that led to silent data corruption due to zero values. This post delves into the issue, its implications, and how to avoid similar pitfalls in your Go applications.",["Date","2026-01-02T00:00:00.000Z"],"## \u003Cimg src=\"/images/golang_sql.webp\" class=\"img-responsive\" alt=\"Golang SQL Hero Image\">\n\n> **TL;DR:** A pgx v5 migration didn’t introduce a bug — it exposed 18 months of silent data corruption caused by a Go map zero-value lookup and treating `time.Time{}` as valid.\n\n---\n\nMigrations are rarely just about swapping versions. Sometimes they act as a wake-up call, revealing cracks in your system you didn't know were there.\n\nWe recently upgraded our Go PostgreSQL driver from pgx v4 to v5. We expected some compile fixes and minor tweaks. Instead, we triggered a production incident that revealed a long-standing logic error silently corrupting our data.\n\n### The Context: pgx v4 vs. v5\n\nTo understand the incident, you need to know what changed in the codebase.\n\n- In pgx v4, timestamp handling was largely implicit: passing a Go `time.Time` value (including its zero value) would be encoded and sent directly to Postgres. A zero-value `time.Time{}` serialises to `0001-01-01 00:00:00`.\n- Postgres treats that as a valid timestamp, so the write succeeds.\n\n- In pgx v5, type safety and performance are handled via the `pgtype` package. Migrating to `pgtype` required explicit conversions (e.g. `pgtype.Timestamptz`), which is more correct — but it exposed places where our code relied on the old implicit behavior.\n\nTo handle zero timestamps we added a small helper:\n\n```go\nfunc TimeToTimestamptz(t time.Time) pgtype.Timestamptz {\n    if t.IsZero() {\n        return pgtype.Timestamptz{Valid: false}\n    }\n    return pgtype.Timestamptz{Time: t, Valid: true}\n}\n```\n\n### The Incident: “Null value in column \\\"created_at\\\" violates not-null constraint”\n\nShortly after releasing to production we saw errors spiking on the “Save Floorplan” endpoint. A fixture in our booking system is a series of X/Y coordinates describing a restaurant layout. The error looked like:\n\n```sh\nERROR: null value in column \"created_at\" of relation \"fixtures\" violates not-null constraint\n```\n\nThe `created_at` column is `NOT NULL`. Our code was supposed to preserve the existing timestamp during edits — we weren't trying to insert `NULL`s, or so we thought.\n\n### The Logic Bug\n\nWe traced the error to a loop that updates fixtures. The flow was:\n\n1. Load existing fixtures from the DB into a `map`.\n2. Iterate through the user's request payload.\n3. If the fixture ID exists, grab its original `created_at` from the map to preserve it.\n\nHere is the simplified version of the buggy code:\n\n```go\nif f.FixtureID == uuid.Nil {\n    f.FixtureID = r.uuidGen() // new\n} else {\n    // BUG: accessing a map without checking for existence\n    createdAt = currentFixtureIDs[f.FixtureID].CreatedAt.Time\n}\n```\n\nElsewhere the UUID was being set before this logic ran. The code assumed that “UUID exists” meant “row exists in the database”. That assumption was wrong: when the fixture wasn’t present in the map, the lookup returned the zero value.\n\n### The Go Gotcha\n\nIn Go, reading a missing map key does not panic; it returns the zero value for the value type. In our case, that meant `time.Time{}`.\n\n- For 18 months the code silently returned `time.Time{}` for missing entries.\n- Before the migration: pgx v4 would serialize that zero time as `0001-01-01 00:00:00` and Postgres accepted it as a valid timestamp.\n- After migrating to pgx v5 and `pgtype`, our helper detected the zero time, marked the value as invalid (`Valid: false`), and sent `NULL` to Postgres — which correctly rejected it.\n\nSo while the migration behaved correctly, it exposed a pre-existing data and logic problem.\n\n### The Data Trap\n\nWe fixed the map lookup and tests passed locally. But production data diverged: the DB contained many rows where `created_at` was `0001-01-01`. The sequence that caused the failure was:\n\n- Read: app reads a fixture from the DB; Postgres returns `0001-01-01`.\n- Process: app tries to preserve that date.\n- Write: `TimeToTimestamptz` treats it as zero and converts it to `NULL`.\n- Result: Postgres rejects the update because `created_at` cannot be `NULL`.\n\nEven after fixing the code, the corrupted data still broke the process until the rows were repaired.\n\n### The Fix\n\nWe updated our resolution logic to treat historical zero timestamps as corrupted/missing and overwrite them with a sensible default (usually `time.Now()`) when appropriate. This both avoids the `NULL` rejection and repairs rows on the next write.\n\n```go\nfunc resolveFixtureCreatedAt(defaultTime time.Time, existing queries.Fixture) time.Time {\n    // If the existing value is valid, preserve it\n    if existing.CreatedAt.Valid && !existing.CreatedAt.Time.IsZero() {\n        return existing.CreatedAt.Time\n    }\n\n    // Fallback: if data is missing or corrupted, use the provided default (e.g. time.Now())\n    return defaultTime\n}\n```\n\n### Check the Database\n\nAudit your rows for zero timestamps and repair them. For example, identify rows with `created_at = '0001-01-01'` and update them to a sensible value or mark them for review.\n\n### Takeaways\n\n- When a dependency upgrade surfaces errors, treat it as an opportunity to ask why the previous version tolerated the behavior.\n- Be explicit about conversions for nullable/zero values when dealing with DB drivers and typed encoding libraries like `pgtype`.\n- Tests that only create new, valid data can mask historical corruption — include migration and historical-data checks in your QA workflows.\n\nIf a dependency upgrade introduces bugs, don’t rush to patch around it — ask what the old version was quietly tolerating.","src/content/blog/2026-01-02-golang-zero-value.md","072772ad7605ed04",{"html":720,"metadata":721},"\u003Ch2 id=\"\">\u003Cimg src=\"/images/golang_sql.webp\" class=\"img-responsive\" alt=\"Golang SQL Hero Image\">\u003C/h2>\n\u003Cblockquote>\n\u003Cp>\u003Cstrong>TL;DR:\u003C/strong> A pgx v5 migration didn’t introduce a bug — it exposed 18 months of silent data corruption caused by a Go map zero-value lookup and treating \u003Ccode>time.Time{}\u003C/code> as valid.\u003C/p>\n\u003C/blockquote>\n\u003Chr>\n\u003Cp>Migrations are rarely just about swapping versions. Sometimes they act as a wake-up call, revealing cracks in your system you didn’t know were there.\u003C/p>\n\u003Cp>We recently upgraded our Go PostgreSQL driver from pgx v4 to v5. We expected some compile fixes and minor tweaks. Instead, we triggered a production incident that revealed a long-standing logic error silently corrupting our data.\u003C/p>\n\u003Ch3 id=\"the-context-pgx-v4-vs-v5\">The Context: pgx v4 vs. v5\u003C/h3>\n\u003Cp>To understand the incident, you need to know what changed in the codebase.\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>In pgx v4, timestamp handling was largely implicit: passing a Go \u003Ccode>time.Time\u003C/code> value (including its zero value) would be encoded and sent directly to Postgres. A zero-value \u003Ccode>time.Time{}\u003C/code> serialises to \u003Ccode>0001-01-01 00:00:00\u003C/code>.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Postgres treats that as a valid timestamp, so the write succeeds.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>In pgx v5, type safety and performance are handled via the \u003Ccode>pgtype\u003C/code> package. Migrating to \u003Ccode>pgtype\u003C/code> required explicit conversions (e.g. \u003Ccode>pgtype.Timestamptz\u003C/code>), which is more correct — but it exposed places where our code relied on the old implicit behavior.\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Cp>To handle zero timestamps we added a small helper:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"go\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">func\u003C/span>\u003Cspan style=\"color:#B392F0\"> TimeToTimestamptz\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#FFAB70\">t\u003C/span>\u003Cspan style=\"color:#B392F0\"> time\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Time\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#B392F0\">pgtype\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Timestamptz\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> t.\u003C/span>\u003Cspan style=\"color:#B392F0\">IsZero\u003C/span>\u003Cspan style=\"color:#E1E4E8\">() {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003Cspan style=\"color:#B392F0\"> pgtype\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Timestamptz\u003C/span>\u003Cspan style=\"color:#E1E4E8\">{Valid: \u003C/span>\u003Cspan style=\"color:#79B8FF\">false\u003C/span>\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#B392F0\"> pgtype\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Timestamptz\u003C/span>\u003Cspan style=\"color:#E1E4E8\">{Time: t, Valid: \u003C/span>\u003Cspan style=\"color:#79B8FF\">true\u003C/span>\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"the-incident-null-value-in-column-created_at-violates-not-null-constraint\">The Incident: “Null value in column “created_at” violates not-null constraint”\u003C/h3>\n\u003Cp>Shortly after releasing to production we saw errors spiking on the “Save Floorplan” endpoint. A fixture in our booking system is a series of X/Y coordinates describing a restaurant layout. The error looked like:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"sh\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">ERROR:\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> null\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> value\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> in\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> column\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"created_at\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> of\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> relation\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"fixtures\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> violates\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> not-null\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> constraint\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The \u003Ccode>created_at\u003C/code> column is \u003Ccode>NOT NULL\u003C/code>. Our code was supposed to preserve the existing timestamp during edits — we weren’t trying to insert \u003Ccode>NULL\u003C/code>s, or so we thought.\u003C/p>\n\u003Ch3 id=\"the-logic-bug\">The Logic Bug\u003C/h3>\n\u003Cp>We traced the error to a loop that updates fixtures. The flow was:\u003C/p>\n\u003Col>\n\u003Cli>Load existing fixtures from the DB into a \u003Ccode>map\u003C/code>.\u003C/li>\n\u003Cli>Iterate through the user’s request payload.\u003C/li>\n\u003Cli>If the fixture ID exists, grab its original \u003Ccode>created_at\u003C/code> from the map to preserve it.\u003C/li>\n\u003C/ol>\n\u003Cp>Here is the simplified version of the buggy code:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"go\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> f.FixtureID \u003C/span>\u003Cspan style=\"color:#F97583\">==\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> uuid.Nil {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    f.FixtureID \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> r.\u003C/span>\u003Cspan style=\"color:#B392F0\">uuidGen\u003C/span>\u003Cspan style=\"color:#E1E4E8\">() \u003C/span>\u003Cspan style=\"color:#6A737D\">// new\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">} \u003C/span>\u003Cspan style=\"color:#F97583\">else\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    // BUG: accessing a map without checking for existence\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    createdAt \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> currentFixtureIDs[f.FixtureID].CreatedAt.Time\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Elsewhere the UUID was being set before this logic ran. The code assumed that “UUID exists” meant “row exists in the database”. That assumption was wrong: when the fixture wasn’t present in the map, the lookup returned the zero value.\u003C/p>\n\u003Ch3 id=\"the-go-gotcha\">The Go Gotcha\u003C/h3>\n\u003Cp>In Go, reading a missing map key does not panic; it returns the zero value for the value type. In our case, that meant \u003Ccode>time.Time{}\u003C/code>.\u003C/p>\n\u003Cul>\n\u003Cli>For 18 months the code silently returned \u003Ccode>time.Time{}\u003C/code> for missing entries.\u003C/li>\n\u003Cli>Before the migration: pgx v4 would serialize that zero time as \u003Ccode>0001-01-01 00:00:00\u003C/code> and Postgres accepted it as a valid timestamp.\u003C/li>\n\u003Cli>After migrating to pgx v5 and \u003Ccode>pgtype\u003C/code>, our helper detected the zero time, marked the value as invalid (\u003Ccode>Valid: false\u003C/code>), and sent \u003Ccode>NULL\u003C/code> to Postgres — which correctly rejected it.\u003C/li>\n\u003C/ul>\n\u003Cp>So while the migration behaved correctly, it exposed a pre-existing data and logic problem.\u003C/p>\n\u003Ch3 id=\"the-data-trap\">The Data Trap\u003C/h3>\n\u003Cp>We fixed the map lookup and tests passed locally. But production data diverged: the DB contained many rows where \u003Ccode>created_at\u003C/code> was \u003Ccode>0001-01-01\u003C/code>. The sequence that caused the failure was:\u003C/p>\n\u003Cul>\n\u003Cli>Read: app reads a fixture from the DB; Postgres returns \u003Ccode>0001-01-01\u003C/code>.\u003C/li>\n\u003Cli>Process: app tries to preserve that date.\u003C/li>\n\u003Cli>Write: \u003Ccode>TimeToTimestamptz\u003C/code> treats it as zero and converts it to \u003Ccode>NULL\u003C/code>.\u003C/li>\n\u003Cli>Result: Postgres rejects the update because \u003Ccode>created_at\u003C/code> cannot be \u003Ccode>NULL\u003C/code>.\u003C/li>\n\u003C/ul>\n\u003Cp>Even after fixing the code, the corrupted data still broke the process until the rows were repaired.\u003C/p>\n\u003Ch3 id=\"the-fix\">The Fix\u003C/h3>\n\u003Cp>We updated our resolution logic to treat historical zero timestamps as corrupted/missing and overwrite them with a sensible default (usually \u003Ccode>time.Now()\u003C/code>) when appropriate. This both avoids the \u003Ccode>NULL\u003C/code> rejection and repairs rows on the next write.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"go\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">func\u003C/span>\u003Cspan style=\"color:#B392F0\"> resolveFixtureCreatedAt\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#FFAB70\">defaultTime\u003C/span>\u003Cspan style=\"color:#B392F0\"> time\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Time\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">existing\u003C/span>\u003Cspan style=\"color:#B392F0\"> queries\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Fixture\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#B392F0\">time\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Time\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    // If the existing value is valid, preserve it\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> existing.CreatedAt.Valid \u003C/span>\u003Cspan style=\"color:#F97583\">&#x26;&#x26;\u003C/span>\u003Cspan style=\"color:#F97583\"> !\u003C/span>\u003Cspan style=\"color:#E1E4E8\">existing.CreatedAt.Time.\u003C/span>\u003Cspan style=\"color:#B392F0\">IsZero\u003C/span>\u003Cspan style=\"color:#E1E4E8\">() {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> existing.CreatedAt.Time\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    // Fallback: if data is missing or corrupted, use the provided default (e.g. time.Now())\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> defaultTime\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"check-the-database\">Check the Database\u003C/h3>\n\u003Cp>Audit your rows for zero timestamps and repair them. For example, identify rows with \u003Ccode>created_at = '0001-01-01'\u003C/code> and update them to a sensible value or mark them for review.\u003C/p>\n\u003Ch3 id=\"takeaways\">Takeaways\u003C/h3>\n\u003Cul>\n\u003Cli>When a dependency upgrade surfaces errors, treat it as an opportunity to ask why the previous version tolerated the behavior.\u003C/li>\n\u003Cli>Be explicit about conversions for nullable/zero values when dealing with DB drivers and typed encoding libraries like \u003Ccode>pgtype\u003C/code>.\u003C/li>\n\u003Cli>Tests that only create new, valid data can mask historical corruption — include migration and historical-data checks in your QA workflows.\u003C/li>\n\u003C/ul>\n\u003Cp>If a dependency upgrade introduces bugs, don’t rush to patch around it — ask what the old version was quietly tolerating.\u003C/p>",{"headings":722,"localImagePaths":749,"remoteImagePaths":750,"frontmatter":751,"imagePaths":753},[723,725,728,731,734,737,740,743,746],{"depth":42,"slug":724,"text":724},"",{"depth":119,"slug":726,"text":727},"the-context-pgx-v4-vs-v5","The Context: pgx v4 vs. v5",{"depth":119,"slug":729,"text":730},"the-incident-null-value-in-column-created_at-violates-not-null-constraint","The Incident: “Null value in column “created_at” violates not-null constraint”",{"depth":119,"slug":732,"text":733},"the-logic-bug","The Logic Bug",{"depth":119,"slug":735,"text":736},"the-go-gotcha","The Go Gotcha",{"depth":119,"slug":738,"text":739},"the-data-trap","The Data Trap",{"depth":119,"slug":741,"text":742},"the-fix","The Fix",{"depth":119,"slug":744,"text":745},"check-the-database","Check the Database",{"depth":119,"slug":747,"text":748},"takeaways","Takeaways",[],[],{"title":713,"description":714,"pubDate":752},"2026-01-02",[]]